{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "gOTbX73FQ4jT"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T16:11:29.495955Z",
     "start_time": "2021-02-23T16:11:29.192871Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# MAGIC\n",
    "\n",
    "# %load_ext nb_black\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "# ---\n",
    "\n",
    "# IMPORT\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint\n",
    "\n",
    "import math\n",
    "import random as rn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as la\n",
    "# from scipy.stats import rankdata\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "# from itertools import combinations\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "import timeit\n",
    "\n",
    "# import pytz\n",
    "from datetime import datetime\n",
    "# from datetime import timedelta\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import matplotlib, cm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "import networkx as nx\n",
    "# from networkx.algorithms import bipartite\n",
    "\n",
    "# import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# ---\n",
    "\n",
    "# PLOT\n",
    "\n",
    "# List of matplotlib styles\n",
    "# print(plt.style.available)\n",
    "# plt.style.use('classic')\n",
    "# plt.style.use('dark_background')\n",
    "# plt.style.use('fast')\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# plt.style.use('ggplot')\n",
    "# plt.style.use('grayscale')\n",
    "# plt.style.use('seaborn')\n",
    "# plt.style.use('seaborn-bright')\n",
    "# plt.style.use('seaborn-colorblind')\n",
    "# plt.style.use('seaborn-dark')\n",
    "# plt.style.use('seaborn-dark-palette')\n",
    "# plt.style.use('seaborn-darkgrid')\n",
    "# plt.style.use('seaborn-deep')\n",
    "# plt.style.use('seaborn-muted')\n",
    "# plt.style.use('seaborn-notebook')\n",
    "# plt.style.use('seaborn-paper')\n",
    "# plt.style.use('seaborn-pastel')\n",
    "# plt.style.use('seaborn-poster')\n",
    "# plt.style.use('seaborn-talk')\n",
    "# plt.style.use('seaborn-ticks')\n",
    "# plt.style.use('seaborn-white')\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "# plt.style.use('tableau-colorblind10')\n",
    "# plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Publication plot style\n",
    "# plt.rc('figure', figsize=(4, 3))  # or (8, 6)\n",
    "# plt.rc('font', family='serif')\n",
    "# plt.rc('font', family='monospace')\n",
    "# plt.rc('font', size=12)\n",
    "# plt.rc('xtick', labelsize=10)\n",
    "# plt.rc('ytick', labelsize=10)\n",
    "# plt.rc('xtick', labelsize='x-small')\n",
    "# plt.rc('ytick', labelsize='x-small')\n",
    "# plt.rc('savefig', dpi=400)\n",
    "# plt.rc('savefig', bbox_inches='tight')\n",
    "\n",
    "# My plot style\n",
    "# mpl.rc('font', **{'family': 'sans-serif', 'sans-serif': ['Arial']})\n",
    "# mpl.rc('font', size=16)  # default text sizes\n",
    "# plt.rc('figure', titlesize=16)  # fontsize of the figure title\n",
    "# plt.rc('axes', titlesize=16)  # fontsize of the axes title\n",
    "# mpl.rc('axes', labelsize=14)  # fontsize of the x and y labels\n",
    "# plt.rc('legend', fontsize=14)  # legend fontsize\n",
    "# mpl.rc('xtick', labelsize=14, color='#222222')  # fontsize of the tick labels\n",
    "# mpl.rc('ytick', labelsize=14, color='#222222')  # 222222 is dark grey\n",
    "# mpl.rc('xtick.major', size=6, width=1)\n",
    "# mpl.rc('xtick.minor', size=3, width=1)\n",
    "# mpl.rc('ytick.major', size=6, width=1)\n",
    "# mpl.rc('ytick.minor', size=3, width=1)\n",
    "# mpl.rc('axes', linewidth=1, edgecolor='#222222', labelcolor='#222222')\n",
    "# mpl.rc('text', usetex=False, color='#222222')\n",
    "# mpl.rc('text', usetex=True, color='#222222')\n",
    "\n",
    "# Reset plot style\n",
    "# plt.rcdefaults()\n",
    "\n",
    "# Seaborn library config\n",
    "# sns.set(color_codes=True)\n",
    "# sns.set(rc={'figure.figsize':(5,5)})\n",
    "\n",
    "# --- METHOD ---\n",
    "\n",
    "# -- DB ---\n",
    "\n",
    "\n",
    "def db_connect(db):\n",
    "    \"\"\"\n",
    "    Connect to the input database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return None\n",
    "\n",
    "\n",
    "def db_close(conn):\n",
    "    \"\"\"\n",
    "    Close the connection to the database\n",
    "    \"\"\"\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def db_row_count(conn, table_name, print_out=False):\n",
    "    \"\"\"\n",
    "    Calculate the number of entries in the input table\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('SELECT COUNT(*) FROM {}'.format(table_name))\n",
    "    count = cur.fetchall()\n",
    "    if print_out:\n",
    "        print('\\n|DB|={}'.format(count[0][0]))\n",
    "    return count[0][0]\n",
    "\n",
    "\n",
    "def db_create(db, query):\n",
    "    \"\"\"\n",
    "    Create a database based using the input CREATE query\n",
    "    \"\"\"\n",
    "    conn = db_connect(db)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def db_insert_many(db, query, data):\n",
    "    \"\"\"\n",
    "    Insert many (more than one) row to a table of a database\n",
    "    \"\"\"\n",
    "    conn = db_connect(db)\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        cur.executemany(query, data)\n",
    "        conn.commit()\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# --- System ---\n",
    "\n",
    "\n",
    "def dir_walk(path, ext='', save=False):\n",
    "    \"\"\"\n",
    "    Walk through a directory, find all the file with specified extension\n",
    "    \"\"\"\n",
    "    # set ext to a specific extension if needed\n",
    "    f = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # relative path of file\n",
    "            relative_path = os.path.join(root, file)\n",
    "            # extention (type of file)\n",
    "            ext_of_file = os.path.splitext(relative_path)[-1].lower()[1:]\n",
    "            # if extension is set and equal to what we want\n",
    "            if ext != '' and ext_of_file == ext:\n",
    "                f.append(os.path.abspath(relative_path))\n",
    "            # if extension is not set add the file anyway\n",
    "            else:\n",
    "                f.append(os.path.abspath(relative_path))\n",
    "    f.sort()\n",
    "    if save:\n",
    "        np.savetxt('files.csv', f, delimiter=',', fmt='%s')\n",
    "    return f\n",
    "\n",
    "\n",
    "def line_count(filename):\n",
    "    \"\"\"\n",
    "    Count the number of lines in the input file\n",
    "    \"\"\"\n",
    "    # count = 0\n",
    "    # for line in open(filename).xreadlines(  ): count += 1\n",
    "    # return count\n",
    "    # OR\n",
    "    return len(open(filename).readlines())\n",
    "\n",
    "\n",
    "# --- Colors ---\n",
    "\n",
    "\n",
    "def colors_create(number_of_colors=1, color_map='Wistia', output=False):\n",
    "    \"\"\"\n",
    "    create a series of colors from the selected spectrum, e.g., Wistia [cold to hot]\n",
    "    \"\"\"\n",
    "    color_list = []\n",
    "    cmap = cm.get_cmap(color_map, number_of_colors)\n",
    "    for i in range(cmap.N):\n",
    "        # will return rgba, we take only first 3 so we get rgb\n",
    "        rgb = cmap(i)[:3]\n",
    "        # print(matplotlib.colors.rgb2hex(rgb))\n",
    "        color_list.append(matplotlib.colors.rgb2hex(rgb))\n",
    "    if output:\n",
    "        for i in range(len(color_list)):\n",
    "            plt.scatter(i, 1, c=color_list[i], s=20)\n",
    "    return color_list\n",
    "\n",
    "\n",
    "# --- Data Structure ---\n",
    "\n",
    "\n",
    "def dict_add(dictionary, key, value):\n",
    "    \"\"\"\n",
    "    Add a key:value to dictionary if key does not already exist\n",
    "    \"\"\"\n",
    "    if key not in dictionary:\n",
    "        dictionary[key] = value\n",
    "\n",
    "\n",
    "def dict_lookup(dictionary, key):\n",
    "    \"\"\"\n",
    "    Search the given KEY in dictionary ...\n",
    "    found -> return its value (could be an index assign to that value)\n",
    "    not found -> add the key and return its value (which is a new index)\n",
    "    useful for creating hash table of KEY->INDEX\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    if key not in dictionary:\n",
    "        value = len(dictionary)\n",
    "        dictionary[key] = value\n",
    "    else:\n",
    "        value = dictionary.get(key)\n",
    "    return value\n",
    "\n",
    "\n",
    "def label_amend(label_list, input_label, end=True):\n",
    "    \"\"\"\n",
    "    Add (or remove) an input label to list of labels\n",
    "    Type of input_label can be integer or string\n",
    "    List of labels are assume to have a name like:\n",
    "    ['folder/file.extension','folder/file.extension']\n",
    "    Output looks like:\n",
    "    ['folder/file_label.extension','folder/file_label.extension']\n",
    "    \"\"\"\n",
    "    # Fist check to see if list and input_label are not None\n",
    "    if len(label_list) > 0 and len(input_label) > 0:\n",
    "        # Then amend all labels in the label list\n",
    "        if end:\n",
    "            # Can be use to amend file name (before the file type)\n",
    "            # By default adds input label to end of all labels in the list\n",
    "            label_list = [\n",
    "                label.split('.')[0] + '_' + str(input_label) + '.' +\n",
    "                label.split('.')[1] for label in label_list\n",
    "            ]\n",
    "        else:\n",
    "            # Can be use to amend folder of a file and adds a pre-fix name to the folder\n",
    "            # We assume we onle have one sub-folder-level (or character \"/\") in the name\n",
    "            label_list_new = []\n",
    "            root = os.getcwd()\n",
    "            for label in label_list:\n",
    "                path = os.path.join(\n",
    "                    root,\n",
    "                    label.split('/')[0] + '/' + str(input_label)\n",
    "                )\n",
    "                # Making sure the folder exist, otherwise create it\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                label_list_new.append(\n",
    "                    label.split('/')[0] + '/' + str(input_label) + '/' +\n",
    "                    label.split('/')[1]\n",
    "                )\n",
    "            label_list = label_list_new[:]\n",
    "    return label_list\n",
    "\n",
    "\n",
    "def list_intersection(lst1, lst2, version=4):\n",
    "    \"\"\"\n",
    "    Intersection of two list or all common elements of two lists\n",
    "    \"\"\"\n",
    "    if version == 1:\n",
    "        return [value for value in lst1 if value in lst2]\n",
    "    elif version == 2:\n",
    "        return list(set(lst1) & set(lst2))\n",
    "    elif version == 3:\n",
    "        if len(lst1) > len(lst2):\n",
    "            return set(lst1).intersection(lst2)\n",
    "        else:\n",
    "            return set(lst2).intersection(lst1)\n",
    "    elif version == 4:  # O(n)\n",
    "        temp = set(lst2)\n",
    "        return [value for value in lst1 if value in temp]\n",
    "\n",
    "\n",
    "def rank(x, return_rank=False):\n",
    "    \"\"\"\n",
    "    Rank items of a list from largest to smallest value\n",
    "    and return a list of [(index,value,rank)]\n",
    "    \"\"\"\n",
    "    # Input is list\n",
    "    if isinstance(x, list):\n",
    "        # Convert to series\n",
    "        s = pd.Series(x)\n",
    "    # Input is series\n",
    "    if isinstance(x, pd.Series):\n",
    "        # Only sort based on the index\n",
    "        s = x.sort_index()\n",
    "    # Input is 2D array\n",
    "    if isinstance(x, np.ndarray):\n",
    "        s = pd.Series(x.flatten())\n",
    "    # Input is dictionary\n",
    "    if isinstance(x, dict):\n",
    "        s = pd.Series(x, index=sorted(x.keys()))\n",
    "    # Rank the data\n",
    "    ranked = s.rank(method='dense', ascending=False).astype(int).sort_values()\n",
    "    # If input was 2D array change index to tuple (i,j) of matrix\n",
    "    if isinstance(x, np.ndarray):\n",
    "        temp = np.unravel_index(ranked.index, x.shape)\n",
    "        ranked.index = list(zip(temp[0],temp[1]))\n",
    "    # If the rank values are needed then return entire series\n",
    "    if return_rank:\n",
    "        return ranked\n",
    "    # Otherwise return ranked index of items\n",
    "    return list(ranked.index)\n",
    "\n",
    "\n",
    "# Test for rank\n",
    "assert rank(\n",
    "    {\n",
    "        0: 2,\n",
    "        1: 4,\n",
    "        2: 6,\n",
    "        3: 8,\n",
    "        4: 10,\n",
    "        5: 9,\n",
    "        6: 7,\n",
    "        7: 7,\n",
    "        8: 7,\n",
    "        9: 0,\n",
    "        10: 1,\n",
    "        11: 2\n",
    "    }\n",
    ") == [4, 5, 3, 6, 7, 8, 2, 1, 0, 11, 10, 9]\n",
    "\n",
    "\n",
    "def breakdown(lst, num):\n",
    "    \"\"\"\n",
    "    Breakdown a list into chunks of sub-list with size of n\n",
    "    \"\"\"\n",
    "    # pprint(list(range(len(lst))))\n",
    "    # pprint(lst)\n",
    "\n",
    "    # Sort the list (high -> low)\n",
    "    # Rank the sorted list\n",
    "    ranks = pd.Series(lst).rank(method='dense',\n",
    "                                ascending=False).astype(int).sort_values()\n",
    "    # Divide the ranks into chunk of desired size\n",
    "    chunks = [list(ranks.iloc[i:i + num]) for i in range(0, len(ranks), num)]\n",
    "    # Dictionary of {rank : indices}\n",
    "    rank_idx = {i: set() for i in set(ranks)}\n",
    "    for idx, rank in ranks.items():\n",
    "        # print(f'{rank} : {idx}')\n",
    "        rank_idx[rank].add(idx)\n",
    "    # Create a new chunk, but index of high ranks to low ranks\n",
    "    bd = []\n",
    "    for chunk in chunks:\n",
    "        lst_temp = []\n",
    "        for rank in chunk:\n",
    "            # Picl a random index from the selected rank\n",
    "            idx = rn.sample(rank_idx[rank], 1)[0]\n",
    "            lst_temp.append(idx)\n",
    "            rank_idx.get(rank).remove(idx)\n",
    "        bd.append(lst_temp)\n",
    "    return bd\n",
    "\n",
    "\n",
    "# --- Save & load ---\n",
    "\n",
    "\n",
    "def fig_save(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    \"\"\"\n",
    "    Method for saving figure\n",
    "    \"\"\"\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + '.' + fig_extension)\n",
    "    print('saving figure ...', fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "\n",
    "def dict_save(data, file='data', method='n', sort=False):\n",
    "    \"\"\"\n",
    "    Save input dictionary using different method:\n",
    "        n: numpy (npy) -> good for saving the data type\n",
    "        c: pandas (csv) -> good for looking at data after saving and sorting\n",
    "        j: json -> also good for looking at data (simple key type) and sorting\n",
    "        p: pickle -> should be fastest, good for simple data type\n",
    "    \"\"\"\n",
    "    # npy\n",
    "    if method == 'n':\n",
    "        filename = file + '.npy'\n",
    "        np.save(filename, data)\n",
    "    # csv\n",
    "    elif method == 'c':\n",
    "        filename = file + '.csv'\n",
    "        if not sort:\n",
    "            pd.DataFrame.from_dict(data, orient='index'\n",
    "                                   ).to_csv(filename, header=False)\n",
    "        else:\n",
    "            pd.DataFrame.from_dict(data, orient='index').sort_index(\n",
    "                axis=0\n",
    "            ).to_csv(filename, header=False)\n",
    "    # json\n",
    "    elif method == 'j':\n",
    "        filename = file + '.json'\n",
    "        with open(filename, 'w') as fp:\n",
    "            if not sort:\n",
    "                json.dump(data, fp)\n",
    "            else:\n",
    "                json.dump(data, fp, sort_keys=True, indent=4)\n",
    "    # pickle\n",
    "    elif method == 'p':\n",
    "        filename = file + '.p'\n",
    "        with open(filename, 'wb') as fp:\n",
    "            pickle.dump(data, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def dict_load(file='data', method='n'):\n",
    "    \"\"\"\n",
    "    Load input file into a dictionary using different method:\n",
    "        n: numpy (npy)\n",
    "        c: pandas (csv)\n",
    "        j: json\n",
    "        p: pickle\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    # npy\n",
    "    if method == 'n':\n",
    "        filename = file + '.npy'\n",
    "        data = np.load(filename, allow_pickle='True').item()\n",
    "    # csv\n",
    "    elif method == 'c':\n",
    "        filename = file + '.csv'\n",
    "        data = pd.read_csv(filename, header=None,\n",
    "                           index_col=0).T.to_dict('records')[0]\n",
    "    # json\n",
    "    elif method == 'j':\n",
    "        filename = file + '.json'\n",
    "        with open(filename, 'r') as fp:\n",
    "            data = json.load(fp)\n",
    "    # pickle\n",
    "    elif method == 'p':\n",
    "        filename = file + '.p'\n",
    "        with open(filename, 'rb') as fp:\n",
    "            data = pickle.load(fp)\n",
    "    return data\n",
    "\n",
    "\n",
    "def list_save(\n",
    "    input_list,\n",
    "    file_name='list',\n",
    "    file_postname='',\n",
    "    file_extension='csv',\n",
    "    delimiter=',',\n",
    "    replace=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Method for saving list\n",
    "    \"\"\"\n",
    "    if len(postname) > 0:\n",
    "        file_name = file_name + '_' + file_postname\n",
    "    file_path = os.path.join(OUTPUT_PATH, file_name + '.' + file_extension)\n",
    "    if os.path.exists(file_input):\n",
    "        print('file already exist.')\n",
    "        if replace:\n",
    "            print('overwriting ... ')\n",
    "            np.savetxt(file_path, input_list, delimiter=delimiter, fmt='%s')\n",
    "        else:\n",
    "            print('saving ... ')\n",
    "            file_path = os.path.join(\n",
    "                OUTPUT_PATH, file_name + '_new.' + file_extension\n",
    "            )\n",
    "            np.savetxt(file_path, input_list, delimiter=delimiter, fmt='%s')\n",
    "        print('done!')\n",
    "    else:\n",
    "        print('saving ... ')\n",
    "        np.savetxt(file_path, input_list, delimiter=delimiter, fmt='%s')\n",
    "        print('done!')\n",
    "\n",
    "\n",
    "# --- Linear Algebra ---\n",
    "\n",
    "# def array_top_n(arr, top_N=1):\n",
    "#     \"\"\"\n",
    "#     find top 'N' values in 2d numpy array\n",
    "#     \"\"\"\n",
    "#     idx = np.argpartition(arr, arr.size - top_N, axis=None)[-top_N:][::-1]\n",
    "#     result = np.column_stack(np.unravel_index(idx, arr.shape))\n",
    "#     return [(e[0], e[1]) for e in result]\n",
    "\n",
    "\n",
    "def top_n(arr, top_N=1, index=True):\n",
    "    \"\"\"\n",
    "    find top 'N' values of 1d numpy array or list\n",
    "    Return the index of top values (if index == True) or index and value as tuple\n",
    "    \"\"\"\n",
    "    idx = np.argsort(arr)[::-1][:top_N]\n",
    "    if index:\n",
    "        return idx\n",
    "    else:\n",
    "        return [(e, arr[e]) for e in idx]\n",
    "\n",
    "\n",
    "def array_top_n(arr, top_N=1):\n",
    "    \"\"\"\n",
    "    find top 'N' values in 2d numpy array\n",
    "    \"\"\"\n",
    "    idx = (-arr).argsort(axis=None, kind='mergesort')\n",
    "    # idx = (-arr).argsort(axis=None, kind='mergesort')[:top_N]\n",
    "    result = np.vstack(np.unravel_index(idx, arr.shape)).T\n",
    "    return [(e[0], e[1]) for e in result]\n",
    "\n",
    "\n",
    "def matrix_print(M, out_int=True):\n",
    "    \"\"\"\n",
    "    Print input matrix in terminal, without any cut-off\n",
    "    \"\"\"\n",
    "    for i, element in enumerate(M):\n",
    "        if out_int:\n",
    "            print(*element.astype(int))\n",
    "        else:\n",
    "            print(*element)\n",
    "\n",
    "\n",
    "# --- FOLDERS ---\n",
    "\n",
    "# --- Common Folders ---\n",
    "# ROOT_PATH = '.'\n",
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'data')\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "IMAGE_PATH = os.path.join(ROOT_PATH, 'image')\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "# --- Project Specific Folders ---\n",
    "DB_PATH = os.path.join(ROOT_PATH, 'db')\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "FILE_PATH = os.path.join(ROOT_PATH, 'file')\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "NET_PATH = os.path.join(ROOT_PATH, 'network')\n",
    "os.makedirs(NET_PATH, exist_ok=True)\n",
    "HITS_PATH = os.path.join(ROOT_PATH, 'hits')\n",
    "os.makedirs(HITS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Time takes for a part of code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start_time = timeit.default_timer()\n",
    "# CODE\n",
    "# elapsed_time = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save a dictionary to **CSV** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('dict.csv', 'w') as csv_file:\n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in time_out_degrees.items():\n",
    "#         writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save and load a dictionary using **PICKLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump(your_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('filename.pickle', 'rb') as handle:\n",
    "#     unserialized_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Save a list to **TXT** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt(file_path, list_name, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Breakdown a list into chunk of size **n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:37:54.287095Z",
     "start_time": "2021-02-09T16:37:54.278500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# pprint(list(chunks(list(range(10, 75)), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T16:39:06.471688Z",
     "start_time": "2021-02-09T16:39:06.463009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "lst = list(range(10, 75))\n",
    "lst = [lst[i:i + n] for i in range(0, len(lst), n)]\n",
    "\n",
    "# pprint(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Rank 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T15:16:36.189210Z",
     "start_time": "2021-02-23T15:16:36.179957Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6 6 8 4]\n",
      " [5 7 3 8 9]\n",
      " [8 0 8 5 1]]\n",
      "[[4 4 4 2 6]\n",
      " [5 3 7 2 1]\n",
      " [2 9 2 5 8]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "arr = np.random.randint(10, size=(3, 5))\n",
    "print(arr)\n",
    "\n",
    "ranks=rankdata(arr.max()-arr,method='dense').reshape(arr.shape)\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dataset_db(file_input=['data'], file_output=['uiuc.db'], output=True):\n",
    "    \"\"\"\n",
    "    Read log files of each user, detect user_hash based on the log names\n",
    "    then rename the folder to the user_id and extract data from log files\n",
    "    \"\"\"\n",
    "\n",
    "    # Paths\n",
    "    DATA = os.path.join(ROOT_PATH, file_input[0])\n",
    "    DB = os.path.join(DB_PATH, file_output[0])\n",
    "\n",
    "    # Reading files\n",
    "    files = dir_walk(DATA)\n",
    "\n",
    "    # Dictionary {User : MAC}\n",
    "    user_mac = {}\n",
    "    for f in files:\n",
    "        sp = f.split('/')\n",
    "        # Second to last is user_id [1-28]\n",
    "        u = int(sp[-2])\n",
    "        # Last is the file_name = datetime + mac\n",
    "        d, mac = sp[-1].split('.')\n",
    "        dict_add(user_mac, u, mac)\n",
    "\n",
    "    # Sort user_mac based on user_id (key)\n",
    "    user_mac = sorted([(int(k), v) for k, v in user_mac.items()])\n",
    "\n",
    "    # Save user_mac to DB\n",
    "    query = f'''\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    mac TEXT NOT NULL\n",
    "    );\n",
    "    '''\n",
    "    db_create(DB, query)\n",
    "    query = 'INSERT INTO users VALUES (?,?)'\n",
    "    db_insert_many(DB, query, user_mac)\n",
    "\n",
    "    # Read BT and WiFI data\n",
    "    # error_list = [] # TEST\n",
    "    user_b = defaultdict(list)  # Bluetooth\n",
    "    user_w = defaultdict(list)  # WiFi\n",
    "    b_mac = []  # Bluetooth MAC list\n",
    "    w_mac = []  # WiFi MAC list\n",
    "\n",
    "    # Add BT users to the list of bluetooth devices\n",
    "    # They are not needed in WiFi\n",
    "    # Because they only see access points MAC in logs\n",
    "    b_mac = [u[1] for u in user_mac]\n",
    "\n",
    "    for file in files:\n",
    "        is_b = False\n",
    "        is_w = False\n",
    "        sp = file.split('/')\n",
    "        u = int(sp[-2])\n",
    "        d, mac = sp[-1].split('.')\n",
    "        # Check the type of log file\n",
    "        if d[0] == 's':  # BT\n",
    "            is_b = True\n",
    "        else:  # WiFi\n",
    "            is_w = True\n",
    "\n",
    "        # Read file line by line -> save to DF -> convert DF column to series\n",
    "        df = pd.read_csv(file, names=['line'])\n",
    "        lines = df.line\n",
    "        last_time = ''\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            # Length of line with time info = 21\n",
    "            # Length of line with mac info = 40\n",
    "            if len(lines[i]) < 40:  # We read a line with time info ...\n",
    "                # String format\n",
    "                # last_time = lines[i]\n",
    "                # Datetime format\n",
    "                last_time = datetime.strptime(lines[i][2:], '%m-%d-%Y %H:%M:%S')\n",
    "                continue\n",
    "            else:  # We read a line with mac address ...\n",
    "                # If last_time is out of range i.e. < 2010-2-25 then skip until read a valid one\n",
    "                if last_time < datetime(2010, 2, 25):\n",
    "                    continue\n",
    "                # If the last_time was valid and the line is a BT or wifi do ...\n",
    "                if is_b:  # Bluetooth\n",
    "                    b_index = 0\n",
    "                    # b_index = -1  # TEST: changed to -1 to detect the possible errors\n",
    "                    if lines[i] not in b_mac:\n",
    "                        # Add new MAC to list\n",
    "                        b_mac.append(lines[i])\n",
    "                        # Index of new mac = (len - 1) of list, becasue it was the latest added element\n",
    "                        b_index = len(b_mac) - 1\n",
    "                    else:\n",
    "                        b_index = b_mac.index(lines[i])\n",
    "                    # Add new BT log to dictionary {user:[(time,mac)]\n",
    "                    user_b[u].append((last_time, b_index))\n",
    "                    # TEST\n",
    "                    # if b_index < 0:\n",
    "                    # if last_time < datetime(2010,2,24):\n",
    "                    # error_list.append((file,lines[i],last_time))\n",
    "\n",
    "                else:  # WiFi\n",
    "                    w_index = 0\n",
    "                    # w_index = -1  # TEST\n",
    "                    if lines[i] not in w_mac:\n",
    "                        # Add new MAC to list\n",
    "                        w_mac.append(lines[i])\n",
    "                        # Index of new mac = (len - 1) of list becasue it is the last added element\n",
    "                        w_index = len(w_mac) - 1\n",
    "                    else:\n",
    "                        w_index = w_mac.index(lines[i])\n",
    "                    user_w[u].append((last_time, w_index))\n",
    "                    # TEST\n",
    "                    # if w_index < 0:\n",
    "                    # if last_time < datetime(2010,2,24):\n",
    "                    # error_list.append((file,lines[i],last_time))\n",
    "    # TEST : save the errors\n",
    "    # np.savetxt('error_list.csv', error_list, delimiter=',', fmt='%s')\n",
    "\n",
    "    if output:\n",
    "        print('# BT devices:', len(b_mac))\n",
    "        print('# WiFi devices:', len(w_mac))\n",
    "\n",
    "    # Save {BT_id : BT_MAC} to DB\n",
    "    query = f'''\n",
    "    CREATE TABLE IF NOT EXISTS bluetooth (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    mac TEXT NOT NULL\n",
    "    );\n",
    "    '''\n",
    "    db_create(DB, query)\n",
    "    b_mac_insert = [(i, b_mac[i]) for i in range(len(b_mac))]\n",
    "    query = 'INSERT INTO bluetooth VALUES (?,?)'\n",
    "    db_insert_many(DB, query, b_mac_insert)\n",
    "\n",
    "    # Save {WiFi_id : WiFi_MAC} to DB\n",
    "    query = f'''\n",
    "    CREATE TABLE IF NOT EXISTS wifi (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    mac TEXT NOT NULL\n",
    "    );\n",
    "    '''\n",
    "    db_create(DB, query)\n",
    "    w_mac_insert = [(i, w_mac[i]) for i in range(len(w_mac))]\n",
    "    query = 'INSERT INTO wifi VALUES (?,?)'\n",
    "    db_insert_many(DB, query, w_mac_insert)\n",
    "\n",
    "    # Save BT and WiFi logs to DB with following columns\n",
    "    # user_node, bluetooth_node / ap_node , time, bluetooth = 0 / wifi = 1\n",
    "    query = f'''\n",
    "    CREATE TABLE IF NOT EXISTS logs (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    user INTEGER NOT NULL,\n",
    "    mac INTEGER NOT NULL,\n",
    "    time TIMESTAMP,\n",
    "    wifi INTEGER\n",
    "    );\n",
    "    '''\n",
    "    db_create(DB, query)\n",
    "    interaction_insert = []\n",
    "    for k, v in user_b.items():  # k = user\n",
    "        # v is a list of interactions = (time, MAC)\n",
    "        for item in v:\n",
    "            interaction_insert.append((k, item[1], item[0], 0))\n",
    "    for k, v in user_w.items():\n",
    "        for item in v:\n",
    "            interaction_insert.append((k, item[1], item[0], 1))\n",
    "    query = 'INSERT INTO logs(user,mac,time,wifi) VALUES (?,?,?,?)'\n",
    "    db_insert_many(DB, query, interaction_insert)\n",
    "\n",
    "    if output:\n",
    "        print('# interactions:', len(interaction_insert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dataset_csv(file_input=['db/uiuc.db'],\n",
    "                file_output=['db/bt.csv', 'db/wifi.csv'],\n",
    "                input_label='',\n",
    "                output_label='',\n",
    "                time_bin=False,\n",
    "                output=False,\n",
    "                save=False):\n",
    "    \"\"\"\n",
    "    Read logs from DB and divide them based on type (BT/WiFi)\n",
    "    into 2 seperate dataset files bt.csv and wifi.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # Modify input & output file names\n",
    "    if len(input_label) > 0:\n",
    "        file_input = [\n",
    "            label.split('.')[0] + '_' + str(input_label) + '.' +\n",
    "            label.split('.')[1] for label in file_input\n",
    "        ]\n",
    "    if len(output_label) > 0:\n",
    "        file_output = [\n",
    "            label.split('.')[0] + '_' + str(output_label) + '.' +\n",
    "            label.split('.')[1] for label in file_output\n",
    "        ]\n",
    "\n",
    "    # Read DB\n",
    "    # BT logs : wifi=0\n",
    "    # WiFi logs : wifi=1\n",
    "    print('Reading DB ...')\n",
    "    conn = db_connect(file_input[0])\n",
    "\n",
    "    # wifi = 0\n",
    "    # query = f'''\n",
    "    # SELECT user, mac, time\n",
    "    # FROM logs\n",
    "    # WHERE wifi={wifi} AND time >= '2010-03-01' AND time < '2010-03-21'\n",
    "    # ORDER BY time\n",
    "    # '''\n",
    "\n",
    "    query = f'''\n",
    "    SELECT user, mac, time, wifi\n",
    "    FROM logs\n",
    "    WHERE time >= '2010-03-01' AND time < '2010-03-21'\n",
    "    ORDER BY time\n",
    "    '''\n",
    "\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    db_close(conn)\n",
    "    print(len(df), 'records')\n",
    "\n",
    "    # Change the time columns type : string -> datetime\n",
    "    df['time'] = df['time'].astype('datetime64[ns]')\n",
    "    # Create timestamp list\n",
    "    times = pd.Series(sorted(df.time.unique()))\n",
    "    # Range of timestamp\n",
    "    if output:\n",
    "        print('First timestamp: {}\\nLast timestamp: {}'.format(\n",
    "            times.iloc[0], times.iloc[-1]))\n",
    "\n",
    "    # Reset the minute and the second of timestamps to zero\n",
    "    # Bining the timestamps to the grangularity of hour\n",
    "    if time_bin:\n",
    "        df['time'] = df['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "        # Create timestamp list (again after filtering)\n",
    "        times = pd.Series(sorted(df.time.unique()))\n",
    "\n",
    "    # Distribution of timestamps throughout dataset\n",
    "    # ts_count = dict(Counter(times))\n",
    "    # ts_count = sorted(ts_count.items())\n",
    "    # ts_count = sorted(ts_count.items(), key=lambda x: x[0], reverse=True)\n",
    "    # x, y = zip(*ts_count)\n",
    "    # plt.plot(x, y)\n",
    "\n",
    "    if save:\n",
    "        df[df['wifi'] == 0].iloc[:, :-1].to_csv(file_output[0], header=False, index=False)\n",
    "        df[df['wifi'] == 1].iloc[:, :-1].to_csv(file_output[1], header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dataset_read(\n",
    "    file_input=['db/bt.csv', 'db/wifi.csv'],\n",
    "    file_output=[],\n",
    "    input_label='',\n",
    "    output_label='',\n",
    "    wifi=False,  # default = BT, if True => WiFi\n",
    "    output=False,\n",
    "    save=False):\n",
    "    \"\"\"\n",
    "    Read the WiFi or BT (csv) dataset file based on wifi flag\n",
    "    return Dataframe with 3 columns of (u, v, t)\n",
    "    \"\"\"\n",
    "\n",
    "    # Modify input & output file names\n",
    "    if len(input_label) > 0:\n",
    "        file_input = [\n",
    "            label.split('.')[0] + '_' + str(input_label) + '.' +\n",
    "            label.split('.')[1] for label in file_input\n",
    "        ]\n",
    "    if len(output_label) > 0:\n",
    "        file_output = [\n",
    "            label.split('.')[0] + '_' + str(output_label) + '.' +\n",
    "            label.split('.')[1] for label in file_output\n",
    "        ]\n",
    "\n",
    "    df = []\n",
    "    if not wifi:  # BT\n",
    "        df = pd.read_csv(file_input[0],\n",
    "                         header=None,\n",
    "                         names=['u', 'v', 't'],\n",
    "                         parse_dates=['t'])\n",
    "    else:  # WiFI\n",
    "        df = pd.read_csv(file_input[1],\n",
    "                         header=None,\n",
    "                         names=['u', 'v', 't'],\n",
    "                         parse_dates=['t'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create DB\n",
    "Create SQLite database from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create CSV\n",
    "Times are binned with 1-hour windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dataset_csv(time_bin=True, output=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Create CSV - Raw\n",
    "Times are not binned. File names has postfix of __raw__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dataset_csv(output_label='raw', time_bin=False, output=True, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Read BT/WiFi data CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read the CSV file for Bluetooth records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_bt = dataset_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read the CSV file for WiFi records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_wifi = dataset_read(wifi=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read the paths of DB files for each user (in a separate folder) and save in a list   \n",
    "It turned out that **29,656** records has the date less than 2010-2-25 and therefore are considered as noise, and no error was risen from the name of the devices or users   \n",
    "After removing the WiFi and Bluetooth devices out of acceptable datetime range i.e. less than 2010-2-24, from **8,055** Bluetooth and **6,722** WiFi devices we are down to **7,995** Bluetooth and **6,705** WiFi devices now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sample Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read node list and edge list data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Grab node list data hosted on Gist\n",
    "# nodelist = pd.read_csv('https://gist.githubusercontent.com/brooksandrew/f989e10af17fb4c85b11409fea47895b/raw/a3a8da0fa5b094f1ca9d82e1642b384889ae16e8/nodelist_sleeping_giant.csv')\n",
    "nodelist = pd.read_csv('file/nodelist_sleeping_giant.csv')\n",
    "\n",
    "# Grab edge list data hosted on Gist\n",
    "# edgelist = pd.read_csv('https://gist.githubusercontent.com/brooksandrew/e570c38bcc72a8d102422f2af836513b/raw/89c76b2563dbc0e88384719a35cba0dfc04cd522/edgelist_sleeping_giant.csv')\n",
    "edgelist = pd.read_csv('file/edgelist_sleeping_giant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preview nodelist\n",
    "nodelist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preview edgelist\n",
    "edgelist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create empty graph\n",
    "g = nx.Graph()\n",
    "\n",
    "# Add edges and edge attributes\n",
    "for i, elrow in edgelist.iterrows():\n",
    "    # (node1, node2, edge attribute dict)\n",
    "    # g.add_edge(elrow[0], elrow[1], attr_dict=elrow[2:].to_dict())  # deprecated\n",
    "    g.add_edge(elrow[0], elrow[1])\n",
    "    g[elrow[0]][elrow[1]].update(elrow[2:].to_dict())\n",
    "\n",
    "# Add node attributes\n",
    "for i, nlrow in nodelist.iterrows():\n",
    "    # g.node[nlrow['id']] = nlrow[1:].to_dict()  # deprecated\n",
    "    g.nodes[nlrow['id']].update(nlrow[1:].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preview first x edges\n",
    "list(g.edges(data=True))[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preview first x nodes\n",
    "list(g.nodes(data=True))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "print('# of edges: {}'.format(g.number_of_edges()))\n",
    "print('# of nodes: {}'.format(g.number_of_nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define node positions data structure (dict) for plotting\n",
    "# X and Y info are from dataset\n",
    "node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in g.nodes(data=True)}\n",
    "\n",
    "# Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "dict(list(node_positions.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define node labels (dict) for plotting\n",
    "# node_labels = {node: node for node in g.nodes()}\n",
    "\n",
    "c = 0\n",
    "node_labels = {}\n",
    "for node in g.nodes():\n",
    "    node_labels[node] = c\n",
    "    # Save index in node for reference in future\n",
    "    g.nodes[node]['idx'] = c\n",
    "    c+= 1\n",
    "\n",
    "# Preview of node_positions\n",
    "dict(list(node_labels.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define data structure (list) of edge colors for plotting\n",
    "edge_colors = [e[2]['color'] for e in g.edges(data=True)]\n",
    "\n",
    "# Preview first 10\n",
    "edge_colors[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "nx.draw(g, pos=node_positions, labels=node_labels, font_color='white', edge_color=edge_colors)\n",
    "# nx.draw(g, pos=node_positions, edge_color=edge_colors, node_size=10, node_color='black')\n",
    "plt.title('Graph Representation of Sleeping Giant Trail Map', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Save Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Undirected (Original)\n",
    "# nx.write_gpickle(g, 'file/sleeping_giant.gpickle')\n",
    "\n",
    "# Directed graph\n",
    "nx.write_gpickle(g.to_directed(), 'file/sleeping_giant_directed.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = nx.read_gpickle('file/sleeping_giant_directed.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Effective Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:55:53.582234Z",
     "start_time": "2021-02-23T01:55:53.524770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def effective_distance(\n",
    "        file_input=['db/wifi_raw.csv', 'db/bt_raw.csv', 'db/uiuc.db'],\n",
    "        file_output=['network/bt_temporal_network.gpickle'],\n",
    "        label_input='',\n",
    "        label_output='',\n",
    "        time_max=3600,\n",
    "        print_distance=False,\n",
    "        print_times=False,\n",
    "        save_distance=True,\n",
    "        save_times=True):\n",
    "    \"\"\"\n",
    "    Read WiFi interactions and create effective distance for\n",
    "    WiFi Access Points (AP)\n",
    "    \"\"\"\n",
    "    # Edit file names\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read WiFi data\n",
    "    data = pd.read_csv(file_input[0],\n",
    "                       header=None,\n",
    "                       names=['user', 'mac', 'time'],\n",
    "                       dtype={\n",
    "                           'user': int,\n",
    "                           'mac': int\n",
    "                       },\n",
    "                       parse_dates=['time'])\n",
    "\n",
    "    # Number of users\n",
    "    U = len(data.user.unique())\n",
    "\n",
    "    # Add filtered time as a new column to DF\n",
    "    # data['timestamp'] = data['time'].apply(lambda x: x.replace(minute=0, second=0))\n",
    "\n",
    "    # Analyze times\n",
    "    # (1)\n",
    "    times = sorted(data.time.unique())  # List\n",
    "    # (2)\n",
    "    # times = pd.Series(sorted(data.time.unique()))  # Series\n",
    "    # print('# Timestamps:', len(times))\n",
    "\n",
    "    # Plot times-frequency\n",
    "    # times_freq = data.groupby('time').size().to_frame(name='freq').reset_index()\n",
    "    # fig, ax = plt.subplots(figsize=(24, 12))\n",
    "    # ax.scatter(times_freq.time, times_freq.freq)\n",
    "    # sns.lineplot(x='time', y='freq', data=times_freq, ax=ax)\n",
    "    # sns.histplot(x='time', kde=True, data=data, ax=ax)\n",
    "    # ax.set_title('Timestamp Frequency')\n",
    "    # plt.show()\n",
    "\n",
    "    # Create location ID from observing groupd of AP at the same time\n",
    "    loc_id = 0\n",
    "    ap_loc = {}  # {Access-Point ID : Location ID}\n",
    "    loc_ap = defaultdict(set)  # {Location : {AP1, AP2, ...}}\n",
    "\n",
    "    for user in sorted(data.user.unique()):\n",
    "        for key, group in data[data.user == user].sort_values('time').groupby(\n",
    "                'time'):\n",
    "            # Check if any of AP (or MAC) are already registered in dictionary\n",
    "            aps = set(group.mac.unique())\n",
    "            ap_found = list(aps.intersection(ap_loc))\n",
    "            # Nothing found -> all AP are new\n",
    "            if len(ap_found) == 0:\n",
    "                for ap in aps:\n",
    "                    ap_loc[ap] = loc_id\n",
    "                loc_ap[loc_id].update(aps)\n",
    "                loc_id += 1\n",
    "                continue\n",
    "            # BUT if we have some are already keyed in dictionary\n",
    "            loc_found = [ap_loc[ap] for ap in ap_found]\n",
    "            # Do all the existing AP have same location ID ?\n",
    "            if len(set(\n",
    "                    loc_found)) == 0:  # if all(i == first for i in loc_found)\n",
    "                # YES\n",
    "                # Add the rest of AP (new ones) to that location ID as well\n",
    "                loc_ap[loc_found[0]].update(aps)\n",
    "                for ap in aps:\n",
    "                    ap_loc[ap] = loc_found[0]\n",
    "            else:\n",
    "                # NO\n",
    "                # Some AP have differnet locations (while they should have same)\n",
    "                # Take smallest location ID (sooner defined location ID) and set to all\n",
    "                loc_sel = min(loc_found)\n",
    "                loc_update = set(loc_found)\n",
    "                loc_update.remove(loc_sel)\n",
    "                # Fix dictionaries\n",
    "                for loc in loc_update:\n",
    "                    for ap in loc_ap[loc]:\n",
    "                        ap_loc[ap] = loc_sel\n",
    "                    loc_ap[loc_sel].update(loc_ap[loc])\n",
    "                    del loc_ap[loc]\n",
    "                # Add/update new AP\n",
    "                ap_rest = list(set(aps) - set(ap_found))\n",
    "                for ap in ap_rest:\n",
    "                    ap_loc[ap] = loc_sel\n",
    "                loc_ap[loc_sel].update(ap_rest)\n",
    "\n",
    "    # Clean-up the Location:AP dictionary\n",
    "    loc_id = 0\n",
    "    loc_ap_new = {}\n",
    "    for k, v in loc_ap.items():\n",
    "        if len(v) == 0:  # Empty set => location with no AP\n",
    "            print('location {} has no AP : {}'.format(k, v))\n",
    "        else:\n",
    "            loc_ap_new[loc_id] = v\n",
    "            for ap in v:\n",
    "                ap_loc[ap] = loc_id\n",
    "            loc_id += 1\n",
    "    loc_ap = loc_ap_new\n",
    "\n",
    "    # Save {AP : Location} and {Location : AP} dictionaries\n",
    "    with open('distance/ap_location.pickle', 'wb') as handle:\n",
    "        pickle.dump(ap_loc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('distance/location_ap.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_ap, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # We can think of APs - or Locations they represent - as Airports\n",
    "    # Calculate the number of user move from one AP to another (or from one Location to another)\n",
    "    # Think of them as flow of population between Airports\n",
    "    # Create the movement porobability matrix from the flows\n",
    "    # Also, create average distance between every pair of APs or Locations\n",
    "    # Finally, save these information in AP-network or Location-network\n",
    "\n",
    "    # Dataframe of [ source_ap | destination_ap | user | start_time | finish_time | gap ]\n",
    "    # This DF contains all the movement AP->AP (even when location is not changed) and Location->Location\n",
    "    c_sap = []\n",
    "    c_dap = []\n",
    "    c_sl = []  # Source Location\n",
    "    c_dl = []  # Destination Location\n",
    "    c_u = []\n",
    "    c_st = []\n",
    "    c_ft = []\n",
    "    c_g = []\n",
    "\n",
    "    # Note: all 3 dictionary of \"loc_loc_user\", \"loc_loc_time\", and \"loc_loc_dist\" can be calculated from DF later as well\n",
    "    # This is exactly what we do for AP-AP dictionaries, where we create them from DF (only AP-AP-Distance is needed)\n",
    "\n",
    "    # {(location, location):[user_1, user_2, ...]}\n",
    "    # Useful to calculate flow OR number of travelers between locations\n",
    "    loc_loc_user = defaultdict(list)\n",
    "    # ---\n",
    "    # {(location, location):[(time_1, time_2), ...]} which is Dictionary of List of Tuples\n",
    "    # Useful to calculate timestamp of source_destination travels\n",
    "    loc_loc_time = defaultdict(list)\n",
    "    # ---\n",
    "    # {(location, location):[travel_time_1, travel_time_2, ...]}\n",
    "    # Useful to calculate average time-distance between locations\n",
    "    loc_loc_dist = defaultdict(list)\n",
    "\n",
    "    # Time -> User -> (Location,Location)\n",
    "    tu_loc = defaultdict(lambda: defaultdict(set))\n",
    "    # tu_loc = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    # Time -> User -> (AP,AP)\n",
    "    tu_ap = defaultdict(lambda: defaultdict(set))\n",
    "    # tu_ap = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for user in sorted(data.user.unique()):\n",
    "        data_temp = data[data.user ==\n",
    "                         user].loc[:, ['mac', 'time']].sort_values('time')\n",
    "        # First timestamp of a user\n",
    "        time_1 = data_temp.iloc[0, 1]\n",
    "        # First observed MAC (of AP) of a user\n",
    "        macs_1 = list(data_temp[data_temp.time == time_1]['mac'].unique())\n",
    "        mac_1 = macs_1[0]\n",
    "        for key, group in data_temp.groupby('time'):\n",
    "            # if (key - time_1).seconds != 0:  # Assuring time has moved forward\n",
    "            macs_2 = list(group.mac.unique())\n",
    "            mac_2 = macs_2[0]\n",
    "            # IF user has moved to a new location ...\n",
    "            if ap_loc[mac_1] != ap_loc[mac_2]:\n",
    "                loc_loc_user[(ap_loc[mac_1], ap_loc[mac_2])].append(user)\n",
    "                loc_loc_time[(ap_loc[mac_1], ap_loc[mac_2])].append(\n",
    "                    (time_1, key))\n",
    "                loc_loc_dist[(ap_loc[mac_1], ap_loc[mac_2])].append(\n",
    "                    (key - time_1).seconds)\n",
    "                # Add movement to Time-User-AP dictionary\n",
    "                # We can add the condition of if time-gap is less than one hour\n",
    "                # if (key - time_1).seconds <= time_max:\n",
    "                tu_loc[time_1.replace(minute=0, second=0)][user].add(\n",
    "                    (ap_loc[mac_1], ap_loc[mac_2]))\n",
    "            # Despite the movement from on location to another\n",
    "            # Record AP->AP movement in DF\n",
    "            # (1)\n",
    "            # macs_3 = [macs_1, macs_2]\n",
    "            # combs = [p for p in itertools.product(*macs_3)]\n",
    "            # (2)\n",
    "            for m1 in macs_1:\n",
    "                for m2 in macs_2:\n",
    "                    c_sap.append(m1)\n",
    "                    c_dap.append(m2)\n",
    "                    c_sl.append(ap_loc[m1])\n",
    "                    c_dl.append(ap_loc[m2])\n",
    "                    c_u.append(user)\n",
    "                    c_st.append(time_1)\n",
    "                    c_ft.append(key)\n",
    "                    c_g.append((key - time_1).seconds)\n",
    "                    # Add movement to Time-User-Location dictionary\n",
    "                    tu_ap[time_1.replace(minute=0, second=0)][user].add(\n",
    "                        (m1, m2))\n",
    "            time_1 = key\n",
    "            macs_1 = macs_2[:]\n",
    "            mac_1 = macs_1[0]\n",
    "\n",
    "    # Create dataframe of all movements (and stay or hold)\n",
    "    df = pd.DataFrame(list(zip(c_sap, c_dap, c_sl, c_dl, c_u, c_st, c_ft,\n",
    "                               c_g)),\n",
    "                      columns=['sap', 'dap', 'sl', 'dl', 'u', 'st', 'ft', 'g'])\n",
    "\n",
    "    # Save movements dataframe\n",
    "    df.to_csv('distance/movement.csv', header=False, index=False)\n",
    "\n",
    "    # Read movements dataframe from file and convert datetime columns\n",
    "    # df = pd.read_csv('distance/movement.csv', header=None, names=['sap', 'dap', 'sl', 'dl', 'u', 'st', 'ft', 'g'], parse_dates=['st', 'ft'])\n",
    "    \n",
    "    tut = {}\n",
    "    for k1 in tu_ap:  # k1 = times\n",
    "        user_dict = {}\n",
    "        for k2 in tu_ap[k1]:  # k2 = users\n",
    "            move_set = set()\n",
    "            for k3 in tu_ap[k1][k2]:  # k3 = (ap,ap)\n",
    "                move_set.add(k3)\n",
    "            user_dict[k2] = move_set\n",
    "        tut[k1] = user_dict\n",
    "    tu_ap = tut\n",
    "    \n",
    "    # Save Time-User-AP-Movement dictionary\n",
    "    with open('distance/time_user_ap.pickle', 'wb') as handle:\n",
    "        pickle.dump(tu_ap, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    tut = {}\n",
    "    for k1 in tu_loc:  # k1 = times\n",
    "        user_dict = {}\n",
    "        for k2 in tu_loc[k1]:  # k2 = users\n",
    "            move_set = set()\n",
    "            for k3 in tu_loc[k1][k2]:  # k3 = (location,location)\n",
    "                move_set.add(k3)\n",
    "            user_dict[k2] = move_set\n",
    "        tut[k1] = user_dict\n",
    "    tu_loc = tut\n",
    "    \n",
    "    # Save Time-User-Location-Movement dictionary\n",
    "    with open('distance/time_user_location.pickle', 'wb') as handle:\n",
    "        pickle.dump(tu_loc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Save location dictionaries\n",
    "    with open('distance/location_distance.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_loc_dist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('distance/location_user.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_loc_user, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('distance/location_time.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_loc_time, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Save only location-distance dictionary as csv file\n",
    "    # with open('distance/location_distance.csv', 'w') as csv_file:\n",
    "    # writer = csv.writer(csv_file)\n",
    "    # for key, value in loc_loc_dist.items():\n",
    "    # writer.writerow([*key, value])\n",
    "\n",
    "    # DF for time where users staty in one place (i.e. AP) or moved\n",
    "    # df_stay = df[df.sap == df.dap]\n",
    "    # df_move = df[df.sap != df.dap]\n",
    "\n",
    "    # ----- FLOW -----\n",
    "\n",
    "    # Create dictinary of flow from AP to another AP {(AP1,AP2):Flow}\n",
    "    # (1) freq : total flow or repeated users travels\n",
    "    # (2) user : unique number of users' flow between APs\n",
    "    ap_flow_freq = {}\n",
    "    ap_flow_user = {}\n",
    "    apff_in = {}\n",
    "    apfu_in = {}\n",
    "    apff_out = {}\n",
    "    apfu_out = {}\n",
    "\n",
    "    for key, group in df[df.sap == df.dap].groupby(['sap', 'dap']):\n",
    "        ap_flow_freq[key] = len(group)\n",
    "        ap_flow_user[key] = len(group.u.unique())\n",
    "\n",
    "    # Unique number of users that have moved between APs\n",
    "    # U = len(df[df.sap != df.dap].u.unique())\n",
    "\n",
    "    for key, group in df[df.sap != df.dap].groupby(['sap', 'dap']):\n",
    "        val_1 = len(group)\n",
    "        val_2 = len(group.u.unique())\n",
    "        ap_flow_freq[key] = val_1\n",
    "        ap_flow_user[key] = val_2\n",
    "        # Sum of incoming flow to each AP or location\n",
    "        # Equals to column sum of flow matrix [A -> B]\n",
    "        apff_in[key[1]] = val_1 + apff_in.get(key[1], 0)\n",
    "        apfu_in[key[1]] = val_2 + apfu_in.get(key[1], 0)\n",
    "        # Sum of outgoing flow from each AP or location\n",
    "        apff_out[key[0]] = val_1 + apff_out.get(key[0], 0)\n",
    "        apfu_out[key[0]] = val_2 + apfu_out.get(key[0], 0)\n",
    "        # Create Ap -> AP temporal distance\n",
    "\n",
    "    # Normalize flow values\n",
    "    for key in ap_flow_freq:\n",
    "        if key[0] == key[1]:\n",
    "            # Ratio of all the stay logs over all travel flows (incoming + outgoing)\n",
    "            val_3 = apff_in.get(key[0], 0) + apff_out.get(key[0], 0)\n",
    "            val_4 = apfu_in.get(key[0], 0) + apfu_out.get(key[0], 0)\n",
    "            ap_flow_freq[(key[0], key[1])] /= val_3\n",
    "            # ap_flow_user[(key[0], key[1])] /= val_4\n",
    "            ap_flow_user[(key[0], key[1])] /= U\n",
    "        else:\n",
    "            val_3 = apff_in.get(key[1], 0)\n",
    "            val_4 = apfu_in.get(key[1], 0)\n",
    "            ap_flow_freq[(key[0], key[1])] /= val_3\n",
    "            # ap_flow_user[(key[0], key[1])] /= val_4\n",
    "            ap_flow_user[(key[0], key[1])] /= U\n",
    "\n",
    "    with open('distance/ap_flow_freq.pickle', 'wb') as handle:\n",
    "        pickle.dump(ap_flow_freq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('distance/ap_flow_user.pickle', 'wb') as handle:\n",
    "        pickle.dump(ap_flow_user, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Location-Location-Flow dictionary\n",
    "    loc_flow_freq = {}\n",
    "    loc_flow_user = {}\n",
    "    locff_in = {}\n",
    "    locfu_in = {}\n",
    "    locff_out = {}\n",
    "    locfu_out = {}\n",
    "\n",
    "    for key, group in df[df.sl == df.dl].groupby(['sl', 'dl']):\n",
    "        loc_flow_freq[key] = len(group)\n",
    "        loc_flow_user[key] = len(group.u.unique())\n",
    "\n",
    "    # Unique number of users that have moved between locations\n",
    "    # U = len(df[df.sl != df.dl].u.unique())\n",
    "\n",
    "    for key, group in df[df.sl != df.dl].groupby(['sl', 'dl']):\n",
    "        val_1 = len(group)\n",
    "        val_2 = len(group.u.unique())\n",
    "        loc_flow_freq[key] = val_1\n",
    "        loc_flow_user[key] = val_2\n",
    "        locff_in[key[1]] = val_1 + locff_in.get(key[1], 0)\n",
    "        locfu_in[key[1]] = val_2 + locfu_in.get(key[1], 0)\n",
    "        locff_out[key[0]] = val_1 + locff_out.get(key[0], 0)\n",
    "        locfu_out[key[0]] = val_2 + locfu_out.get(key[0], 0)\n",
    "\n",
    "    for key in loc_flow_freq:\n",
    "        if key[0] == key[1]:\n",
    "            val_3 = locff_in.get(key[0], 0) + locff_out.get(key[0], 0)\n",
    "            val_4 = locfu_in.get(key[0], 0) + locfu_out.get(key[0], 0)\n",
    "            loc_flow_freq[(key[0], key[1])] /= val_3\n",
    "            # loc_flow_user[(key[0], key[1])] /= val_4\n",
    "            loc_flow_user[(key[0], key[1])] /= U\n",
    "        else:\n",
    "            val_3 = locff_in.get(key[1], 0)\n",
    "            val_4 = locfu_in.get(key[1], 0)\n",
    "            loc_flow_freq[(key[0], key[1])] /= val_3\n",
    "            # loc_flow_user[(key[0], key[1])] /= val_4\n",
    "            loc_flow_user[(key[0], key[1])] /= U\n",
    "\n",
    "    with open('distance/location_flow_freq.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_flow_freq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('distance/location_flow_user.pickle', 'wb') as handle:\n",
    "        pickle.dump(loc_flow_user, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # TODO : We can visualize the values of AP and Location flows\n",
    "    # Show how much is difference between normalize value of frequency and unique user's flow\n",
    "\n",
    "    # ----- GRAPH -----\n",
    "\n",
    "    # AP Graph (ap -> ap, weighted by distance and flow)\n",
    "    graph_ap = nx.DiGraph()\n",
    "\n",
    "    # AP-AP-Distance\n",
    "    ap_ap_dist = {}\n",
    "\n",
    "    for key, group in df[df.sap != df.dap].groupby(['sap', 'dap']):\n",
    "        # gaps = [e if e <= time_max else time_max for e in group.g]\n",
    "        gaps = [e for e in group.g if e <= time_max]\n",
    "        # If all of time distance are more than time_max (e.g. 1 hour)\n",
    "        if len(gaps) == 0:\n",
    "            # (1)\n",
    "            # Just add 1 time_max entry\n",
    "            # gaps.append(time_max)\n",
    "            # (2)\n",
    "            # Add 1 time_max IF ...\n",
    "            # The user was at a AP then moved home (turned Wi-Fi off - sometimes before 12 am) and turned it on in the morning (7 am)\n",
    "            if len(group) == 1 and (list(group.ft)[0].dayofyear -\n",
    "                                    list(group.st)[0].dayofyear) == 1 and list(\n",
    "                                        group.ft)[0].hour == 7:\n",
    "                # if len(group) == 1 and (list(group.ft)[0].dayofyear - list(group.st)[0].dayofyear) == 1:\n",
    "                # if len(group) == 1 and list(group.st)[0].hour == 22 and list(group.ft)[0].hour == 7:\n",
    "                gaps.append(time_max)\n",
    "            # OR - preserve the original time distance > time_max (e.g. 1 hour)\n",
    "            # gaps = [e for e in group.g]\n",
    "            else:\n",
    "                # If several time distance > time_max => just ignore this movement and consider it as noise in data\n",
    "                continue\n",
    "                # OR\n",
    "                if len(group.u.unique()) == 1:  # Noise\n",
    "                    continue\n",
    "                else:\n",
    "                    # Multiple user recorded more that 1-hour travel time -> register 1-hour as the maximum possible time travel\n",
    "                    gaps.append(time_max)\n",
    "        # Calculate mean of recorded time distances and add to dictionary\n",
    "        ap_ap_dist[key] = int(np.mean(gaps))\n",
    "        # Add the AP graph edge\n",
    "        graph_ap.add_edge(key[0],\n",
    "                          key[1],\n",
    "                          distance=ap_ap_dist.get(key, 0),\n",
    "                          flow=ap_flow_freq.get(key, 0),\n",
    "                          uflow=ap_flow_user.get(key, 0))\n",
    "\n",
    "    # Save graph\n",
    "    nx.write_gpickle(graph_ap, 'distance/ap_network.gpickle')\n",
    "\n",
    "    # Visualize graph\n",
    "    # plt.figure(figsize=(16, 12))\n",
    "    # nx.draw(graph_ap)\n",
    "    # plt.show()\n",
    "\n",
    "    with open('distance/ap_distance.pickle', 'wb') as handle:\n",
    "        pickle.dump(ap_ap_dist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # Location Graph (location -> location, weighted by distance)\n",
    "    # If we set \"time_max = 3600\" or 1 hour then the maximum time distance we allow by default is 1 hour\n",
    "    graph_loc = nx.DiGraph()\n",
    "    l1, l2 = map(list, zip(*loc_loc_dist.keys()))\n",
    "    nodes = set(l1).union(l2)\n",
    "    graph_loc.add_nodes_from(nodes)\n",
    "    for k, v in loc_loc_dist.items():\n",
    "        if len(v) > 1:  # If more than one time distance\n",
    "            time_temp = [i if i <= time_max else time_max for i in v]\n",
    "            loc_loc_dist[k] = int(np.mean(time_temp))\n",
    "        else:  # Contains only 1 time distance\n",
    "            if v[0] >= time_max:\n",
    "                loc_loc_dist[k] = time_max\n",
    "            else:\n",
    "                loc_loc_dist[k] = v[0]\n",
    "        graph_loc.add_edge(k[0],\n",
    "                           k[1],\n",
    "                           distance=loc_loc_dist.get(k, 0),\n",
    "                           flow=loc_flow_freq.get(k, 0),\n",
    "                           uflow=loc_flow_user.get(k, 0))\n",
    "\n",
    "    # Save graph\n",
    "    nx.write_gpickle(graph_loc, 'distance/location_network.gpickle')\n",
    "\n",
    "    # Visualize graph\n",
    "    # plt.figure(figsize=(16, 12))\n",
    "    # nx.draw(graph_loc)\n",
    "    # plt.show()\n",
    "\n",
    "    # return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:55:55.206058Z",
     "start_time": "2021-02-23T01:55:55.195504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def time_user_ap_movement(file_input=[\n",
    "    'distance/time_user_ap.pickle', 'distance/ap_distance.pickle',\n",
    "    'distance/ap_flow_freq.pickle', 'distance/ap_flow_user.pickle'\n",
    "],\n",
    "                          file_output=[''],\n",
    "                          label_input='',\n",
    "                          label_output='',\n",
    "                          print_tua=False):\n",
    "    \"\"\"\n",
    "    Read movements of users from AP to AP on each time step as a dictionary of\n",
    "    { Time : User : (AP , AP) }\n",
    "        Note that time is only hour of movement (minute and seconds are zero) and it may not be\n",
    "        in the set of timestamps in temporal network (users have moved but did not make any contact)\n",
    "    \"\"\"\n",
    "\n",
    "    tua = {}\n",
    "    with open(file_input[0], 'rb') as handle:\n",
    "        tua = pickle.load(handle)\n",
    "\n",
    "    # Print info about movement dictionary\n",
    "    if print_tua:\n",
    "        print('# times =', len(tua.keys()), '\\n---')\n",
    "        for k in tua:\n",
    "            print(k, end='\\t')\n",
    "            print('# users =', len(tua[k].items()))\n",
    "\n",
    "    ad = {}\n",
    "    apf = {}\n",
    "    apu = {}\n",
    "    # with open(file_input[1], 'rb') as handle:\n",
    "    # apf = pickle.load(handle)\n",
    "    # with open(file_input[2], 'rb') as handle:\n",
    "    # apf = pickle.load(handle)\n",
    "    # with open(file_input[3], 'rb') as handle:\n",
    "    # apf = pickle.load(handle)\n",
    "\n",
    "    return tua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T17:44:49.482271Z",
     "start_time": "2021-01-26T17:39:22.352982Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# effective_distance(print_distance=False,\n",
    "#                    print_times=False,\n",
    "#                    save_distance=True,\n",
    "#                    save_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-29T01:51:22.004782Z",
     "start_time": "2021-01-29T01:51:22.001026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tau = time_user_ap_movement()\n",
    "# tau = time_user_ap_movement(print_tua=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The result is **5912** access points and **145** locations ...    \n",
    "While users moved between **238** pairs of locations (x -> y) resulting in Location-location network with 248 edges and 145 nodes.    \n",
    "Dataframe of AP -> AP movement has following columns: source AP, destination AP, source Location, destination Location, starting Time, finishing Time, Gap, User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "wPcoh-nXQ4jo"
   },
   "source": [
    "# Temporal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:56:18.270430Z",
     "start_time": "2021-02-23T01:56:18.225339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def temporal_bt_create(\n",
    "    file_input=['db/bt.csv', 'db/uiuc.db'],\n",
    "    file_output=[\n",
    "        'network/bt_temporal_network.gpickle',\n",
    "        'network/bt_bipartite_network.gpickle',\n",
    "        'network/bt_temporal_edgelist.csv',\n",
    "        'network/bt_bipartite_edgelist.csv', 'network/bt_temporal_times.csv',\n",
    "        'network/bt_bipartite_times.csv', 'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_bipartite_nodes.csv', 'network/bt_temporal_weights.csv',\n",
    "        'network/bt_bipartite_weights.csv',\n",
    "        'network/bt_temporal_weights_scaled.csv',\n",
    "        'network/bt_bipartite_weights_scaled.csv'\n",
    "    ],\n",
    "    input_label='',\n",
    "    output_label='',\n",
    "    output_times=False,\n",
    "    output_network=True,\n",
    "    save_times=True,\n",
    "    save_nodes=True,\n",
    "    save_weights=True,\n",
    "    save_network_db=True,\n",
    "    save_network_csv=True,\n",
    "    save_network_file=True):\n",
    "    \"\"\"\n",
    "    Read CSV dataset with 3 columns of: node-1, node-2, timestamp\n",
    "    Then create an (aggregated) temporal network using Multiedge Directed Graph\n",
    "    \"\"\"\n",
    "\n",
    "    # Modify input & output file names\n",
    "    file_input = label_amend(file_input, input_label)\n",
    "    file_output = label_amend(file_output, output_label)\n",
    "\n",
    "    # Create empty network\n",
    "    graph = nx.MultiDiGraph()  # G: user-user interaction network\n",
    "    bipartite = nx.MultiDiGraph()  # B: (28)user-others biparite network\n",
    "\n",
    "    # Read the dataset\n",
    "    print('Reading dataset ... ')\n",
    "    data = pd.read_csv(file_input[0],\n",
    "                       header=None,\n",
    "                       names=['user', 'mac', 'time'],\n",
    "                       parse_dates=['time'])\n",
    "    print('Done!\\n')\n",
    "\n",
    "    # Print Time : Frequency\n",
    "    if output_times:\n",
    "        times = pd.Series(sorted(data.time.unique()))\n",
    "        print(len(times), 'timestamps')\n",
    "        print('Timestamp : Frequency')\n",
    "        for t_size in data.groupby('time').size().iteritems():\n",
    "            print('{}) {} : {}'.format(times[times == t_size[0]].index[0],\n",
    "                                       t_size[0], t_size[1]))\n",
    "        print()\n",
    "\n",
    "    # Create timestamp list\n",
    "    times = []\n",
    "    times_bipartite = []\n",
    "\n",
    "    # Dictionary {time:{(user-1,user-2):weight}}\n",
    "    time_edges = defaultdict()  # User -> User\n",
    "    time_bipartite_edges = defaultdict()  # Users -> other devices\n",
    "\n",
    "    # Group interactions by time\n",
    "    for key_time, group_user_mac in data[['user',\n",
    "                                          'mac']].groupby(data['time']):\n",
    "        # print()\n",
    "        # print('Time:\\n', key_time)\n",
    "        # print('Group:\\n', group_user_mac)\n",
    "\n",
    "        # Normal (co-location) graph edges in filtered timestamp\n",
    "        temp_edges = []\n",
    "        for key_mac, group_connection in group_user_mac.groupby(['mac'\n",
    "                                                                 ])['user']:\n",
    "            # print('Mac:', key_mac)\n",
    "            # print('Group:', group_connection)\n",
    "\n",
    "            # Users of connecting to filtered MAC\n",
    "            temp_users = list(group_connection.unique())\n",
    "            # print('Unique users:', temp_users)\n",
    "\n",
    "            # If the ID of shared connected mac is [0-27]\n",
    "            # Users directly connect to another bluetooth user\n",
    "            if key_mac < 28:\n",
    "                # Case where only one direct connection: user -> user\n",
    "                if len(temp_users) <= 1:\n",
    "                    temp_edges.append((temp_users[0], key_mac))\n",
    "                    # Comment next line, if wanna have directed edges\n",
    "                    temp_edges.append((key_mac, temp_users[0]))\n",
    "                else:\n",
    "                    # Case where more than 1 user undirectly connect together via 1 direct user -> user edge\n",
    "                    # Direct edges\n",
    "                    for element in temp_users:\n",
    "                        temp_edges.append((element, key_mac))\n",
    "                        # Uncomment next line, if wanna have undirected edges when one user observe another user directly\n",
    "                        temp_edges.append((key_mac, element))\n",
    "                    # Undirect edges\n",
    "                    connected_users = list(permutations(temp_users, 2))\n",
    "                    connected_users = [tuple(e) for e in connected_users]\n",
    "                    temp_edges.extend(connected_users)\n",
    "            # If users are connected to device with ID > 28\n",
    "            # Meaning indirect edges with each other\n",
    "            else:\n",
    "                # Only consider cases of more than 1 unique user for co-location indirected edges\n",
    "                if len(temp_users) > 1:\n",
    "                    # Undirect edges\n",
    "                    connected_users = list(permutations(temp_users, 2))\n",
    "                    connected_users = [tuple(e) for e in connected_users]\n",
    "                    temp_edges.extend(connected_users)\n",
    "\n",
    "        # Add edges of current timestamp (with their strength) to dictionary\n",
    "        if len(temp_edges) > 0:\n",
    "            time_edges[key_time] = dict(Counter(temp_edges))\n",
    "\n",
    "        # Bipartite graph edges\n",
    "        # We don't care about MAC < 28, just want to count\n",
    "        # How many times in each timestamp a user connect to a MAC\n",
    "        # Dictionary {time:{(user,mac):weight}}\n",
    "        bipartite_edges = {}\n",
    "        # Filter connections based on (user -> mac) interaction and its weight\n",
    "        # for key_mac, group_connection in group_user_mac.groupby(['mac'])['user']:\n",
    "        for key_mac, group_connection in group_user_mac.groupby(\n",
    "            ['mac', 'user']):\n",
    "            # User connected to filtered MAC with X number of times\n",
    "            # print('{}\\t{}'.format(key_mac, len(group_connection)))\n",
    "            bipartite_edges[key_mac] = len(group_connection)\n",
    "\n",
    "        # Add edges of this time (with their strength) to dictionary\n",
    "        time_bipartite_edges[key_time] = bipartite_edges\n",
    "\n",
    "    # Normal (co-location) network\n",
    "    l1, l2, l3, l4 = [], [], [], []  # time, node, node, weight\n",
    "    for k1, v1 in time_edges.items():\n",
    "        for k2, v2 in v1.items():  # k2 = edge = (u,v)\n",
    "            if k2[0] != k2[1]:\n",
    "                l1.append(k1)\n",
    "                l2.append(k2[0])\n",
    "                l3.append(k2[1])\n",
    "                l4.append(v2)\n",
    "    data_graph = pd.DataFrame(list(zip(l1, l2, l3, l4)),\n",
    "                              columns=['t', 'u', 'v', 'w'])\n",
    "\n",
    "    # Weights\n",
    "    # Scale the weights of connection [0-1]\n",
    "    X = [[entry] for entry in data_graph['w']]\n",
    "    if save_weights: np.savetxt(file_output[8], X, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Plot the distribution of original weights\n",
    "    plt.figure()\n",
    "    # ax = sns.distplot(X, bins=10)\n",
    "    ax = sns.distplot(X,\n",
    "                      bins=max(X),\n",
    "                      kde=True,\n",
    "                      hist_kws={\n",
    "                          \"linewidth\": 15,\n",
    "                          'alpha': 1\n",
    "                      })\n",
    "    ax.set(xlabel='Distribution', ylabel='Frequency')\n",
    "\n",
    "    # Max-Min Normalizer (produce many zeros)\n",
    "    # transformer = MinMaxScaler()\n",
    "    # X_scaled = transformer.fit_transform(X)\n",
    "    # Returning column to row vector again\n",
    "    # X_scaled = [entry[0] for entry in X_scaled]\n",
    "\n",
    "    # Quantile normalizer (normal distribution)\n",
    "    # transformer = QuantileTransformer()\n",
    "    # Quantile normalizer (uniform distribution)\n",
    "    transformer = QuantileTransformer(n_quantiles=1000,\n",
    "                                      output_distribution='uniform')\n",
    "    X_scaled = transformer.fit_transform(X)\n",
    "    X_scaled = [entry[0] for entry in X_scaled]\n",
    "\n",
    "    # Normalize by dividing to max\n",
    "    # X_max = max(data_graph['w'])\n",
    "    # X_scaled = [entry[0] / X_max for entry in X]\n",
    "\n",
    "    # Fixing 0's and 1's entries\n",
    "    # X_scaled = [entry if entry != 1 else 0.99 for entry in X_scaled]\n",
    "    # X_scaled = [entry if entry > 0 else 0.1 for entry in X_scaled]\n",
    "    \n",
    "    # Scale everything between [a,b] or [0.5,1]\n",
    "    # Because we do not want these weight become less than temporal weights\n",
    "    # X_scaled = (b - a) * ((X_scaled - min(X_scaled)) / (max(X_scaled) - min(X_scaled))) + a\n",
    "    X_scaled = (0.5 * np.array(X_scaled)) + 0.5\n",
    "    \n",
    "    # Rounding to X decimal point\n",
    "    # X_scaled = [round(entry, 2) for entry in X_scaled]  # List\n",
    "    X_scaled = np.round(X_scaled, 2)  # Array\n",
    "\n",
    "    # Plot the distribution of scaled weights\n",
    "    plt.figure()\n",
    "    # ax = sns.distplot(X_scaled, bins=10)\n",
    "    ax = sns.distplot(X_scaled, bins=10, kde=True, hist_kws={'alpha': 1})\n",
    "    ax.set(xlabel='Distribution ', ylabel='Frequency')\n",
    "\n",
    "    # Save scaled weights back to DF\n",
    "    data_graph['w'] = X_scaled\n",
    "\n",
    "    # Save the new weights\n",
    "    if save_weights:\n",
    "        np.savetxt(file_output[10], X_scaled, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Save to DB\n",
    "    if save_network_db:\n",
    "        data_graph[['u', 'v', 't', 'w']].to_sql(name='bluetooth_edgelist',\n",
    "                                                con=db_connect(file_input[1]),\n",
    "                                                if_exists='replace',\n",
    "                                                index_label='id')\n",
    "\n",
    "    # Save to file\n",
    "    if save_network_csv:\n",
    "        data_graph[['u', 'v', 't', 'w']].to_csv(file_output[2],\n",
    "                                                header=False,\n",
    "                                                index=False)\n",
    "\n",
    "    # Add edges to network object\n",
    "    # Complete network object\n",
    "    for row in data_graph.itertuples(index=True, name='Pandas'):\n",
    "        graph.add_edge(getattr(row, 'u'),\n",
    "                       getattr(row, 'v'),\n",
    "                       t=getattr(row, 't'),\n",
    "                       w=getattr(row, 'w'))\n",
    "\n",
    "    # Save graph\n",
    "    if save_network_file:\n",
    "        nx.write_gpickle(graph, file_output[0])\n",
    "\n",
    "    # Save timestamps\n",
    "    if save_times:\n",
    "        np.savetxt(file_output[4] + '.csv', times, delimiter=',', fmt='%s')\n",
    "        # pd.DataFrame(sorted(list(times))).to_csv(file_output[4], header=None, index=False)\n",
    "        times_set = set()\n",
    "        for u, v, w in graph.edges(data=True):\n",
    "            times_set.add(w['t'])\n",
    "        times = pd.Series(sorted(list(times_set)))\n",
    "        np.savetxt(file_output[4], times, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Save nodes\n",
    "    if save_nodes:\n",
    "        # List of nodes in the graph\n",
    "        nodes = pd.Series(sorted(list(graph.nodes)))\n",
    "        # Save node list in a file \"node.csv\"\n",
    "        np.savetxt(file_output[6], nodes, delimiter=',', fmt='%s')\n",
    "        # pd.DataFrame(sorted(list(times))).to_csv(file_output[6], header=None, index=False)\n",
    "\n",
    "    # Bipartite network\n",
    "    # Complete network object\n",
    "    l1, l2, l3, l4 = [], [], [], []\n",
    "    for k1, v1 in time_bipartite_edges.items():\n",
    "        for k2, v2 in v1.items():  # k2 = edge = (u,v)\n",
    "            if k2[0] != k2[1]:\n",
    "                l1.append(k1)\n",
    "                l2.append(k2[0])\n",
    "                l3.append(k2[1])\n",
    "                l4.append(v2)\n",
    "    data_bi_graph = pd.DataFrame(list(zip(l1, l2, l3, l4)),\n",
    "                                 columns=['t', 'u', 'v', 'w'])\n",
    "\n",
    "    # Weights\n",
    "    X = [[entry] for entry in data_bi_graph['w']]\n",
    "    if save_weights: np.savetxt(file_output[9], X, delimiter=',', fmt='%s')\n",
    "    transformer = QuantileTransformer(n_quantiles=100,\n",
    "                                      output_distribution='uniform')\n",
    "    X_scaled = transformer.fit_transform(X)\n",
    "    X_scaled = [entry[0] for entry in X_scaled]\n",
    "    if save_weights:\n",
    "        np.savetxt(file_output[11], X_scaled, delimiter=',', fmt='%s')\n",
    "    # data_bi_graph['w'] = X_scaled\n",
    "\n",
    "    # Save to DB\n",
    "    if save_network_db:\n",
    "        data_bi_graph[['u', 'v', 't',\n",
    "                       'w']].to_sql(name='bluetooth_bipartite_edgelist',\n",
    "                                    con=db_connect(file_input[1]),\n",
    "                                    if_exists='replace',\n",
    "                                    index_label='id')\n",
    "\n",
    "    # Save to file\n",
    "    if save_network_csv:\n",
    "        data_bi_graph[['u', 'v', 't', 'w']].to_csv(file_output[3],\n",
    "                                                   header=False,\n",
    "                                                   index=False)\n",
    "\n",
    "    # Add nodes and then edges\n",
    "    # We need to add a prefix \"u_\" for users & \"b_\" for BT devices to the node id\n",
    "    # So that we can distinguish them from each others\n",
    "    for row in data_bi_graph.itertuples(index=True, name='Pandas'):\n",
    "        # In bluetooth connections, user devices ID are repeated in all BT devices\n",
    "        # So there is no need to differentiate between them, unless for some research necessary\n",
    "        bipartite.add_edge(getattr(row, 'u'),\n",
    "                           getattr(row, 'v'),\n",
    "                           t=getattr(row, 't'),\n",
    "                           w=getattr(row, 'w'))\n",
    "        # Uncomment next 5 lines if wanna difrentiate users from other devices\n",
    "        # node_user = 'u_' + str(getattr(row, 'u'))\n",
    "        # node_ap = 'b_' + str(getattr(row, 'v'))\n",
    "        # bipartite.add_node(node_user, bipartite=0)\n",
    "        # bipartite.add_node(node_ap, bipartite=1)\n",
    "        # bipartite.add_edge(node_user, node_ap, t=getattr(row, 't'), w=getattr(row, 'w'))\n",
    "\n",
    "    # Save graph\n",
    "    if save_network_file:\n",
    "        nx.write_gpickle(bipartite, file_output[1])\n",
    "\n",
    "    # Save timestamps\n",
    "    if save_times:\n",
    "        times_set = set()\n",
    "        for u, v, w in bipartite.edges(data=True):\n",
    "            times_set.add(w['t'])\n",
    "        times_bipartite = pd.Series(sorted(list(times_set)))\n",
    "        np.savetxt(file_output[5], times_bipartite, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Save nodes\n",
    "    if save_nodes:\n",
    "        # List of nodes in the bipartite graph\n",
    "        nodes = pd.Series(sorted(list(bipartite.nodes)))\n",
    "        # Save node list in a file \"node.csv\"\n",
    "        np.savetxt(file_output[7], nodes, delimiter=',', fmt='%s')\n",
    "        # pd.DataFrame(sorted(list(times))).to_csv(file_input[2], header=None, index=False)\n",
    "\n",
    "    # Print network statistics\n",
    "    if output_network:\n",
    "        print('Temporal netwrok:')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        print('T =', len(times))\n",
    "        print('---')\n",
    "        print('Bipartite Temporal netwrok:')\n",
    "        print('N =', bipartite.number_of_nodes())\n",
    "        print('L =', bipartite.number_of_edges())\n",
    "        print('T =', len(times_bipartite))\n",
    "\n",
    "    return graph\n",
    "    # return graph, bipartite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:56:22.496638Z",
     "start_time": "2021-02-23T01:56:22.479796Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def temporal_bt_read(\n",
    "    file_input=['network/bt_temporal_network.gpickle'],\n",
    "    label_input='',\n",
    "    output=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads temporal graph\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Network\n",
    "    graph = nx.MultiDiGraph()\n",
    "    # Read from file\n",
    "    if os.path.exists(file_input[0]):\n",
    "        if output: print('Reading temporal network ...')\n",
    "        graph = nx.read_gpickle(file_input[0])\n",
    "    else:\n",
    "        if output: print('Temporal network file was not found')\n",
    "        return None\n",
    "    # Print graph statistics\n",
    "    if output: print(nx.info(graph))\n",
    "    return graph\n",
    "\n",
    "\n",
    "def temporal_bt_times_read(\n",
    "    file_input=['network/bt_temporal_times.csv'],\n",
    "    label_input='',\n",
    "    output=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads timestamps of temporal graph\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Times\n",
    "    times = []\n",
    "    # Read from file\n",
    "    if os.path.exists(file_input[0]):\n",
    "        if output: print('Reading times ...')\n",
    "        times = pd.read_csv(\n",
    "            file_input[0], index_col=False, header=None, names=['times']\n",
    "        ).iloc[:, 0]\n",
    "    else:\n",
    "        if output: print('Times file was not found')\n",
    "        return None\n",
    "    # Change type (str -> datetime)\n",
    "    times = times.astype('datetime64[ns]')\n",
    "    return times\n",
    "\n",
    "\n",
    "def temporal_bt_nodes_read(\n",
    "    file_input=['network/bt_temporal_nodes.csv'],\n",
    "    label_input='',\n",
    "    output=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads nodes of temporal graph\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Nodes\n",
    "    nodes = []\n",
    "    # Read from file\n",
    "    if os.path.exists(file_input[0]):\n",
    "        if output: print('Reading nodes ...')\n",
    "        nodes = pd.read_csv(\n",
    "            file_input[0], index_col=False, header=None, names=['nodes']\n",
    "        ).iloc[:, 0]\n",
    "    else:\n",
    "        if output: print('Nodes file was not found')\n",
    "        return None\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# T = temporal_bt_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T19:57:48.209541Z",
     "start_time": "2021-02-20T19:57:48.205594Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# T = temporal_bt_create(\n",
    "#     output_times=False,\n",
    "#     output_network=True,\n",
    "#     save_times=False,\n",
    "#     save_nodes=False,\n",
    "#     save_weights=False,\n",
    "#     save_network_db=False,\n",
    "#     save_network_csv=False,\n",
    "#     save_network_file=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T19:58:36.267601Z",
     "start_time": "2021-02-20T19:58:36.263693Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# T = temporal_bt_read(output_network=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For now, we are normalizing the link weight over all of them possible weight (not focusing on time of the connection or the end nodes of a pair in the connection)    \n",
    "But a future goal is to normalize on the both of time dimension and the pair-wise nodes ... Therefore we decide to not to normalize in aggregate temporal network step, rather do this later over the Directed-time-series network creation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Static Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def static_bt_create(\n",
    "    file_input=[\n",
    "        'network/bt_temporal_network.gpickle', 'network/bt_temporal_times.csv'\n",
    "    ],\n",
    "    file_output=[\n",
    "        'network/bt_static_network.gpickle', 'network/bt_static_edgelist.csv'\n",
    "    ],\n",
    "    input_label='',\n",
    "    output_label='',\n",
    "    temporal=None,\n",
    "    undirected=True,\n",
    "    output_network=False,\n",
    "    save_network_csv=False,\n",
    "    save_network_file=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert input temporal network to the (aggregated) static network\n",
    "    the edge weights are sum of temporal interactions over entire time window\n",
    "    \"\"\"\n",
    "\n",
    "    # Modify input & output file names\n",
    "    file_input = label_amend(file_input, input_label)\n",
    "    file_output = label_amend(file_output, output_label)\n",
    "\n",
    "    # Read temporal network from file\n",
    "    times = []\n",
    "    if temporal is None:\n",
    "        temporal, times = temporal_bt_read(\n",
    "            file_input=[file_input[0], file_input[1]])\n",
    "\n",
    "    # Create empty static network\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Update static network edges\n",
    "    for u, v, data in temporal.edges(data=True):\n",
    "        t = 1 if 't' in data else 0\n",
    "        if graph.has_edge(u, v):\n",
    "            graph[u][v]['w'] += t\n",
    "            graph[u][v]['s'] = graph[u][v]['s'] + data['w']\n",
    "        else:\n",
    "            graph.add_edge(u, v, w=t, s=data['w'])\n",
    "\n",
    "    # Devide weights (both requency and strength) by 2, because they have been counted twice\n",
    "    if input_undirected:\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            graph[u][v]['w'] //= 2\n",
    "            graph[u][v]['s'] //= 2\n",
    "\n",
    "    # Print network statistics\n",
    "    if output_network:\n",
    "        print('static netwrok:')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        if nx.algorithms.components.is_connected(graph):\n",
    "            print('Network is connected with diameter',\n",
    "                  nx.algorithms.distance_measures.diameter(graph))\n",
    "        else:\n",
    "            print('Network is not connected and has {} connected components.'.\n",
    "                  format(\n",
    "                      nx.algorithms.components.number_connected_components(\n",
    "                          graph)))\n",
    "        print('Density =', nx.classes.function.density(graph))\n",
    "\n",
    "    # Save the network\n",
    "    if save_network_file:\n",
    "        nx.write_gpickle(graph, file_output[0])\n",
    "\n",
    "    if save_network_csv:\n",
    "        # (1)\n",
    "        # nx.write_edgelist(graph, file_output[1], data=True)\n",
    "        # (2)\n",
    "        # List of list containing columns of [[node-1],[node-2],[weight-1(e.g. time)],[weight-2],...]\n",
    "        w_keys = list(\n",
    "            list(graph.edges(list(T.nodes(0))[0][0], data=True))[0][2].keys())\n",
    "        edgelist_columns = []\n",
    "        edgelist_columns.append([])\n",
    "        edgelist_columns.append([])\n",
    "        for w_id in range(len(w_keys)):\n",
    "            edgelist_columns.append([])\n",
    "        for u, v, w in graph.edges(data=True):\n",
    "            edgelist_columns[0].append(u)\n",
    "            edgelist_columns[1].append(v)\n",
    "            for i in range(len(w_keys)):\n",
    "                edgelist_columns[2 + i].append(w[w_keys[i]])\n",
    "            # The associative undirected edge\n",
    "            edgelist_columns[1].append(u)\n",
    "            edgelist_columns[0].append(v)\n",
    "            for i in range(len(w_keys)):\n",
    "                edgelist_columns[2 + i].append(w[w_keys[i]])\n",
    "        pd.DataFrame(edgelist_columns).transpose().to_csv(file_output[1],\n",
    "                                                          header=False,\n",
    "                                                          index=False)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def static_bt_read(file_input=['network/bt_static_network.gpickle'],\n",
    "                input_label='',\n",
    "                output_network=False):\n",
    "\n",
    "    # Modify input file names\n",
    "    file_input = label_amend(file_input, input_label)\n",
    "\n",
    "    # Read the network from file\n",
    "    graph = nx.Graph()\n",
    "    print('Reading the static network ...')\n",
    "    graph = nx.read_gpickle(file_input)\n",
    "\n",
    "    # Print network statistics\n",
    "    if output_network:\n",
    "        print('static netwrok:')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        if nx.algorithms.components.is_connected(graph):\n",
    "            print('Network is connected with diameter',\n",
    "                  nx.algorithms.distance_measures.diameter(graph))\n",
    "        else:\n",
    "            print('Network is not connected and has {} connected components.'.format(\n",
    "                nx.algorithms.components.number_connected_components(graph)))\n",
    "        print('Density =', nx.classes.function.density(graph))\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Time-ordered Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create\n",
    "TN network for the Bluetooth connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### __Light__ version\n",
    "Skip creating edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:56:48.003696Z",
     "start_time": "2021-02-23T01:56:47.968290Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tn_bt_light_create(file_input=[\n",
    "    'db/uiuc.db', 'network/bt_temporal_network.gpickle',\n",
    "    'network/bt_temporal_times.csv'\n",
    "],\n",
    "                       file_output=[\n",
    "                           'network/bt_tn_light_network.gpickle',\n",
    "                           'network/bt_tn_light_edgelist.csv'\n",
    "                       ],\n",
    "                       temporal=None,\n",
    "                       times=None,\n",
    "                       label_input='',\n",
    "                       label_output='',\n",
    "                       _directed=True,\n",
    "                       _trans=True,\n",
    "                       _teleport=False,\n",
    "                       _loop=False,\n",
    "                       output_network=False,\n",
    "                       save_network_db=False,\n",
    "                       save_network_csv=True,\n",
    "                       save_network_file=True):\n",
    "    \"\"\"\n",
    "    Convert a temporal (aggregated) network to a (directed) time-ordered (temporal) network\n",
    "    Light version means edges are not labelled\n",
    "        _directed: add bi-directional temporal edges i.e. t <-> t+1\n",
    "        _teleport: activate temporal teleportation\n",
    "        _loop: add last timestamp nodes to first i.e. T -> t1\n",
    "    \"\"\"\n",
    "    # Amend input / output file names using label\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read temporal networks and its node list from file\n",
    "    if temporal is None or times is None:\n",
    "        temporal, times = temporal_bt_read(\n",
    "            file_input=[file_input[1], file_input[2]])\n",
    "\n",
    "    # GRAPH\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # TIME\n",
    "    # Size of timestamp list\n",
    "    T = len(times)\n",
    "    # Time -> Index\n",
    "    time_index = dict((v, i) for i, v in enumerate(times))\n",
    "\n",
    "    # NODE\n",
    "    # Size of node list\n",
    "    N = temporal.number_of_nodes()\n",
    "    # Node -> Index\n",
    "    nodes = pd.Series(sorted(list(temporal.nodes)))\n",
    "    node_index = dict((v, i) for i, v in enumerate(nodes))\n",
    "\n",
    "    # EDGE\n",
    "    # Node (index) -> horizontal edges\n",
    "    L = temporal.number_of_edges()\n",
    "    node_edges = {}\n",
    "    for n in range(N):\n",
    "        node_edges[n] = [(N * t + n, N * (t + 1) + n) for t in range(T)]\n",
    "\n",
    "    # Horizontal edges\n",
    "    if _directed:\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):  # [0,T]\n",
    "                # Add edges: node(i) -> node(i+1)\n",
    "                graph.add_edge(edges[i][0], edges[i][1])\n",
    "                # With temporal teleportation\n",
    "                if _teleport:\n",
    "                    for j in range(i + 1, len(edges)):\n",
    "                        graph.add_edge(edges[i][0], edges[j][1])\n",
    "        # With temporal loop\n",
    "        if _loop:\n",
    "            for node in node_edges:\n",
    "                graph.add_edge(node_edges[node][-1][1], node_edges[node][0][0])\n",
    "    else:  # Undirected\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):\n",
    "                graph.add_edge(edges[i][0], edges[i][1])\n",
    "                # Backward horizontal edge (i.e. moving back in time)\n",
    "                graph.add_edge(edges[i][1], edges[i][0])\n",
    "                if _teleport:\n",
    "                    for j in range(i + 1, len(edges)):\n",
    "                        graph.add_edge(edges[i][0], edges[j][1])\n",
    "                        # Backward teleportation in time\n",
    "                        graph.add_edge(edges[j][1], edges[i][0])\n",
    "        # With tempora loop\n",
    "        if _loop:\n",
    "            for node in node_edges:\n",
    "                graph.add_edge(node_edges[node][-1][1], node_edges[node][0][0])\n",
    "                graph.add_edge(node_edges[node][0][0], node_edges[node][-1][1])\n",
    "\n",
    "    # Crossed edges\n",
    "    # Only edge weight carrying to lite version is 'weight' of interactions\n",
    "    # Becasue we need 'weight' for spread probability edge attribute\n",
    "    if _directed:  # Directed\n",
    "        for u, v, edge_data in temporal.edges(data=True):\n",
    "            u_index = node_index[u]\n",
    "            v_index = node_index[v]\n",
    "            t_index = time_index[edge_data['t']]\n",
    "            graph.add_edge(u_index + t_index * N,\n",
    "                           v_index + (t_index + 1) * N,\n",
    "                           w=edge_data['w'])\n",
    "    else:  # Undirected\n",
    "        for u, v, edge_data in temporal.edges(data=True):\n",
    "            u_index = node_index[u]\n",
    "            v_index = node_index[v]\n",
    "            t_index = time_index[edge_data['t']]\n",
    "            graph.add_edge(u_index + t_index * N,\n",
    "                           v_index + (t_index + 1) * N,\n",
    "                           w=edge_data['w'])\n",
    "            graph.add_edge(v_index + (t_index + 1) * N,\n",
    "                           u_index + t_index * N,\n",
    "                           w=edge_data['w'])\n",
    "\n",
    "    # Transitive closure\n",
    "    trans_num = 0\n",
    "    if _trans:\n",
    "        for t in range(T):\n",
    "            snap_nodes = [(t * N) + n for n in range(N)]\n",
    "            snap_nodes.extend([((t + 1) * N) + n for n in range(N)])\n",
    "            snap_graph = graph.subgraph(snap_nodes)\n",
    "            A = nx.to_numpy_matrix(snap_graph)\n",
    "            A_t = A[:len(A) // 2, len(A) // 2:]\n",
    "            snap_trans = nx.to_numpy_matrix(\n",
    "                nx.transitive_closure(\n",
    "                    nx.from_numpy_matrix(A_t, create_using=nx.DiGraph)))\n",
    "            # Compare edges of transitive closure with edges we had before, find new edges, add them to network\n",
    "            snap_edges = np.transpose(np.nonzero(A_t != snap_trans))\n",
    "            snap_weights = np.tile(\n",
    "                0.5 * np.random.sample(len(snap_edges) // 2) + 0.5, 2)\n",
    "            # index of new edges should be converted into node ID in network\n",
    "            for r in range(len(snap_edges)):\n",
    "                if not graph.has_edge(snap_nodes[snap_edges[r][0]],\n",
    "                                      snap_nodes[snap_edges[r][1] + N]):\n",
    "                    trans_num += 1  # Counter of transitive edges\n",
    "                    graph.add_edge(snap_nodes[snap_edges[r][0]],\n",
    "                                   snap_nodes[snap_edges[r][1] + N],\n",
    "                                   w=snap_weights[r],\n",
    "                                   trans=True)\n",
    "                    if not _directed:\n",
    "                        graph.add_edge(snap_nodes[snap_edges[r][0]] + N,\n",
    "                                       snap_nodes[snap_edges[r][1]],\n",
    "                                       w=snap_weights[r],\n",
    "                                       trans=True)\n",
    "\n",
    "    # Print\n",
    "    if output_network:\n",
    "        print('Time-ordered Network (Light)')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        if _trans:\n",
    "            print('{} transitive closure edges were added.'.format(trans_num))\n",
    "\n",
    "    # Save\n",
    "    if save_network_db:\n",
    "        edge_list = pd.DataFrame.from_dict(graph.edges)\n",
    "        edge_list.columns = ['u', 'v']\n",
    "        edge_list.to_sql(name='bluetooth_time_ordered_edgelist',\n",
    "                         con=db_connect(file_input[0]),\n",
    "                         if_exists='replace',\n",
    "                         index_label='id')\n",
    "\n",
    "    if save_network_file:\n",
    "        nx.write_gpickle(graph, file_output[0])\n",
    "\n",
    "    if save_network_csv:\n",
    "        # nx.write_edgelist(graph, file_output[1])\n",
    "        nx.write_weighted_edgelist(graph, file_output[1], delimiter=',')\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### __Full__ version\n",
    "TS network for the Bluetooth connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:56:49.464407Z",
     "start_time": "2021-02-23T01:56:49.407988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tn_bt_full_create(file_input=[\n",
    "    'db/uiuc.db', 'network/bt_temporal_network.gpickle',\n",
    "    'network/bt_temporal_times.csv', 'network/bt_temporal_nodes.csv'\n",
    "],\n",
    "                      file_output=[\n",
    "                          'network/bt_tn_full_network.gpickle',\n",
    "                          'network/bt_tn_full_edgelist.csv',\n",
    "                          'network/bt_tn_delta.csv',\n",
    "                          'network/bt_tn_weights.csv'\n",
    "                      ],\n",
    "                      temporal=None,\n",
    "                      times=None,\n",
    "                      label_input='',\n",
    "                      label_output='',\n",
    "                      _directed=True,\n",
    "                      _trans=True,\n",
    "                      _teleport=False,\n",
    "                      _loop=False,\n",
    "                      _color=True,\n",
    "                      _delta=True,\n",
    "                      _delta_type='h',\n",
    "                      version=0,\n",
    "                      _omega=1,\n",
    "                      _gamma=0.0001,\n",
    "                      _epsilon=1,\n",
    "                      _distance=1,\n",
    "                      _alpha=0.5,\n",
    "                      output_delta=False,\n",
    "                      output_weight=False,\n",
    "                      output_network=False,\n",
    "                      save_delta=False,\n",
    "                      save_weight=False,\n",
    "                      save_network_csv=True,\n",
    "                      save_network_file=True):\n",
    "    \"\"\"\n",
    "    Convert a temporal (aggregated) network to a (directed) time-series (DS) temporal network\n",
    "    Full version means nodes and edges get labelled with differnt information such as:\n",
    "        * color\n",
    "        * position\n",
    "        * delta time: the time differnec of an edge from previous timestamp\n",
    "        * temporal weight considering horizontal and crossed format with following versions:\n",
    "            0: None\n",
    "            1: (1/2)^(lengh of horizontal path)\n",
    "            2: 1/(lenght of horizontal path)\n",
    "            3: 1/(log2(lenght of horizontal path))\n",
    "    Other parameters are:\n",
    "        _directed: add bi-directional temporal edges i.e. t <-> t+1\n",
    "        _teleport: activate temporal teleportation\n",
    "        _loop: add last timestamp nodes to first i.e. T -> t1\n",
    "        _color: add color to nodes\n",
    "        _delta: add delta-time weight to edges\n",
    "        _delta_type: h (for hour) & m (for minute)\n",
    "        _omega: horizontal edge weight (by default = 1, without penalize)\n",
    "        _gamma: teleportation edge weight\n",
    "        _epsilon: crossed edge weight\n",
    "        _distance: horizontal edge distance\n",
    "        _alpha: dynamic penalizing coefficient\n",
    "        _sigma: crossed teleportation (not used here - applied during HITS)\n",
    "    \"\"\"\n",
    "    # Amend input / output file names using label\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read temporal networks and its node list from file\n",
    "    if temporal is None or times is None:\n",
    "        temporal, times = temporal_bt_read(\n",
    "            file_input=[file_input[1], file_input[2]])\n",
    "\n",
    "    # GRAPH\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # TIME\n",
    "    # Size of timestamp list\n",
    "    T = len(times)\n",
    "    # Time -> Index\n",
    "    time_index = dict((v, i) for i, v in enumerate(times))\n",
    "\n",
    "    # NODE\n",
    "    # Size of node list\n",
    "    N = temporal.number_of_nodes()\n",
    "    # Node -> Index\n",
    "    nodes = pd.Series(sorted(list(temporal.nodes)))\n",
    "    node_index = dict((v, i) for i, v in enumerate(nodes))\n",
    "\n",
    "    # EDGE\n",
    "    # Node (index) -> horizontal edges\n",
    "    L = temporal.number_of_edges()\n",
    "    node_edges = {}\n",
    "    for n in range(N):\n",
    "        node_edges[n] = [(N * t + n, N * (t + 1) + n) for t in range(T)]\n",
    "\n",
    "    # Colors for nodes at differnt timestamp\n",
    "    colors = []\n",
    "    if _color:\n",
    "        cmap = cm.get_cmap('Wistia', T + 1)\n",
    "        for i in range(cmap.N):\n",
    "            rgb = cmap(i)[:3]\n",
    "            colors.append(matplotlib.colors.rgb2hex(rgb))\n",
    "    else:\n",
    "        colors = ['#000000'] * (T + 1)\n",
    "\n",
    "    # Create the time difference between all consecutive timestamps\n",
    "    # Convert delta to second and then hour\n",
    "    # Add '1' to the begginig so that -> len(delta) == len(times)\n",
    "    # Because there is nothing before the first timestamp\n",
    "    # So we assume delta for nodes at the first timestamp is '1' hour\n",
    "    delta = []\n",
    "    if _delta:\n",
    "        times = list(times)\n",
    "        delta_temp = pd.Series(pd.Series(times[1:]) - pd.Series(times[:T - 1]))\n",
    "        if _delta_type == 'h':  # Hour scale\n",
    "            delta = [int(ts.total_seconds() // 3600) for ts in delta_temp]\n",
    "        elif _delta_type == 'm':  # Minute scale\n",
    "            delta = [int(ts.total_seconds() // 60) for ts in delta_temp]\n",
    "        delta.insert(0, 1)  # (postion, value)\n",
    "        times = pd.Series(times)\n",
    "    else:\n",
    "        delta = [1] * T  # 1 hour (default value for all edges)\n",
    "        times = pd.Series(times)\n",
    "\n",
    "    if output_delta:\n",
    "        # Count the unique delta values\n",
    "        print(\"Delta time distribution:\")\n",
    "        if _delta_type == 'h':\n",
    "            delta_count = pd.Series(Counter(delta)).sort_index()\n",
    "            # print(delta_count[:24])  # Print deltas up to 1 day = 24 hours\n",
    "            print(delta_count)\n",
    "        elif _delta_type == 'm':\n",
    "            # 1 day = 24 H * 60 Min = 1440, anything more than that could be outlier\n",
    "            # Or some sort of time jumps in the dataset\n",
    "            delta_count = pd.Series([d for d in delta if d <= 24 * 60\n",
    "                                     ]).value_counts(normalize=True)\n",
    "            print(delta_count)\n",
    "\n",
    "    if save_delta:  # save delta\n",
    "        np.savetxt(file_output[2], delta, delimiter=',', fmt='%s')\n",
    "        # pd.DataFrame(delta).to_csv(file_output[2], header=None, index=False)\n",
    "\n",
    "    # Convert delta list to series for easy access in future\n",
    "    delta = pd.Series(delta)\n",
    "\n",
    "    # Horizontal edges\n",
    "    if _directed:\n",
    "        for node, edges in node_edges.items():\n",
    "            # Add the first node at the first timestamp\n",
    "            graph.add_node(\n",
    "                node,\n",
    "                # It turned out 'parent' is not necessary, because we use 'node' or parent_index in 'pos'\n",
    "                # So it can always be extracted from postion of node or id/number_of_nodes\n",
    "                # parent=nodes[node],  # Parent\n",
    "                c=colors[0],  # Color\n",
    "                p=(0, node))  # Position / Cordinates\n",
    "            for i in range(len(edges)):  # i = time, and in range [0,T]\n",
    "                # Add the edge (u,v) with its attribute\n",
    "                graph.add_edge(\n",
    "                    edges[i][0],\n",
    "                    edges[i][1],\n",
    "                    t=times[i],  # Time\n",
    "                    d=delta[i],  # Delta or temporal distance\n",
    "                    tw=_omega,  # Temporal weight (tw)\n",
    "                    c='silver')  # Color\n",
    "\n",
    "                # Then set node attribute of second node from created edge of (u,v)\n",
    "                # graph.nodes[edges[i][1]]['parent'] = nodes[node]\n",
    "                graph.nodes[edges[i][1]]['c'] = colors[i + 1]\n",
    "                graph.nodes[edges[i][1]]['p'] = (i + 1, node)\n",
    "                # With temporal teleportation\n",
    "                if _teleport:\n",
    "                    for j in range(i + 1, len(edges)):\n",
    "                        graph.add_edge(\n",
    "                            edges[i][0],\n",
    "                            edges[j][1],\n",
    "                            # d=sum(delta[i:j])  # Needs testing\n",
    "                            tw=_gamma,\n",
    "                            c='gold')\n",
    "        # With temporal loop\n",
    "        if _loop:\n",
    "            for node in node_edges:\n",
    "                graph.add_edge(\n",
    "                    node_edges[node][-1][1],\n",
    "                    node_edges[node][0][0],\n",
    "                    # d=sum(delta)  # Needs testing\n",
    "                    tw=_omega,\n",
    "                    c='orange')\n",
    "    else:  # Undirected\n",
    "        for node, edges in node_edges.items():\n",
    "            graph.add_node(node, c=colors[0], p=(0, node))\n",
    "            for i in range(len(edges)):\n",
    "                graph.add_edge(edges[i][0],\n",
    "                               edges[i][1],\n",
    "                               t=times[i],\n",
    "                               d=delta[i],\n",
    "                               tw=_omega,\n",
    "                               c='silver')\n",
    "                # Backward horizontal edge (i.e. moving back in time)\n",
    "                graph.add_edge(edges[i][1],\n",
    "                               edges[i][0],\n",
    "                               t=times[i],\n",
    "                               d=delta[i],\n",
    "                               tw=_omega,\n",
    "                               c='silver')\n",
    "                graph.nodes[edges[i][1]]['c'] = colors[i + 1]\n",
    "                graph.nodes[edges[i][1]]['p'] = (i + 1, node)\n",
    "                if _teleport:\n",
    "                    for j in range(i + 1, len(edges)):\n",
    "                        graph.add_edge(edges[i][0],\n",
    "                                       edges[j][1],\n",
    "                                       tw=_gamma,\n",
    "                                       c='gold')\n",
    "                        # Backward teleportation in time\n",
    "                        graph.add_edge(edges[j][1],\n",
    "                                       edges[i][0],\n",
    "                                       tw=_gamma,\n",
    "                                       c='gold')\n",
    "        # With temporal loop\n",
    "        if _loop:\n",
    "            for node in node_edges:\n",
    "                graph.add_edge(node_edges[node][-1][1],\n",
    "                               node_edges[node][0][0],\n",
    "                               tw=_omega,\n",
    "                               c='orange')\n",
    "                graph.add_edge(node_edges[node][0][0],\n",
    "                               node_edges[node][-1][1],\n",
    "                               tw=_omega,\n",
    "                               c='orange')\n",
    "\n",
    "    # Crossed edges\n",
    "    if _directed:\n",
    "        for u, v, edge_data in temporal.edges(data=True):\n",
    "            u_index = node_index[u]\n",
    "            v_index = node_index[v]\n",
    "            t_index = time_index[edge_data['t']]\n",
    "            graph.add_edge(u_index + t_index * N,\n",
    "                           v_index + (t_index + 1) * N,\n",
    "                           t=edge_data['t'],\n",
    "                           w=edge_data['w'],\n",
    "                           d=delta[t_index],\n",
    "                           tw=_epsilon,\n",
    "                           c='black')\n",
    "    else:  # Undirected\n",
    "        for u, v, edge_data in temporal.edges(data=True):\n",
    "            u_index = node_index[u]\n",
    "            v_index = node_index[v]\n",
    "            t_index = time_index[edge_data['t']]\n",
    "            graph.add_edge(u_index + t_index * N,\n",
    "                           v_index + (t_index + 1) * N,\n",
    "                           t=edge_data['t'],\n",
    "                           w=edge_data['w'],\n",
    "                           d=delta[t_index],\n",
    "                           tw=_epsilon,\n",
    "                           c='black')\n",
    "            graph.add_edge(v_index + (t_index + 1) * N,\n",
    "                           u_index + t_index * N,\n",
    "                           t=edge_data['t'],\n",
    "                           w=edge_data['w'],\n",
    "                           d=delta[t_index],\n",
    "                           tw=_epsilon,\n",
    "                           c='black')\n",
    "\n",
    "    # Transitive closure\n",
    "    trans_num = 0\n",
    "    if _trans:\n",
    "        for t in range(T):\n",
    "            snap_nodes = [(t * N) + n for n in range(N)]\n",
    "            snap_nodes.extend([((t + 1) * N) + n for n in range(N)])\n",
    "            snap_graph = graph.subgraph(snap_nodes)\n",
    "            A = nx.to_numpy_matrix(snap_graph)\n",
    "            A_t = A[:len(A) // 2, len(A) // 2:]\n",
    "            snap_trans = nx.to_numpy_matrix(\n",
    "                nx.transitive_closure(\n",
    "                    nx.from_numpy_matrix(A_t, create_using=nx.DiGraph)))\n",
    "            # Compare edges of transitive closure with edges we had before, find new edges, add them to network\n",
    "            snap_edges = np.transpose(np.nonzero(A_t != snap_trans))\n",
    "            snap_weights = np.tile(\n",
    "                0.5 * np.random.sample(len(snap_edges) // 2) + 0.5, 2)\n",
    "            # index of new edges should be converted into node ID in network\n",
    "            for r in range(len(snap_edges)):\n",
    "                if not graph.has_edge(snap_nodes[snap_edges[r][0]],\n",
    "                                      snap_nodes[snap_edges[r][1] + N]):\n",
    "                    trans_num += 1  # Counter of transitive edges\n",
    "                    graph.add_edge(snap_nodes[snap_edges[r][0]],\n",
    "                                   snap_nodes[snap_edges[r][1] + N],\n",
    "                                   t=times[t],\n",
    "                                   w=snap_weights[r],\n",
    "                                   d=delta[t],\n",
    "                                   tw=_epsilon,\n",
    "                                   trans=True,\n",
    "                                   c='red')  # Only for trans edges\n",
    "                    if not _directed:\n",
    "                        graph.add_edge(snap_nodes[snap_edges[r][0]] + N,\n",
    "                                       snap_nodes[snap_edges[r][1]],\n",
    "                                       t=times[t],\n",
    "                                       w=snap_weights[r],\n",
    "                                       d=delta[t],\n",
    "                                       tw=_epsilon,\n",
    "                                       trans=True,\n",
    "                                       c='red')\n",
    "\n",
    "    # Prepare for penalizing horizontal edge weights\n",
    "    # If version == 0 -> skip penalizing all together\n",
    "    if version != 0:\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):  # len = T\n",
    "                # If in-degree second node 'v' of underlying edge (u,v) is 1\n",
    "                # Meaning it (u,v) is a horizontal edge with no other node connecting to it\n",
    "                if graph.in_degree(edges[i][1]) == 1:  # 'v' of (u,v)\n",
    "                    if i == 0:  # There is no edge before so ...\n",
    "                        graph[edges[i][0]][edges[i][1]][\n",
    "                            'tw'] = 1 + _distance  # e.g. _distance = 1\n",
    "                    else:\n",
    "                        graph[edges[i][0]][edges[i][1]]['tw'] = graph[edges[\n",
    "                            i - 1][0]][edges[i - 1][1]]['tw'] + _distance\n",
    "\n",
    "    # (alpha)^(distance) e.g. (1/2)^(1), (1/2)^(2), ...\n",
    "    # This version is exponential penalization = very harsh\n",
    "    if version == 1:\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):\n",
    "                graph[edges[i][0]][edges[i][1]]['tw'] = _alpha**(\n",
    "                    graph[edges[i][0]][edges[i][1]]['tw'] - 1)\n",
    "\n",
    "    # 1/(distance)\n",
    "    # This version is polinomial penalization = harsh :/\n",
    "    elif version == 2:\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):\n",
    "                graph[edges[i][0]][edges[i][1]]['tw'] = 1 / graph[edges[i][0]][\n",
    "                    edges[i][1]]['tw']\n",
    "\n",
    "    # 1/log2(distance + 1)\n",
    "    # This version is logarithmic penalization = not so harsh :)\n",
    "    elif version == 3:\n",
    "        for node, edges in node_edges.items():\n",
    "            for i in range(len(edges)):\n",
    "                graph[edges[i][0]][edges[i][1]]['tw'] = 1 / np.log2(\n",
    "                    graph[edges[i][0]][edges[i][1]]['tw'] + 1)\n",
    "\n",
    "    if save_weight:\n",
    "        ew = {}\n",
    "        for u, v, tw in graph.edges(data='tw'):\n",
    "            ew[(u, v)] = tw\n",
    "        pd.DataFrame.from_dict(ew, orient='index').to_csv(file_output[3])\n",
    "\n",
    "    # Print temporal weights for horizontal edges\n",
    "    if output_weight:\n",
    "        print('node @ time -> edge(u,v) = weight')\n",
    "        for node, edges in node_edges.items():\n",
    "            for edge in edges:\n",
    "                print('{} @ {}: ({},{}) = {}'.format(\n",
    "                    node, edge[1] // N, edge[0], edge[1],\n",
    "                    graph[edge[0]][edge[1]]['tw']))\n",
    "\n",
    "    if output_network:\n",
    "        print('Time-ordered Network (Full):')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        if _trans:\n",
    "            print('{} transitive closure edges were added.'.format(trans_num))\n",
    "\n",
    "    if save_network_file:\n",
    "        nx.write_gpickle(graph, file_output[0])\n",
    "\n",
    "    if save_network_csv:\n",
    "        w_keys = list(\n",
    "            list(graph.edges(list(graph.nodes(0))[0][0],\n",
    "                             data=True))[0][2].keys())\n",
    "        # list of list containing columns of [[node-1],[node-2],[weight-1(e.g. time)],[weight-2],...]\n",
    "        edgelist_columns = []\n",
    "        edgelist_columns.append([])\n",
    "        edgelist_columns.append([])\n",
    "        for w_id in range(len(w_keys)):\n",
    "            edgelist_columns.append([])\n",
    "        for u, v, w in graph.edges(data=True):\n",
    "            edgelist_columns[0].append(u)\n",
    "            edgelist_columns[1].append(v)\n",
    "            for i in range(len(w_keys)):\n",
    "                edgelist_columns[2 + i].append(w[w_keys[i]])\n",
    "        pd.DataFrame(edgelist_columns).transpose().to_csv(file_output[1],\n",
    "                                                          header=False,\n",
    "                                                          index=False)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Reverse (TODO)\n",
    "Convert a (multi-layer) time-ordered network to (multi-edge) temporal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tn_bt_to_temporal(file_input=[\n",
    "    'remove/bt_tn_network.gpickle', 'network/bt_temporal_times.csv'\n",
    "],\n",
    "                      file_output=[\n",
    "                          'remove/bt_temporal_network.gpickle',\n",
    "                          'remove/bt_temporal_edgelist.csv',\n",
    "                          'remove/bt_temporal_times.csv',\n",
    "                          'remove/bt_temporal_nodes.csv'\n",
    "                      ],\n",
    "                      temporal=None,\n",
    "                      label_input='',\n",
    "                      label_output='',\n",
    "                      output_network=False,\n",
    "                      save_network_csv=True,\n",
    "                      save_network_file=True):\n",
    "    \"\"\"\n",
    "    Convert a (directed) time-ordered network (TON) to a (multi-edge) temporal (T) network\n",
    "    \"\"\"\n",
    "    # Amend filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Create empty TEMPORAL network\n",
    "    graph = nx.MultiDiGraph()\n",
    "\n",
    "    # Read temporal networks from file\n",
    "    if temporal is None:\n",
    "        temporal = nx.read_gpickle(file_input[0])\n",
    "\n",
    "    # Timestamp\n",
    "    # times = temporal_bt_times_read()  # TODO: we may not need it\n",
    "    T = line_count(file_input[2])\n",
    "    \n",
    "    for \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:56:56.740110Z",
     "start_time": "2021-02-23T01:56:56.730115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tn_bt_light_read(\n",
    "    file_input=['network/bt_tn_light_network.gpickle'],\n",
    "    label_input='',\n",
    "    output=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads light version of (directed) time-ordered network of Bluetooth connections\n",
    "    \"\"\"\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    graph = nx.read_gpickle(file_input[0])\n",
    "    graph.name = 'Time-ordered Network'\n",
    "    if output: print(nx.info(graph))\n",
    "    return graph\n",
    "\n",
    "\n",
    "def tn_bt_full_read(\n",
    "    file_input=['network/bt_tn_full_network.gpickle'],\n",
    "    label_input='',\n",
    "    output=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads full version of (directed) time-ordered network of Bluetooth connections\n",
    "    \"\"\"\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    graph = nx.read_gpickle(file_input[0])\n",
    "    graph.name = 'Time-ordered Network (Full Version)'\n",
    "    if output: print(nx.info(graph))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T21:33:58.998797Z",
     "start_time": "2021-02-11T21:33:58.983456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ! pip install mpmath\n",
    "# ! pip install powerlaw\n",
    "\n",
    "import powerlaw\n",
    "\n",
    "def tn_bt_analyze(file_input=[\n",
    "    'network/bt_tn_full_network.gpickle',\n",
    "    'network/bt_temporal_network.gpickle', 'network/bt_temporal_times.csv'\n",
    "],\n",
    "                  label_input=''):\n",
    "    # Read network\n",
    "    graph = dt_read(file_input=[file_input[0]], label_input=label_input)\n",
    "    temporal, times = temporal_bt_read(\n",
    "        file_input=[file_input[1], file_input[2]], output_network=True)\n",
    "\n",
    "    # Variable\n",
    "    T = len(times)\n",
    "    N = temporal.number_of_nodes()\n",
    "    L = temporal.number_of_edges()\n",
    "    # N_new = graph.number_of_nodes()\n",
    "    # L_new = graph.number_of_edges()\n",
    "\n",
    "    # Dictionary of time -> all nodes in that time\n",
    "    time_nodes = {}\n",
    "    for t in range(T):\n",
    "        time_nodes[t] = [N * t + n for n in range(N)]\n",
    "\n",
    "    # Check the edge frequency in each timestamp\n",
    "    time_out_degrees = {}\n",
    "    for t in sorted(time_nodes):  # t in [0,T]\n",
    "        time_out_degrees[t] = [graph.out_degree(n) - 1 for n in time_nodes[t]]\n",
    "    # Dataframe of outdegress with time as columns\n",
    "    time_out_degrees = pd.DataFrame.from_dict(time_out_degrees)\n",
    "    # out_degrees_time = time_out_degrees.sum(0)  # Sum of out degrees over column i.e. time\n",
    "    print(sorted(Counter(time_out_degrees.sum(0)).items(), key=lambda x: x[0]))\n",
    "    # results = powerlaw.Fit(out_degrees_time)\n",
    "    # print(results.power_law.alpha)\n",
    "    # print(results.power_law.xmin)\n",
    "    # R, p = results.distribution_compare('power_law', 'lognormal')\n",
    "    # print(out_degrees_time / max(out_degrees_time))  # Scale of out_degrees_time to [0,1] \n",
    "    print(time_out_degrees.sum(1))  # Sum of out degrees over row i.e. node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tn_bt_analyze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T03:20:24.321890Z",
     "start_time": "2021-02-17T03:20:23.444162Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tn = tn_bt_light_create(\n",
    "#     _directed=True,\n",
    "#     _teleport=False,\n",
    "#     _trans=True,  # transitive edges\n",
    "#     _loop=False,\n",
    "#     output_network=True,\n",
    "#     save_network_db=True,\n",
    "#     save_network_csv=True,\n",
    "#     save_network_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T03:20:35.529491Z",
     "start_time": "2021-02-17T03:20:33.132668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tn = tn_bt_full_create(\n",
    "#     _directed=True,\n",
    "#     _trans=True,\n",
    "#     _teleport=False,\n",
    "#     _loop=False,\n",
    "#     _color=True,\n",
    "#     _delta=True,\n",
    "#     version=3,  # 0\n",
    "#     _omega=1,  # 1\n",
    "#     _gamma=0.0001,\n",
    "#     _epsilon=1,\n",
    "#     _distance=0.1,  # 1\n",
    "#     _alpha=0.5,\n",
    "#     output_delta=False,\n",
    "#     output_weight=False,\n",
    "#     output_network=True,\n",
    "#     save_delta=True,\n",
    "#     save_weight=False,\n",
    "#     save_network_csv=True,\n",
    "#     save_network_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T03:13:25.105585Z",
     "start_time": "2021-02-17T03:13:25.068299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tn = tn_bt_light_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tn = tn_bt_full_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Edge Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create\n",
    "We set the temporal edge weights (from light version of time-ordered network) can be save in a new dictionary to keep the network light or save into network     \n",
    "In addition, add a new weight __probs__ or __normalized_weights__ which can be used in spread / diffusion process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Edge weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:57:33.930458Z",
     "start_time": "2021-02-23T01:57:33.899215Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ew_create(\n",
    "    file_input=[\n",
    "        'network/bt_tn_light_network.gpickle',\n",
    "        'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_temporal_times.csv',\n",
    "    ],\n",
    "    file_output=['network/bt_tn_weights.csv'],\n",
    "    label_input='',\n",
    "    label_output='',\n",
    "    graph=None,\n",
    "    number_of_nodes=None,\n",
    "    number_of_times=None,\n",
    "    version=0,\n",
    "    omega=1,\n",
    "    epsilon=1,\n",
    "    gamma=0.0001,\n",
    "    distance=1,\n",
    "    alpha=0.5,\n",
    "    save_weights=False,\n",
    "    output_weights=False,\n",
    "    plot_weights=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate (and save) the temporal edge weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : NetworkX\n",
    "        time-ordered network (TON)\n",
    "    number_of_nodes : int\n",
    "        numbero of nodes from the temporal graph (default is None, then reads from file)\n",
    "    number_of_times : int\n",
    "        numbero of timestamps from the temporal graph (default is None, then reads from file)\n",
    "    version : int\n",
    "        0 -> contact value of omega\n",
    "        1 -> dynamic (alpha)^(lengh_of_horizontal_path)\n",
    "        2 -> dynamic (1)/(lenght_of_horizontal_path)\n",
    "        3 -> dynamic (1)/(log2(lenght_of_horizontal_path))\n",
    "    omega : float\n",
    "        weight factor of horizontal edges (e.g. 1, 0.01, 0.001, 0.0001 ...)\n",
    "    epsilon :float\n",
    "        weight factor of crossed edgess (e.g. 1, 0.01, 0.001, 0.0001 ...)\n",
    "    gamma : float\n",
    "        weight of horizontal teleport edges (e.g. 0.0001, 0.001, 0.01 ...)\n",
    "    distance : float\n",
    "        value that being added to as lenght to non-active consecutive edges or paths (smaller -> slower weight decay)\n",
    "    alpha : float\n",
    "        magnification factor in version 1 (larger -> slower weight decay), default = 1/2 or 0.5\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {(u,v):temporal_weight}\n",
    "    \"\"\"\n",
    "    def has_crossed_edge(in_graph, in_N, in_node):\n",
    "        in_parent = in_node % in_N\n",
    "        for pre in in_graph.predecessors(in_node):\n",
    "            if pre % in_N != in_parent:\n",
    "                return True\n",
    "        # Else = no crossed edge found ...\n",
    "        return False\n",
    "\n",
    "    # Amend input / output file names using label\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read time-ordered network and size of node from original network\n",
    "    if graph is None:\n",
    "        graph = tn_bt_light_read(file_input=[file_input[0]])\n",
    "\n",
    "    if number_of_nodes is None:\n",
    "        N = line_count(file_input[1])\n",
    "    else:\n",
    "        N = number_of_nodes\n",
    "\n",
    "    if number_of_times is None:\n",
    "        T = line_count(file_input[2])\n",
    "    else:\n",
    "        T = number_of_times\n",
    "        # T = graph.number_of_nodes() // N\n",
    "\n",
    "    # Edge-Weight dictionary\n",
    "    # {(u,v):weight}\n",
    "    ew = {}\n",
    "\n",
    "    # Horizontal edges helper dictionary\n",
    "    hedges = {}\n",
    "\n",
    "    # If node in_degree = 1 then node 'u' has only '1' horizontal-edge of (v,u)\n",
    "    nid = 1\n",
    "\n",
    "    _directed = True\n",
    "    # If node 0 at time 1 connects to node 0 at time 0 -> undirected\n",
    "    if (1 * N) + 0 in graph.predecessors(0):\n",
    "        print('Graph is temporally undirected')\n",
    "        _directed = False\n",
    "        nid += 2\n",
    "\n",
    "    _teleport = False\n",
    "    # If node 0 at time 0 connects to node 0 at time 2 -> teleport\n",
    "    if (2 * N) + 0 in graph.successors(0):\n",
    "        print('Graph has temporal teleporation edges')\n",
    "        _teleport = True\n",
    "\n",
    "    _loop = False\n",
    "    # If node 0 at time T connects to node 0 at time 0 -> loop\n",
    "    if (T * N) + 0 in graph.predecessors(0):\n",
    "        print('Graph has temporal loop edges')\n",
    "        _loop = True\n",
    "\n",
    "    if version == 0:  # Without penalization\n",
    "        if output_weights:\n",
    "            print(\n",
    "                f'Penalizing horizontal edges with contact value of omega = {omega}'\n",
    "            )\n",
    "        for u, v in graph.edges():\n",
    "            # When u & v have same parent -> edge (u,v) is horizontal\n",
    "            time_delta = abs(v - u) // N\n",
    "            parent_u = u % N\n",
    "            parent_v = v % N\n",
    "            if parent_u != parent_v:  # Crossed\n",
    "                ew[(u, v)] = epsilon  # E.g. 1\n",
    "            else:\n",
    "                if time_delta > 1:  # Teleport\n",
    "                    ew[(u, v)] = gamma  # E.g. 0.0001\n",
    "                else:  # # Horizontal OR time_delta = 1\n",
    "                    # Node v is node u at one timestamp after\n",
    "                    ew[(u, v)] = omega\n",
    "\n",
    "    else:  # Penalize ...\n",
    "        # Nodes [0-N]\n",
    "        for u, v in sorted(graph.edges(), key=lambda x: x[0]):\n",
    "            time_delta = abs(v - u) // N\n",
    "            parent_u = u % N\n",
    "            parent_v = v % N\n",
    "            if parent_u != parent_v:\n",
    "                ew[(u, v)] = epsilon\n",
    "            else:\n",
    "                if time_delta > 1:\n",
    "                    ew[(u, v)] = gamma\n",
    "                else:\n",
    "                    # Node v has crossed edge\n",
    "                    # if graph.in_degree(v) != nid:  # 1 or 2\n",
    "                    if has_crossed_edge(graph, N, v):\n",
    "                        hedges[(u, v)] = omega  # E.g. 1\n",
    "                    else:\n",
    "                        # Node v does not have crossed edge\n",
    "                        # Look at the previous edge weight (if exsit, otherwise return omega)\n",
    "                        hedges[(u, v)\n",
    "                               ] = hedges.get((u - N, u), omega) + distance\n",
    "\n",
    "        # Update weights based on version of penalization\n",
    "        if version == 1:\n",
    "            # Decay exponentially fast\n",
    "            # (parameteralpha)^(distance) e.g. 1/2^1 , 1/2^2, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = alpha**(weight - 1)\n",
    "        elif version == 2:\n",
    "            # Decay very fast\n",
    "            # 1/(distance) e.g. 1/2, 1/3, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = 1 / weight\n",
    "        elif version == 3:\n",
    "            # Decay fast\n",
    "            # 1/log2(distance + 1) e.g. 1/log2, 1/log3, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = 1 / np.log2(weight + 1)\n",
    "\n",
    "        # Finish by updating helper dictionary to 'ew'\n",
    "        ew.update(hedges)\n",
    "\n",
    "    if save_weights:\n",
    "        # dict_save(ew, file_output[0][:-4])  # npy\n",
    "        # OR\n",
    "        # dict_save(ew, file_output[0], method='c', sort=True)  # csv\n",
    "        # OR\n",
    "        # pd.DataFrame.from_dict(ew, orient='index').to_csv(file_output[0], header=False)\n",
    "        # OR\n",
    "        pd.Series(ew).reset_index().to_csv(\n",
    "            file_output[0], header=False, index=False\n",
    "        )\n",
    "\n",
    "    if plot_weights and version > 0:\n",
    "        ls = sorted(ew.items())\n",
    "        ls1, ls2 = zip(*ls)\n",
    "        plt.figure()\n",
    "        ax = sns.displot(ls2, kind='kde')\n",
    "\n",
    "    if output_weights:\n",
    "        for e, w in sorted(ew.items(), key=lambda x: x[0]):\n",
    "            if e[0] % N == e[1] % N:  # H\n",
    "                if graph.in_degree(e[1]) == nid:\n",
    "                    print('{}\\t->\\t{}\\t{}'.format(e[0], e[1], w))\n",
    "\n",
    "    return ew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Transition probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T19:59:44.017232Z",
     "start_time": "2021-02-20T19:59:43.978220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ew_prob_create(\n",
    "    file_input=[\n",
    "        'network/bt_tn_light_network.gpickle',\n",
    "        'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_temporal_times.csv',\n",
    "    ],\n",
    "    file_output=[\n",
    "        'network/bt_tn_weights.csv',\n",
    "        'network/bt_tn_probs.csv',\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_output='',\n",
    "    graph=None,\n",
    "    number_of_nodes=None,\n",
    "    number_of_times=None,\n",
    "    _version=0,\n",
    "    _omega=1,\n",
    "    _gamma=0.0001,\n",
    "    _epsilon=1,\n",
    "    _distance=1,\n",
    "    _alpha=0.5,\n",
    "    save_weights=True,\n",
    "    save_probs=True,\n",
    "    output_weights=True,\n",
    "    output_probs=True\n",
    "):\n",
    "    \"\"\"\n",
    "    {(u,v):temporal_weight}\n",
    "    {(u,v):probability}\n",
    "    Calculate and set the edge weight (save in dictionary / into graph)\n",
    "    Also, create edge probability for spread (according to weights)\n",
    "    Versions:\n",
    "        0: None (or fixed value of omega)\n",
    "        1: (1/2)^(lengh of horizontal path)\n",
    "        2: 1/(lenght of horizontal path)\n",
    "        3: 1/(log2(lenght of horizontal path))\n",
    "    Parameters:\n",
    "        * graph (networkx): directed temporal network i.e. DT\n",
    "        * undirected (bool): True, if 'dt' is temporaly bi-directional\n",
    "        * omega (float): horizontal edge weight - only used when penalize=False (e.g. 1 ,10 ,100, 1000 ...)\n",
    "        * epsilon (float): crossed edge weight (e.g. 0.1, 1 ,10 ,100 ...)\n",
    "        * gamma (float): horizontal edge teleport weight (e.g. 0.0001, 0.001, 0.01 ...)\n",
    "        * penalize (bool): True, if want to weight horizontal-edges dynamically\n",
    "        * distance (float): penalizing (V1,V2,V3) weight - smaller => slower decay\n",
    "        * alpha (float): penalizing (only in V1)\n",
    "    Notes:\n",
    "        * when we don't penalize the horizontal edges\n",
    "            set a fix value for weights of epsilon and omega (e.g. 1 & 1 or 1 & 10, ...)\n",
    "        * When we penalize\n",
    "            larger alpha => slower decay in weights\n",
    "            larger distance => faster decay in weights\n",
    "            find a balance between 'alpha' and 'distance' based on number of consecutive horizontals\n",
    "    \"\"\"\n",
    "    def has_crossed_edge(in_graph, in_N, in_node):\n",
    "        in_parent = in_node % in_N\n",
    "        for pre in in_graph.predecessors(in_node):\n",
    "            if pre % in_N != in_parent:\n",
    "                return True\n",
    "        # Else = no crossed edge found ...\n",
    "        return False\n",
    "\n",
    "    # Amend input / output file names using label\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read time-ordered network and size of node from original network\n",
    "    if graph is None:\n",
    "        graph = tn_bt_light_read()\n",
    "\n",
    "    if number_of_nodes is None:\n",
    "        N = line_count(file_input[1])\n",
    "    else:\n",
    "        N = number_of_nodes\n",
    "\n",
    "    if number_of_times is None:\n",
    "        T = line_count(file_input[2])\n",
    "    else:\n",
    "        T = number_of_times\n",
    "        # T = graph.number_of_nodes() // N\n",
    "\n",
    "    # Edge-Weight dictionary\n",
    "    # {(u,v):weight}\n",
    "    ew = {}\n",
    "\n",
    "    # Horizontal edges helper dictionary\n",
    "    hedges = {}\n",
    "\n",
    "    # If node in_degree = 1 then node 'u' has only '1' horizontal-edge of (v,u)\n",
    "    nid = 1\n",
    "\n",
    "    _directed = True\n",
    "    # If node 0 at time 1 connects to node 0 at time 0 -> undirected\n",
    "    if (1 * N) + 0 in graph.predecessors(0):\n",
    "        _directed = False\n",
    "        nid += 2\n",
    "\n",
    "    _teleport = False\n",
    "    # If node 0 at time 0 connects to node 0 at time 2 -> teleport\n",
    "    if (2 * N) + 0 in graph.successors(0):\n",
    "        _teleport = True\n",
    "\n",
    "    _loop = False\n",
    "    # If node 0 at time T connects to node 0 at time 0 -> loop\n",
    "    if (T * N) + 0 in graph.predecessors(0):\n",
    "        _loop = True\n",
    "\n",
    "    if _version == 0:  # Without penalization\n",
    "        for u, v in graph.edges():\n",
    "            # u & v have same parent => (u,v) is horizontal\n",
    "            time_delta = abs(v - u) // N\n",
    "            parent_u = u % N\n",
    "            parent_v = v % N\n",
    "            if parent_u != parent_v:  # Crossed\n",
    "                ew[(u, v)] = _epsilon  # E.g. 1\n",
    "            else:  # Horizontal\n",
    "                if time_delta > 1:  # Teleport\n",
    "                    # If _teleport = False during TN, these edges do not exist\n",
    "                    ew[(u, v)] = _gamma  # E.g. 0.0001\n",
    "                else:  # time_delta = 1\n",
    "                    # Node v is node u at one timestamp after\n",
    "                    ew[(u, v)] = _omega\n",
    "\n",
    "    else:  # Penalize ...\n",
    "        # Nodes [0-N]\n",
    "        for u, v in sorted(graph.edges(), key=lambda x: x[0]):\n",
    "            time_delta = abs(v - u) // N\n",
    "            parent_u = u % N\n",
    "            parent_v = v % N\n",
    "            if parent_u != parent_v:\n",
    "                ew[(u, v)] = _epsilon\n",
    "            else:\n",
    "                if time_delta > 1:\n",
    "                    ew[(u, v)] = _gamma\n",
    "                else:\n",
    "                    # Node v has crossed edge\n",
    "                    # if graph.in_degree(v) != nid:  # 1 or 2\n",
    "                    if has_crossed_edge(graph, N, v):\n",
    "                        hedges[(u, v)] = _omega  # E.g. 1\n",
    "                    else:\n",
    "                        # Node v does not have crossed edge\n",
    "                        # Look at the previous edge weight (if exsit, otherwise return omega)\n",
    "                        hedges[(u, v)\n",
    "                               ] = hedges.get((u - N, u), _omega) + _distance\n",
    "\n",
    "        # Update weights based on version of penalization\n",
    "        if _version == 1:\n",
    "            # Decay exponentially fast\n",
    "            # (parameter_alpha)^(distance) e.g. 1/2^1 , 1/2^2, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = _alpha**(weight - 1)\n",
    "        elif _version == 2:\n",
    "            # Decay very fast\n",
    "            # 1/(distance) e.g. 1/2, 1/3, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = 1 / weight\n",
    "        elif _version == 3:\n",
    "            # Decay fast\n",
    "            # 1/log2(distance + 1) e.g. 1/log2, 1/log3, ...\n",
    "            for edge, weight in hedges.items():\n",
    "                hedges[edge] = 1 / np.log2(weight + 1)\n",
    "\n",
    "        # Finish by updating helper dictionary to 'ew'\n",
    "        ew.update(hedges)\n",
    "\n",
    "    if save_weights:\n",
    "        # dict_save(ew, file_output[0][:-4])  # npy\n",
    "        # dict_save(ew, file_output[0], method='c', sort=True)  # csv\n",
    "        pd.DataFrame.from_dict(ew, orient='index').to_csv(file_output[0])\n",
    "        if _version > 0:\n",
    "            ls = sorted(ew.items())\n",
    "            ls1, ls2 = zip(*ls)\n",
    "            plt.figure()\n",
    "            ax = sns.distplot(\n",
    "                ls2,\n",
    "                kde=True,\n",
    "            )\n",
    "\n",
    "    if output_weights:\n",
    "        for e, w in sorted(ew.items(), key=lambda x: x[0]):\n",
    "            if e[0] % N == e[1] % N:  # H\n",
    "                if graph.in_degree(e[1]) == nid:\n",
    "                    print('{}\\t->\\t{}\\t{}'.format(e[0], e[1], w))\n",
    "\n",
    "    # Normalize the weight of node each nodes out-links over Max() or Sum()\n",
    "    # To have spread probability of each link\n",
    "    # First, we scale horizontal edges to [0-0.5] and crossed ones\n",
    "\n",
    "    # Edge-Probability dictionary\n",
    "    # {(u,v):probability}\n",
    "    # prob = {}\n",
    "    prob = ew.copy()\n",
    "\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        # u & v have same parent => (u,v) is horizontal\n",
    "        time_delta = abs(v - u) // N\n",
    "        parent_u = u % N\n",
    "        parent_v = v % N\n",
    "        if parent_u != parent_v:  # Crossed\n",
    "            ew[(u, v)] = _epsilon  # E.g. 1\n",
    "        else:  # Horizontal\n",
    "            if time_delta > 1:  # Teleport\n",
    "                # If _teleport = False during TN, these edges do not exist\n",
    "                ew[(u, v)] = _gamma  # E.g. 0.0001\n",
    "            else:  # time_delta = 1\n",
    "                # Node v is node u at one timestamp after\n",
    "                ew[(u, v)] = _omega\n",
    "\n",
    "    # X_scaled = (b - a) * ((X_scaled - min(X_scaled)) / (max(X_scaled) - min(X_scaled))) + a\n",
    "    # X_scaled = (0.5 * np.array(X_scaled)) + 0.5\n",
    "\n",
    "    for n in graph:\n",
    "        parent_n = n % N\n",
    "        w_c = []  # Weights of crossed edges\n",
    "        w_h = []  # Weights of horizontal edges\n",
    "        for s in graph.successors(n):\n",
    "            parent_s = s % N\n",
    "            if parent_n != parent_s:  # Crossed\n",
    "                # We read the weight of edge coming from aggregated network\n",
    "                w_c.append(graph[n][s]['w'])\n",
    "            else:  # Horizontal or teleport ...\n",
    "                w_h.append(prob[(n, s)])\n",
    "        # If more than just one horizontal or crossed\n",
    "        if len(w_c) + len(w_h) > 1:\n",
    "            w_c_m = 0\n",
    "            if len(w_c) > 0:\n",
    "                w_c_m = max(w_c)\n",
    "            w_h_m = 0\n",
    "            if len(w_h) > 0:\n",
    "                w_h_m = max(w_h)\n",
    "            # Adjust weights of horizontal according to maximum of crossed\n",
    "            if w_h_m > w_c_m:\n",
    "                w_h = [item if item < w_c_m else w_c_m for item in w_h]\n",
    "            # Update probabilities\n",
    "            for s in graph.successors(n):\n",
    "                parent_s = s % N\n",
    "                if parent_n != parent_s:\n",
    "                    # Option 1\n",
    "                    # Leave them to original value from temporal network e.g. [0.5,1]\n",
    "                    # Most likely many are 0.5 which have 50/50 chance of transmission\n",
    "                    prob[(n, s)] = graph[n][s]['w']\n",
    "                    # Option 2\n",
    "                    # Scale up so maximum is 1.0 while rest are relatively scaled up too\n",
    "                    # prob[(n, s)] = graph[n][s]['w'] / w_c_m\n",
    "                else:\n",
    "                    if prob[(n, s)] < w_c_m:\n",
    "                        # Option 1\n",
    "                        # Leave it as it is\n",
    "                        # prob[(n, s)] = prob[(n, s)]\n",
    "                        pass\n",
    "                        # Option 2\n",
    "                        # Normalized it with maximum value\n",
    "                        # prob[(n, s)] /= w_c_m\n",
    "                    else:\n",
    "                        # Means horizontal weight is larger than max(crossed)\n",
    "                        # I.e. If exist a C(rossed) < 1 => exist H(orizontal) = 1\n",
    "                        # Option 1\n",
    "                        # Scale down 'H' to max('C') and end up being normalized to 1.0\n",
    "                        # prob[(n, s)] = 1.0\n",
    "                        # Option 2\n",
    "                        # Scale down to a random value [0.5,1]\n",
    "                        prob[(n, s)] = 0.5 * np.random.sample() + 0.5\n",
    "\n",
    "    if save_probs:\n",
    "        pd.DataFrame.from_dict(prob, orient='index').to_csv(file_output[1])\n",
    "\n",
    "    # Check the distribution of probabilities\n",
    "    if output_probs:\n",
    "        ls = sorted(prob.items())\n",
    "        ls1, ls2 = zip(*ls)\n",
    "        plt.figure()\n",
    "        ax = sns.distplot(ls2, kde=True)\n",
    "        # ax = sns.distplot([i for i in ls2 if i != 1], kde=True)\n",
    "\n",
    "    return ew, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T01:57:41.716593Z",
     "start_time": "2021-02-23T01:57:41.705405Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ew_read(file_input=['network/bt_tn_weights.csv'], label_input=''):\n",
    "    \"\"\"\n",
    "    Read edge weights of time-ordered graph\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Read edge weights file\n",
    "    ew = pd.read_csv(file_input[0], names=['u', 'v', 'w'])\n",
    "    # Fix index from tuple of (u,v)\n",
    "    ew.index = list(zip(ew.u, ew.v))\n",
    "    # Only keep the weights\n",
    "    ew = ew['w']\n",
    "    # Convert to dict\n",
    "    ew = ew.to_dict()\n",
    "    return ew\n",
    "\n",
    "\n",
    "def prob_read(file_input=['network/bt_tn_probs.csv'], label_input=''): # TODO\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Read edge transmission probability file\n",
    "    prob = pd.read_csv(file_input[1], index_col=0)\n",
    "    prob = prob.to_dict()['0']\n",
    "    prob = {eval(k): float(v) for k, v in prob.items()}\n",
    "    # prob = dict_load(file_input[1][:-4])\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Constant Penalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T20:08:18.580093Z",
     "start_time": "2021-02-20T20:08:18.576064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ew = ew_create(\n",
    "#     version=0,\n",
    "#     omega=0.5,\n",
    "#     epsilon=1,\n",
    "#     gamma=0.0001,\n",
    "#     distance=1,\n",
    "#     alpha=0.5,\n",
    "#     save_weights=False,\n",
    "#     output_weights=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Dynamic Penalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T20:41:13.140654Z",
     "start_time": "2021-02-20T20:41:13.136333Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Current config\n",
    "# ew = ew_create(\n",
    "#     label_output='',\n",
    "#     version=3,\n",
    "#     omega=1,\n",
    "#     epsilon=1,\n",
    "#     gamma=0.0001,\n",
    "#     distance=0.1,\n",
    "#     alpha=0.5,\n",
    "#     save_weights=True,\n",
    "#     output_weights=False,\n",
    "#     plot_weights=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-20T20:40:57.340928Z",
     "start_time": "2021-02-20T20:40:57.336488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# New config\n",
    "# This shows more uniform dist for values != 1 and less penalize magnitude compared to omega = 1.0 \n",
    "# ew = ew_create(\n",
    "#     label_output='',\n",
    "#     version=3,\n",
    "#     omega=1.1,\n",
    "#     epsilon=1,\n",
    "#     gamma=0.0001,\n",
    "#     distance=0.1,\n",
    "#     alpha=0.5,\n",
    "#     save_weights=False,\n",
    "#     output_weights=False,\n",
    "#     plot_weights=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ew = ew_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Spread (TODO: Must complete ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spread Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def spread(graph, initial_nodes=None, initial_size=1, time_stop=100):\n",
    "    \"\"\"\n",
    "    Spread\n",
    "    \"\"\"\n",
    "\n",
    "    N = graph.number_of_nodes()\n",
    "    # nodes = graph.nodes  # TODO: can be removed\n",
    "    time_passed = 0\n",
    "    # Susceptible = 0\n",
    "    # Infected = 1\n",
    "    # Recovered = 2\n",
    "    # Blocked = -1\n",
    "    # Every node is susecptible at first\n",
    "    status = {n: 0 for n in graph.nodes}\n",
    "    # Randomly select (one or more) patient zero(s)\n",
    "    if initial_nodes is None:\n",
    "        # Fraction infected ratio\n",
    "        if initial_size > 0 and initial_size < 1:\n",
    "            initial_size = int(initial_size * N)\n",
    "            # Case when ratio is too low compared to N\n",
    "            if initial_size == 0:\n",
    "                initial_size = 1\n",
    "        # Specific number of infected nodes (e.g. 1)\n",
    "        initial_nodes = np.random.choice(graph.nodes,\n",
    "                                         size=initial_size,\n",
    "                                         replace=False)\n",
    "    # Update initial infected and remaining susecptible\n",
    "    for n in initial_nodes:\n",
    "        status[n] = 1\n",
    "    i_nodes = set(initial_nodes)\n",
    "    s_nodes = set([n for n in status if status[n] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Influence Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Linear Threshold Model (LTM):**    \n",
    "In this model, each node has a random threshold __theta_v__ in range [0,1]. For simplicity, we can set all the nodes to an specific threshold of __theta__.    \n",
    "Also, ledges of each node is associated with a weight and summation of the wights <= 1, basically, if the summation of these links weighs > threshold => activate the node    \n",
    "    \n",
    "**Independent Cascade Model (ICM):**    \n",
    "Unlike LTM, in this model, nodes do not have threshold. Se when a node 'v' become active it activate neighbors\n",
    "***\n",
    "In out temporal network, the link has a temporal weight or __tp__. We can (L1) normalize the tp's so for each node, so SUM of wights = 1\n",
    "In other words, we turn tp weights to __p__ weights required for __probabilistic contagion__ or linear threshold model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T19:37:08.056888Z",
     "start_time": "2020-09-29T19:37:08.045037Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def influence_maximization(graph,\n",
    "                           n_nodes=1,\n",
    "                           probability=0.2,\n",
    "                           n_iters=1000,\n",
    "                           output_log=False,\n",
    "                           output_result=True):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        n_nodes: number of desired nodes to include in influential set\n",
    "        p: probability of walking to neighbor in range [0,1]\n",
    "        n_iters: number of iteration of different cascase using a given set of nodes\n",
    "    Return:\n",
    "        influential_nodes: set of most influential node with size < n_nodes\n",
    "        max_spread: maximum of average spread for each node in the set\n",
    "    Note:\n",
    "        - we find the set of influential nodes but we don't know whos is the most influential one\n",
    "        unless we run the set from size 1 to k in a progressive approach and rank the node in a list\n",
    "        - in temporal network if node is in earlier timestamp it become more influential because it reaches more\n",
    "        and to avoid this we can define a time windows with fix size or loop the end time to start time\n",
    "    \"\"\"\n",
    "    max_spreads = []\n",
    "    node_spreads = []\n",
    "    progress_spreads = []\n",
    "    influential_nodes = []\n",
    "    N = graph.number_of_nodes()\n",
    "    for n_idx in range(n_nodes):\n",
    "        best_node = -1\n",
    "        best_spread = -np.inf\n",
    "        remaining_nodes = graph.nodes - influential_nodes\n",
    "        # Look into set of not influential nodes and find a new candidates\n",
    "        for node in remaining_nodes:\n",
    "            candidate_nodes = influential_nodes + [node]\n",
    "            spread = []\n",
    "            for i in range(n_iters):\n",
    "                new_active, visited = candidate_nodes[:], candidate_nodes[:]\n",
    "                while new_active:\n",
    "                    # Get neighbours of newly activated node\n",
    "                    targets = []\n",
    "                    for n in new_active:\n",
    "                        targets += graph.neighbors(n)\n",
    "                    # Calculatie if any of those neighbours can be activated, if yes add them to new_ones\n",
    "                    np.random.seed(i)\n",
    "                    success = np.random.uniform(0, 1,\n",
    "                                                len(targets)) < probability\n",
    "                    # new_ones = list(np.extract(success, sorted(targets)))\n",
    "                    new_ones = list(np.extract(success, targets))\n",
    "                    # Checking which ones in new_ones are not in visited\n",
    "                    # Only adding them to our visited so that no duplicate in visited\n",
    "                    # Basically, this is independent cascade model where a node can not get infeccted more than once\n",
    "                    new_active = list(set(new_ones) - set(visited))\n",
    "                    visited += new_active\n",
    "                # Increase number of overal visited or spread\n",
    "                spread.append(len(visited))\n",
    "            # Average the spread over number of iterations\n",
    "            avg_spread = np.mean(spread)\n",
    "            # Log\n",
    "            if output_log:\n",
    "                print('Candidate node {} has average spread {}'.format(\n",
    "                    node, avg_spread))\n",
    "                print('Candidate list:', candidate_nodes)\n",
    "            # If candidate node is better than best node -> replace them\n",
    "            if avg_spread > best_spread:\n",
    "                progress_spreads.append((node, avg_spread))\n",
    "                best_spread = avg_spread\n",
    "                best_node = node\n",
    "        # Add the best node and its average spread to the answer\n",
    "        influential_nodes.append(best_node)\n",
    "        if n_idx == 0:\n",
    "            node_spreads.append(best_spread)\n",
    "        else:\n",
    "            node_spreads.append(best_spread - max_spreads[n_idx - 1])\n",
    "        max_spreads.append(best_spread)\n",
    "\n",
    "    if output_log:\n",
    "        print('Influential nodes and their spreads:')\n",
    "        print(progress_spreads)\n",
    "\n",
    "    if output_result:\n",
    "        print('Influence Maximization\\n---')\n",
    "        print('N =', graph.number_of_nodes())\n",
    "        print('L =', graph.number_of_edges())\n",
    "        print(\n",
    "            '---\\n{} of the most influential nodes with highest spread\\naveraged over {} iterations'\n",
    "            .format(n_nodes, n_iters))\n",
    "        dash = '-' * 60\n",
    "        print(dash)\n",
    "        print(\n",
    "            '{:<8}{:^8}{:^20}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:>8}'.format(\n",
    "                'Rank', 'Node', 'Node', 'Node', 'Total', 'Spread', 'In', 'Out',\n",
    "                'C', 'B', 'Page'))\n",
    "        print(\n",
    "            '{:<8}{:^8}{:^20}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:>8}'.format(\n",
    "                '', 'Index', 'Name', 'Spread', 'Spread', 'Ratio (%)', 'Degree',\n",
    "                'Degree', 'Crt', 'Crt', 'Crt'))\n",
    "        print(dash)\n",
    "\n",
    "        in_degree_sequence = sorted([(d, n) for n, d in graph.in_degree()],\n",
    "                                    reverse=True)\n",
    "        out_degree_sequence = sorted([(d, n) for n, d in graph.out_degree()],\n",
    "                                     reverse=True)\n",
    "        in_degree_max = in_degree_sequence[0][0]\n",
    "        out_degree_max = in_degree_sequence[0][0]\n",
    "        cc = nx.closeness_centrality(graph)\n",
    "        cc_sorted = sorted(cc.items(), key=lambda x: x[1], reverse=True)\n",
    "        cc_max = cc_sorted[0][1]\n",
    "        bt = nx.betweenness_centrality(graph)\n",
    "        bt_sorted = sorted(bt.items(), key=lambda x: x[1], reverse=True)\n",
    "        bt_max = bt_sorted[0][1]\n",
    "        pg = nx.pagerank(graph)\n",
    "        pg_sorted = sorted(pg.items(), key=lambda x: x[1], reverse=True)\n",
    "        pg_max = pg_sorted[0][1]\n",
    "\n",
    "        for i in range(len(influential_nodes)):\n",
    "            print('{:<8}{:^8}{:^20}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:^8}{:>8}'.\n",
    "                  format(i, graph.nodes[influential_nodes[i]]['idx'],\n",
    "                         influential_nodes[i], round(node_spreads[i],\n",
    "                                                     2), max_spreads[i],\n",
    "                         round(max_spreads[i] / N * 100, 2),\n",
    "                         graph.in_degree(influential_nodes[i]),\n",
    "                         graph.out_degree(influential_nodes[i]),\n",
    "                         round(cc[influential_nodes[i]]/cc_max, 2),\n",
    "                         round(bt[influential_nodes[i]]/bt_max, 2),\n",
    "                         round(pg[influential_nodes[i]]/pg_max, 2)))\n",
    "\n",
    "\n",
    "#         print(*influential_nodes)\n",
    "#         print(*max_spreads)\n",
    "\n",
    "    return influential_nodes, max_spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "g = nx.read_gpickle('file/sleeping_giant_directed.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inf, sp = influence_maximization(g,\n",
    "                                 n_nodes=10,\n",
    "                                 probability=0.5,\n",
    "                                 n_iters=100,\n",
    "                                 output_log=False,\n",
    "                                 output_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# HITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:45:54.396331Z",
     "start_time": "2021-02-23T18:45:54.355844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hits(\n",
    "    file_input=[\n",
    "        'network/bt_tn_full_network.gpickle',\n",
    "        'network/bt_tn_light_network.gpickle',\n",
    "        'network/bt_temporal_times.csv',\n",
    "        'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_tn_weights.csv',\n",
    "    ],\n",
    "    file_output=[\n",
    "        'hits/a.csv',\n",
    "        'hits/h.csv',\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_output='',\n",
    "    graph=None,\n",
    "    times=None,\n",
    "    nodes=None,\n",
    "    ew=None,\n",
    "    nstart=None,\n",
    "    full=False,\n",
    "    version=0,\n",
    "    sigma=0.85,\n",
    "    max_iter=100,\n",
    "    tol=1.0e-8,\n",
    "    norm_max=True,\n",
    "    norm_final_l1=True,\n",
    "    norm_final_l2=False,\n",
    "    norm_iter=False,\n",
    "    norm_degree=False,\n",
    "    norm_damping=False,\n",
    "    output=False,\n",
    "    plot=False,\n",
    "    save=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate HITS centrality of time-ordered network (TON)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    version : int\n",
    "        (1) NetworkX\n",
    "            Normalize scores with SUM=1 Range [0,1]\n",
    "            1/max normalization in each iteration and final normalization at the end\n",
    "        (2) Book\n",
    "            Normalize scores with SUM=1 and DEVIATION=1 and range=[0,1]\n",
    "        (3) Paper (randomization or teleportation)\n",
    "            No regular normalization (v1 & v2), just in-out-normalization & damping/teleport/randomize\n",
    "        (4) NetX + teleport or PARTIAL randomization (NO in-out-normalization)\n",
    "        (5) Book + teleport or PARTIAL randomization (NO in-out-normalization)\n",
    "        (6) Paper + normalization of scores at each iteration\n",
    "        (7) Paper + l2 normalization of score at the end\n",
    "        (0) Default -> no change to parameters (norm_*) -> version 1 (NetX) + other manually set parameters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict , dict\n",
    "        authority and hub scores as {node:score}\n",
    "    \"\"\"\n",
    "\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Finishing iteration\n",
    "    finish_iter = max_iter\n",
    "\n",
    "    h = {}  # Hub scores\n",
    "    a = {}  # Authority scores\n",
    "\n",
    "    # Read graph\n",
    "    if graph == None:\n",
    "        if full:  # Full version\n",
    "            graph = tn_bt_full_read(file_input=[file_input[0]])\n",
    "        else:  # Light version\n",
    "            graph = tn_bt_light_read(file_input=[file_input[1]])\n",
    "\n",
    "    # Number of nodes in time-ordered network (TON)\n",
    "    N_t = graph.number_of_nodes()\n",
    "\n",
    "    # Times\n",
    "    if times is None:\n",
    "        times = list(temporal_bt_times_read([file_input[2]]))\n",
    "        T = line_count(file_input[2])\n",
    "    else:\n",
    "        T = len(times)\n",
    "\n",
    "    # Nodes\n",
    "    if nodes is None:\n",
    "        nodes = list(temporal_bt_nodes_read([file_input[3]]))\n",
    "        N = line_count(file_input[3])\n",
    "    else:\n",
    "        N = len(nodes)\n",
    "        # N = graph.number_of_nodes() // T\n",
    "\n",
    "    # Edge-weight\n",
    "    if ew is None: ew = ew_read(file_input=[file_input[4]])\n",
    "\n",
    "    # Damping-coefficient or normalization-parameter (default = 0.85) is same as sigma in supra-matrix model which applies to crossed edges only\n",
    "    # But here applies to all of the edges (crossed and horizontals ...)\n",
    "    damping = 1 - sigma  # Default = 0.15\n",
    "\n",
    "    # NetX\n",
    "    if version == 1:\n",
    "        norm_max = True  #\n",
    "        norm_final_l1 = True  #\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = False\n",
    "        norm_degree = False\n",
    "        norm_damping = False\n",
    "\n",
    "    # Book\n",
    "    if version == 2:\n",
    "        norm_max = False\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = True  #\n",
    "        norm_degree = False\n",
    "        norm_damping = False\n",
    "\n",
    "    # Paper\n",
    "    if version == 3:\n",
    "        norm_max = False\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = False\n",
    "        norm_degree = True  #\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # NetX + teleport\n",
    "    if version == 4:\n",
    "        norm_max = True  #\n",
    "        norm_final_l1 = True  #\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = False\n",
    "        norm_degree = False\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Book + teleport\n",
    "    if version == 5:\n",
    "        norm_max = False\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = True  #\n",
    "        norm_degree = False\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Paper (degree normalization + teleport) + NetX\n",
    "    if version == 6:\n",
    "        norm_max = True  #\n",
    "        norm_final_l1 = True  #\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = False\n",
    "        norm_degree = True  #\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Paper (degree normalization + teleport) + Book\n",
    "    if version == 7:\n",
    "        norm_max = False\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = True  #\n",
    "        norm_degree = True  #\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Paper + final norm L2\n",
    "    if version == 8:\n",
    "        norm_max = False\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = True  #\n",
    "        norm_iter = False\n",
    "        norm_degree = True  #\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Paper + final norm L2\n",
    "    if version == 9:\n",
    "        norm_max = True  #\n",
    "        norm_final_l1 = False\n",
    "        norm_final_l2 = False\n",
    "        norm_iter = False\n",
    "        norm_degree = True  #\n",
    "        norm_damping = True  #\n",
    "\n",
    "    # Initialize scores\n",
    "    if nstart is None:\n",
    "        h = dict.fromkeys(graph, 1.0 / graph.number_of_nodes())\n",
    "    else:\n",
    "        h = nstart\n",
    "        # Normalize starting vector\n",
    "        s = 1.0 / sum(h.values())\n",
    "        for k in h:\n",
    "            h[k] *= s\n",
    "\n",
    "    # Power iteration\n",
    "    if output: print(f'Calculating HITS (Version {version}) ...')\n",
    "    # Set start time\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        if i != 0 and i % 10 == 0:\n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            if output: print(f'Iteration {i} completed {elapsed:.0f} seconds')\n",
    "            # Reset start time\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "        # Save the last calculated hub score\n",
    "        hlast = h\n",
    "\n",
    "        # Initialize scores for new iteration\n",
    "        h = dict.fromkeys(hlast.keys(), 0)\n",
    "        a = dict.fromkeys(hlast.keys(), 0)\n",
    "\n",
    "        # Authority\n",
    "        # ---------\n",
    "        # Left multiply a^T=hlast^T*G\n",
    "        # Authority of neighbors get value from hub of current node, influenced by weight of edge\n",
    "\n",
    "        # (1) default HITS -> no degree normalization\n",
    "        if not norm_degree:\n",
    "            for n in h:\n",
    "                for nbr in graph[n]:\n",
    "                    a[nbr] += hlast[n] * ew.get((n, nbr), 1)\n",
    "        # (2) degree normalization\n",
    "        else:\n",
    "            for n in h:\n",
    "                for nbr in graph[n]:\n",
    "                    a[nbr] += hlast[n] / graph.in_degree(nbr) * ew.get(\n",
    "                        (n, nbr), 1\n",
    "                    )\n",
    "\n",
    "        # Paper\n",
    "        # Apply damping factor OR random-walk OR teleport\n",
    "        if norm_damping:\n",
    "            for n in a:\n",
    "                a[n] *= sigma\n",
    "                a[n] += damping\n",
    "\n",
    "        # Book\n",
    "        # Normalized authority scores over root of sum of squares after each calculation\n",
    "        # In this case we dont need final normalization\n",
    "        if norm_iter:\n",
    "            s = 1.0 / math.sqrt(sum([x**2 for x in a.values()]))\n",
    "            for n in a:\n",
    "                a[n] *= s\n",
    "\n",
    "        # Hub\n",
    "        # ---\n",
    "        # Multiply h=Ga\n",
    "        # Hub of current node get value from authority values of neighbor nodes, influenced by weight of edge\n",
    "\n",
    "        # (1) default HITS -> no degree normalization\n",
    "        if not norm_degree:\n",
    "            for n in h:\n",
    "                for nbr in graph[n]:\n",
    "                    h[n] += a[nbr] * ew.get((n, nbr), 1)\n",
    "        # (2) degree normalization\n",
    "        else:\n",
    "            for n in h:\n",
    "                for nbr in graph[n]:\n",
    "                    h[n] += a[nbr] / graph.out_degree(n) * ew.get((n, nbr), 1)\n",
    "\n",
    "        # Paper\n",
    "        # Apply damping factor OR randomize\n",
    "        if norm_damping:\n",
    "            for n in h:\n",
    "                h[n] *= sigma\n",
    "                h[n] += damping\n",
    "\n",
    "        # Book\n",
    "        # Normalized hub scores over root of sum of squares after each calculation\n",
    "        # In this case we dont need final normalization\n",
    "        if norm_iter:\n",
    "            s = 1.0 / math.sqrt(sum([x**2 for x in h.values()]))\n",
    "            for n in h:\n",
    "                h[n] *= s\n",
    "\n",
    "        # END of one iteration\n",
    "\n",
    "        # NetX\n",
    "        # Normalize scores over maximum\n",
    "        # Stopping score from getting really large\n",
    "        if norm_max:\n",
    "            a_max = 1.0 / max(a.values())\n",
    "            h_max = 1.0 / max(h.values())\n",
    "            for n in a:  # OR for n in h => both are same\n",
    "                a[n] *= a_max\n",
    "                h[n] *= h_max\n",
    "\n",
    "        # Check convergence\n",
    "        err = sum([abs(h[n] - hlast[n]) for n in h])\n",
    "        if err < tol:\n",
    "            finish_iter = i\n",
    "            if output: print(f'Successful after {finish_iter} iteration')\n",
    "            break\n",
    "\n",
    "    # Program did not meet treshhold for convergence\n",
    "    if finish_iter == max_iter:\n",
    "        if output: print(f'Not converged after {finish_iter} iterations')\n",
    "\n",
    "    # NetX\n",
    "    # Last normalization (L1) using sum\n",
    "    # Output in range (0-1) with sum-all = 1\n",
    "    if norm_final_l1:\n",
    "        a_sum = 1.0 / sum(list(a.values()))\n",
    "        h_sum = 1.0 / sum(list(h.values()))\n",
    "        for n in a:  # OR for n in h => both are same\n",
    "            a[n] *= a_sum\n",
    "            h[n] *= h_sum\n",
    "\n",
    "    # Last normalization (L2) using sum squred\n",
    "    if norm_final_l2 and not norm_final_l1:\n",
    "        a_sum = 1.0 / math.sqrt(sum([x**2 for x in a.values()]))\n",
    "        h_sum = 1.0 / math.sqrt(sum([x**2 for x in h.values()]))\n",
    "        for n in a:  # OR for n in h => both are same\n",
    "            a[n] *= a_sum\n",
    "            h[n] *= h_sum\n",
    "\n",
    "    # Save\n",
    "    if save:\n",
    "        pd.DataFrame.from_dict(a, orient='index'\n",
    "                               ).to_csv(file_output[0], header=False)\n",
    "        pd.DataFrame.from_dict(h, orient='index'\n",
    "                               ).to_csv(file_output[1], header=False)\n",
    "\n",
    "    # Plot distribution of scores\n",
    "    if plot:\n",
    "        # In most cases, A & H has same value so ploting one of them is enough\n",
    "        ls_a = sorted(a.values())  # ls_h = sorted(h.values())\n",
    "        ax = sns.displot(ls_a, kde=True, rug=True)\n",
    "        # BUT if they were different, we can create a dataframe and plot both together\n",
    "        # df = pd.DataFrame([a,h]).T\n",
    "        # df.columns = 'a h'.split()\n",
    "        # plt.figure()\n",
    "        # ax = sns.displot(data=df, kind='kde')\n",
    "\n",
    "    return a, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T01:02:08.820133Z",
     "start_time": "2021-02-24T01:02:08.793354Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hits_conditional(\n",
    "    file_input=['hits/a.csv', 'hits/h.csv'],\n",
    "    file_output=[\n",
    "        'hits/a_array.csv',\n",
    "        'hits/h_array.csv',\n",
    "        'hits/a_avg_node.csv',\n",
    "        'hits/a_avg_time.csv',\n",
    "        'hits/h_avg_node.csv',\n",
    "        'hits/h_avg_time.csv',\n",
    "        'hits/a_norm_node.csv',\n",
    "        'hits/a_norm_time.csv',\n",
    "        'hits/h_norm_node.csv',\n",
    "        'hits/h_norm_time.csv',\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_output='',\n",
    "    a=None,\n",
    "    h=None,\n",
    "    N=None,\n",
    "    T=None,\n",
    "    removed=False,\n",
    "    save=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create conditional centralities by normalizing or averaging HITS scores over time (column) or node (row)\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read scores from file\n",
    "    if a is None or h is None:\n",
    "        a, h = hits_read(file_input=file_input)\n",
    "\n",
    "    # Times\n",
    "    if T is None:\n",
    "        times = list(temporal_bt_times_read())\n",
    "        T = len(times)\n",
    "\n",
    "    # Nodes\n",
    "    if N is None:\n",
    "        nodes = list(temporal_bt_nodes_read())\n",
    "        N = len(nodes)\n",
    "\n",
    "    # Convert score dict to matrix (N x T)\n",
    "    A = np.zeros((N, T + 1))\n",
    "    H = np.zeros((N, T + 1))\n",
    "    # If graph is complete (no node has been removed)\n",
    "    if not removed:\n",
    "        a_sorted = [a[k] for k in sorted(a.keys())]\n",
    "        h_sorted = [h[k] for k in sorted(h.keys())]\n",
    "        A = np.reshape(a_sorted, (T + 1, N)).T\n",
    "        H = np.reshape(h_sorted, (T + 1, N)).T\n",
    "    else:  # Some of the nodes were removed\n",
    "        for node in a.keys():  # h.keys()\n",
    "            row = node % N  # node\n",
    "            col = node // N  # time\n",
    "            A[row, col] = a[node]\n",
    "            H[row, col] = h[node]\n",
    "\n",
    "    # Save A & H matrices\n",
    "    if save:\n",
    "        pd.DataFrame(A).to_csv(file_output[0], header=None, index=None)\n",
    "        pd.DataFrame(H).to_csv(file_output[1], header=None, index=None)\n",
    "\n",
    "    # Read A & H matrices\n",
    "    # A = pd.read_csv(file_input[0]).values\n",
    "    # H = pd.read_csv(file_input[1]).values\n",
    "\n",
    "    # Conditional Centralities\n",
    "    # ------------------------\n",
    "\n",
    "    # Average of score over time (column) or node (row)\n",
    "    A_avg_node = np.mean(A, axis=1)\n",
    "    A_avg_time = np.mean(A, axis=0)\n",
    "    H_avg_node = np.mean(H, axis=1)\n",
    "    H_avg_time = np.mean(H, axis=0)\n",
    "\n",
    "    # Normalized scores over time (column) or node (row)\n",
    "    # L1 is over sum(abs(x)) but scince all values are + L1 is over sum(x)\n",
    "    A_norm_node = normalize(A, axis=1, norm='l1')\n",
    "    A_norm_time = normalize(A, axis=0, norm='l1')\n",
    "    H_norm_node = normalize(H, axis=1, norm='l1')\n",
    "    H_norm_time = normalize(H, axis=0, norm='l1')\n",
    "    # OR\n",
    "    # A_norm_node = A / A.sum(axis=1, keepdims=True)\n",
    "    # A_norm_time = A / A.sum(axis=0, keepdims=True)\n",
    "    # H_norm_node = H / H.sum(axis=1, keepdims=True)\n",
    "    # H_norm_time = H / H.sum(axis=0, keepdims=True)\n",
    "\n",
    "    if save:\n",
    "        pd.DataFrame(A_avg_node).to_csv(file_output[2], header=None, index=None)\n",
    "        pd.DataFrame(A_avg_time).to_csv(file_output[3], header=None, index=None)\n",
    "        pd.DataFrame(H_avg_node).to_csv(file_output[4], header=None, index=None)\n",
    "        pd.DataFrame(H_avg_time).to_csv(file_output[5], header=None, index=None)\n",
    "        pd.DataFrame(A_norm_node).to_csv(file_output[6], header=None, index=None)\n",
    "        pd.DataFrame(A_norm_time).to_csv(file_output[7], header=None, index=None)\n",
    "        pd.DataFrame(H_norm_node).to_csv(file_output[8], header=None, index=None)\n",
    "        pd.DataFrame(H_norm_time).to_csv(file_output[9], header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T01:01:55.786403Z",
     "start_time": "2021-02-24T01:01:55.768572Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hits_read(\n",
    "    file_input=['hits/a.csv', 'hits/h.csv'],\n",
    "    label_input='',\n",
    "    output=False,\n",
    "    plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Read HITS centrality\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    # Read scores from file\n",
    "    if output: print('Reading HITS score ...')\n",
    "    a = pd.read_csv(file_input[0], names=['a'], index_col=0)\n",
    "    h = pd.read_csv(file_input[1], names=['h'], index_col=0)\n",
    "    # Take column a/h and convert to dict\n",
    "    a = a.to_dict()['a']\n",
    "    h = h.to_dict()['h']\n",
    "    # Plot\n",
    "    if plot: sns.displot(a.values(), kde=True, rug=True, legend=False)\n",
    "    return a, h\n",
    "\n",
    "\n",
    "def hits_conditional_read(\n",
    "    file_input=[\n",
    "        'hits/a_array.csv',\n",
    "        'hits/h_array.csv',\n",
    "        'hits/a_avg_node.csv',\n",
    "        'hits/a_avg_time.csv',\n",
    "        'hits/h_avg_node.csv',\n",
    "        'hits/h_avg_time.csv',\n",
    "        'hits/a_norm_node.csv',\n",
    "        'hits/a_norm_time.csv',\n",
    "        'hits/h_norm_node.csv',\n",
    "        'hits/h_norm_time.csv',\n",
    "    ],\n",
    "    label_input='',\n",
    "    return_matrices=[0, 1],\n",
    "    output=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Read conitional HITS centrality scores\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    return_matrices : list\n",
    "        [0,1] -> return matrices A & H only\n",
    "        range(0,10) -> return all conditional matrices\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "\n",
    "    # Return object names\n",
    "    rtn = {}\n",
    "    rtn_names = [\n",
    "        'A', 'H', 'a_avg_node', 'a_avg_time', 'h_avg_node', 'h_avg_time',\n",
    "        'a_norm_node', 'a_norm_time', 'h_norm_node', 'h_norm_time'\n",
    "    ]\n",
    "\n",
    "    # Read scores from file\n",
    "    if return_matrices == [0, 1]:  # Default\n",
    "        if output: print('Reading HITS score matrices ...')\n",
    "        A = pd.read_csv(file_input[0], header=None, index_col=False).values\n",
    "        H = pd.read_csv(file_input[1], header=None, index_col=False).values\n",
    "        return A, H\n",
    "    else:\n",
    "        if output: print('Readinging and returning ...')\n",
    "        for i in return_matrices:\n",
    "            if output: print(f'{rtn_names[i]}')\n",
    "            temp = pd.read_csv(\n",
    "                file_input[i], header=None, index_col=False\n",
    "            ).values\n",
    "            rtn[rtn_names[i]] = temp\n",
    "    return rtn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T01:01:49.492135Z",
     "start_time": "2021-02-24T01:01:49.429458Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hits_analyze(\n",
    "    file_input=[\n",
    "        'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_temporal_times.csv',\n",
    "    ],\n",
    "    file_net=['network/bt_tn_light_network.gpickle'],\n",
    "    file_ew=['network/bt_tn_weights.csv'],\n",
    "    file_hits=[\n",
    "        'hits/a.csv',\n",
    "        'hits/h.csv',\n",
    "        'hits/a_array.csv',\n",
    "        'hits/h_array.csv',\n",
    "        'hits/a_avg_node.csv',\n",
    "        'hits/a_avg_time.csv',\n",
    "        'hits/h_avg_node.csv',\n",
    "        'hits/h_avg_time.csv',\n",
    "        'hits/a_norm_node.csv',\n",
    "        'hits/a_norm_time.csv',\n",
    "        'hits/h_norm_node.csv',\n",
    "        'hits/h_norm_time.csv',\n",
    "    ],\n",
    "    file_output=['hits/report.csv', 'hits/top.csv'],\n",
    "    file_image=[\n",
    "        'hits/fig_a.pdf',\n",
    "        'hits/fig_h.pdf',\n",
    "        'hits/fig_a_report.pdf',\n",
    "        'hits/fig_h_report.pdf',\n",
    "        'hits/fig_a_corr.pdf',\n",
    "        'hits/fig_h_corr.pdf',\n",
    "        'hits/fig_mat.pdf',\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_hits='',\n",
    "    label_output='',\n",
    "    label_image='',\n",
    "    graph=None,\n",
    "    times=None,\n",
    "    nodes=None,\n",
    "    ew=None,\n",
    "    a=None,\n",
    "    h=None,\n",
    "    top=2,\n",
    "    section=4,\n",
    "    report_num=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze HITS scores\n",
    "        Find top rank nodes using averaged-score over time\n",
    "        Highlight top nodes of TON model and other info such as parent, time, in-out-degree, in-out-weight ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        a : dict\n",
    "            authority score {node:score}\n",
    "        h : dict\n",
    "            hub score {node:score}\n",
    "        graph: NetX\n",
    "            time-ordered network (TON) model\n",
    "        times : list\n",
    "        nodes : list\n",
    "        ew : dict\n",
    "            edge weights {(u,v):w}\n",
    "    \"\"\"\n",
    "    # Edit filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_hits = label_amend(file_hits, label_hits)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "    file_image = label_amend(file_image, label_image)\n",
    "\n",
    "    # Read scores from file\n",
    "    if a is None or h is None:\n",
    "        a, h = hits_read(file_input=file_hits[0:2])\n",
    "\n",
    "    # Read graph\n",
    "    if graph == None:\n",
    "        graph = tn_bt_light_read(file_input=file_net)\n",
    "\n",
    "    # Nodes\n",
    "    if nodes is None:\n",
    "        nodes = list(temporal_bt_nodes_read(file_input=[file_input[0]]))\n",
    "    N = len(nodes)\n",
    "\n",
    "    # Times\n",
    "    if times is None:\n",
    "        times = list(temporal_bt_times_read(file_input=[file_input[1]]))\n",
    "    T = len(times)\n",
    "\n",
    "    times.insert(0, times[0])\n",
    "    t_first = times[0].strftime('%d %B\\n%I %p')\n",
    "    t_last = times[-1].strftime('%d %B\\n%I %p')\n",
    "    t_0 = times[0] - pd.Timedelta(1, unit='h')\n",
    "    t_day_idx = []\n",
    "    t_day_week = []\n",
    "    t_day_date = []\n",
    "    t_hour_12 = []\n",
    "    t_hour_17 = []\n",
    "    for i, time in enumerate(times):\n",
    "        if t_0.weekday() != time.weekday():\n",
    "            t_day_idx.append(i)\n",
    "            t_day_week.append(time.strftime('%a'))\n",
    "            t_day_date.append(time.strftime('%d'))\n",
    "            t_0 = time\n",
    "        if time.strftime('%H') == '12':\n",
    "            t_hour_12.append(i)\n",
    "        if time.strftime('%H') == '17':\n",
    "            t_hour_17.append(i)\n",
    "\n",
    "    # Edge-weight\n",
    "    if ew is None: ew = ew_read(file_input=file_ew)\n",
    "\n",
    "    # Conditional HTIS scores\n",
    "    cs = hits_conditional_read(\n",
    "        file_input=file_hits[2:12],\n",
    "        label_input=label_hits,\n",
    "        return_matrices=range(0, 10)\n",
    "    )\n",
    "\n",
    "    # Authority Plot\n",
    "    # --------------\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(32, 20))\n",
    "    # (1)\n",
    "    # norm = mpl.colors.Normalize(vmin=0, vmax=1.0, clip=True)\n",
    "    # mapper = cm.ScalarMappable(norm=norm, cmap=cm.YlGnBu)\n",
    "    # (2)\n",
    "    # mapper = cm.get_cmap('YlGnBu', 10)  # 10 discrete colors\n",
    "    # force the first color entry to be grey\n",
    "    # (3)\n",
    "    cmap = plt.cm.tab10_r  # Define the colormap (tab10_r, YlGnBu, Wistia, ocean, cool)\n",
    "    # Extract all colors from the .jet map\n",
    "    cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "    # If tab10\n",
    "    cmaplist[0], cmaplist[2] = cmaplist[2], cmaplist[0]\n",
    "    # Force the first color entry to be grey\n",
    "    cmaplist[0] = (.8, .8, .8, 1.0)\n",
    "    # Create the new custom map\n",
    "    mapper = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'my cmap', cmaplist, cmap.N\n",
    "    )\n",
    "    # define the bins and normalize\n",
    "    bounds = np.linspace(0, 1, 11)\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    # TODO: we can modify and add removed nodes or score == 0\n",
    "    # as red dots or something like that in the plot, but we need to\n",
    "    # know if score == 0 is considered as removed or we get a removed_list\n",
    "    # of nodes as an input of the hits_analyze method\n",
    "\n",
    "    A = cs['A']\n",
    "    X = list(range(1, T + 2))\n",
    "    for row in range(len(A)):\n",
    "        sc = ax.scatter(\n",
    "            X,\n",
    "            [row + 1] * (T + 1),\n",
    "            s=A[row] * 1000,\n",
    "            # s=[4*x**2 for x in A[row] * 10],\n",
    "            c=A[row],\n",
    "            # c=[mapper.to_rgba(x) for x in A[row]],\n",
    "            marker='|',\n",
    "            alpha=0.8,\n",
    "            # vmin=0,\n",
    "            # vmax=1.0,\n",
    "            norm=norm,  # or use vmin/vmax\n",
    "            # cmap=cm.YlGnBu,\n",
    "            cmap=mapper,\n",
    "        )\n",
    "\n",
    "    A_avg_node = cs['a_avg_node']\n",
    "    ax.scatter(\n",
    "        [T + 3] * N,\n",
    "        list(range(1, N + 1)),\n",
    "        s=A_avg_node * 100,\n",
    "        c=A_avg_node,\n",
    "        marker='s',\n",
    "        alpha=0.5,\n",
    "        cmap=cm.Wistia,\n",
    "    )\n",
    "\n",
    "    A_avg_node_sort = sorted(A_avg_node, reverse=True)\n",
    "    A_avg_node_rank = [A_avg_node_sort.index(e) for e in A_avg_node]\n",
    "    for x, y in zip([T + 5] * N, list(range(1, N + 1))):\n",
    "        ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            str(A_avg_node_rank[y - 1] + 1),\n",
    "            color='black',\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    A_avg_time = cs['a_avg_time']\n",
    "    ax.scatter(\n",
    "        X,\n",
    "        [N + 1] * (T + 1),\n",
    "        s=A_avg_time * 100,\n",
    "        c=A_avg_time,\n",
    "        marker='s',\n",
    "        alpha=0.5,\n",
    "        cmap=cm.Wistia,\n",
    "    )\n",
    "\n",
    "    # Vertical date lines\n",
    "    ax.text(0, -0.2, 'Mon', rotation=90)\n",
    "    for i, point in enumerate(t_day_idx):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='green',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, t_day_week[i], rotation=90)\n",
    "    for i, point in enumerate(t_hour_12):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='red',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, 'Noon', rotation=90, fontsize=6)\n",
    "    for i, point in enumerate(t_hour_17):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='blue',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, '5 PM', rotation=90, fontsize=6)\n",
    "\n",
    "    # X axes\n",
    "    ax.set_xlabel('Time')\n",
    "    # ax.set_xticks(range(0, T + 1, 10))\n",
    "    # OR\n",
    "    ax.set_xticks([1] + t_day_idx + [T + 1])\n",
    "    ax.set_xticklabels([t_first] + t_day_date + [t_last])\n",
    "    # ax_label_first = ax.get_xticklabels()[0]\n",
    "    # ax_label_first.set_rotation(45)\n",
    "    # ax_label_first.set_ha('left')\n",
    "    # ax_label_last = ax.get_xticklabels()[-1]\n",
    "    # ax_label_last.set_rotation(45)\n",
    "    # ax_label_last.set_ha('left')\n",
    "\n",
    "    # Y axes\n",
    "    ax.set_ylabel('Node Index')\n",
    "    ax.set_yticks(range(1, N + 1))\n",
    "\n",
    "    # Figure labels\n",
    "    fig_title = 'Authority Score Over Time'\n",
    "    if len(label_output) > 0: fig_title = fig_title + ' (' + label_output + ')'\n",
    "    ax.set_title(fig_title)\n",
    "\n",
    "    # Color bar\n",
    "    # fig.colorbar(mapper, ax=ax, shrink=0.5, pad=0.01)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.5, pad=0.01)\n",
    "\n",
    "    # Save figure\n",
    "    fig.savefig(file_image[0], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)  # Close the figure window\n",
    "\n",
    "    # HUB\n",
    "    # ---\n",
    "\n",
    "    H = cs['H']\n",
    "    fig, ax = plt.subplots(figsize=(32, 20))\n",
    "    for row in range(len(H)):\n",
    "        ax.scatter(\n",
    "            X,\n",
    "            [row + 1] * (T + 1),\n",
    "            s=H[row] * 1000,\n",
    "            c=H[row],\n",
    "            marker='|',\n",
    "            alpha=0.8,\n",
    "            norm=norm,\n",
    "            cmap=mapper,\n",
    "        )\n",
    "\n",
    "    H_avg_node = cs['h_avg_node']\n",
    "    ax.scatter(\n",
    "        [T + 3] * N,\n",
    "        list(range(1, N + 1)),\n",
    "        s=H_avg_node * 100,\n",
    "        c=H_avg_node,\n",
    "        marker='s',\n",
    "        alpha=0.5,\n",
    "        cmap=cm.Wistia\n",
    "    )\n",
    "\n",
    "    H_avg_node_sort = sorted(H_avg_node, reverse=True)\n",
    "    H_avg_node_rank = [H_avg_node_sort.index(e) for e in H_avg_node]\n",
    "    for x, y in zip([T + 5] * N, list(range(1, N + 1))):\n",
    "        ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            str(H_avg_node_rank[y - 1] + 1),\n",
    "            color='black',\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    H_avg_time = cs['h_avg_time']\n",
    "    ax.scatter(\n",
    "        X,\n",
    "        [N + 1] * (T + 1),\n",
    "        s=H_avg_time * 100,\n",
    "        c=H_avg_time,\n",
    "        marker='s',\n",
    "        alpha=0.5,\n",
    "        cmap=cm.Wistia,\n",
    "    )\n",
    "\n",
    "    ax.text(0, -0.2, 'Mon', rotation=90)\n",
    "    for i, point in enumerate(t_day_idx):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='green',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, t_day_week[i], rotation=90)\n",
    "    for i, point in enumerate(t_hour_12):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='red',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, 'Noon', rotation=90, fontsize=6)\n",
    "    for i, point in enumerate(t_hour_17):\n",
    "        ax.axvline(\n",
    "            x=point,\n",
    "            ymin=0.03,\n",
    "            ymax=0.95,\n",
    "            linewidth=0.5,\n",
    "            color='blue',\n",
    "            alpha=0.25\n",
    "        )\n",
    "        ax.text(point - 1, -0.2, '5 PM', rotation=90, fontsize=6)\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_xticks([1] + t_day_idx + [T + 1])\n",
    "    ax.set_xticklabels([t_first] + t_day_date + [t_last])\n",
    "    ax.set_ylabel('Node Index')\n",
    "    ax.set_yticks(range(1, N + 1))\n",
    "    fig_title = 'Hub Score Over Time'\n",
    "    if len(label_output) > 0: fig_title = fig_title + ' (' + label_output + ')'\n",
    "    ax.set_title(fig_title)\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.5, pad=0.01)\n",
    "    fig.savefig(file_image[1], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # REPORT\n",
    "    # -------\n",
    "    # Top node of temporal graph\n",
    "\n",
    "    # Check the correctness value of section and top\n",
    "    if top * section > N or top == -1:\n",
    "        # Analyze all nodes\n",
    "        top = N\n",
    "        section = 1\n",
    "\n",
    "    # Selected top nodes\n",
    "    selected_a = []\n",
    "    selected_h = []\n",
    "    # Rank nodes based on average score\n",
    "    idx_a, _ = list(zip(*rank(A_avg_node)))\n",
    "    idx_h, _ = list(zip(*rank(H_avg_node)))\n",
    "\n",
    "    # Select top 'n' nodes from each split\n",
    "    for sp in np.array_split(idx_a, section):\n",
    "        selected_a.extend(sp[:top])\n",
    "    for sp in np.array_split(idx_h, section):\n",
    "        selected_h.extend(sp[:top])\n",
    "\n",
    "    df_columns = []\n",
    "    df_labels = ['_a', '_h', '_id', '_od', '_iw', '_ow']\n",
    "    df_index = []\n",
    "\n",
    "    for node in selected_a:\n",
    "        df_index.extend([str(node) + e for e in df_labels])\n",
    "        col_in_d = []\n",
    "        col_out_d = []\n",
    "        col_in_w = []\n",
    "        col_out_w = []\n",
    "        for i in range(0, N * (T + 1), N):  # T + 1 iteration\n",
    "            n = node + i  # Give node index over different times\n",
    "            if graph.has_node(n):\n",
    "                col_in_d.append(graph.in_degree(n))\n",
    "                col_out_d.append(graph.out_degree(n))\n",
    "                sum_w = 0\n",
    "                for nbr in graph.predecessors(n):\n",
    "                    sum_w += ew.get((nbr, n), 1)\n",
    "                col_in_w.append(sum_w)\n",
    "                sum_w = 0\n",
    "                for nbr in graph.successors(n):\n",
    "                    sum_w += ew.get((n, nbr), 1)\n",
    "                col_out_w.append(sum_w)\n",
    "            else:  # The node has been removed from graph\n",
    "                col_in_d.append(0)\n",
    "                col_out_d.append(0)\n",
    "                col_in_w.append(0)\n",
    "                col_out_w.append(0)\n",
    "        df_columns.append(A[node])\n",
    "        df_columns.append(H[node])\n",
    "        df_columns.append(col_in_d)\n",
    "        df_columns.append(col_out_d)\n",
    "        df_columns.append(col_in_w)\n",
    "        df_columns.append(col_out_w)\n",
    "\n",
    "    for node in selected_h:\n",
    "        df_index.extend([str(node) + e for e in df_labels])\n",
    "        col_in_d = []\n",
    "        col_out_d = []\n",
    "        col_in_w = []\n",
    "        col_out_w = []\n",
    "        for i in range(0, N * (T + 1), N):\n",
    "            n = node + i\n",
    "            if graph.has_node(n):\n",
    "                col_in_d.append(graph.in_degree(n))\n",
    "                col_out_d.append(graph.out_degree(n))\n",
    "                sum_w = 0\n",
    "                for nbr in graph.predecessors(n):\n",
    "                    sum_w += ew.get((nbr, n), 1)\n",
    "                col_in_w.append(sum_w)\n",
    "                sum_w = 0\n",
    "                for nbr in graph.successors(n):\n",
    "                    sum_w += ew.get((n, nbr), 1)\n",
    "                col_out_w.append(sum_w)\n",
    "            else:\n",
    "                col_in_d.append(0)\n",
    "                col_out_d.append(0)\n",
    "                col_in_w.append(0)\n",
    "                col_out_w.append(0)\n",
    "        df_columns.append(A[node])\n",
    "        df_columns.append(H[node])\n",
    "        df_columns.append(col_in_d)\n",
    "        df_columns.append(col_out_d)\n",
    "        df_columns.append(col_in_w)\n",
    "        df_columns.append(col_out_w)\n",
    "\n",
    "    # Create and save dataframe of top nodes from each percentile (or split)\n",
    "    df = pd.DataFrame(df_columns, index=df_index, columns=list(range(T + 1))).T\n",
    "    df.to_csv(file_output[0], index=False)\n",
    "\n",
    "    # Plot the report\n",
    "    # df = df.t\n",
    "    df = pd.read_csv(file_output[0]).T\n",
    "    times = list(df.columns)\n",
    "    idx = df.index.values.tolist()\n",
    "    a_nodes, h_nodes = np.array_split(\n",
    "        [int(idx[i].split('_')[0]) for i in range(0, len(idx), 6)], 2\n",
    "    )\n",
    "\n",
    "    # Authority scores\n",
    "    fig, axs = plt.subplots(\n",
    "        len(a_nodes),\n",
    "        1,\n",
    "        figsize=(16, 12),\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    # cmap = plt.get_cmap('tab10')\n",
    "    for i in range(len(a_nodes)):\n",
    "        row_label = str(a_nodes[i]) + '_a'\n",
    "        row_label2 = str(a_nodes[i]) + '_id'  # id,od,iw,ow\n",
    "        legend_label = str(a_nodes[i])\n",
    "        # axs[i].scatter(\n",
    "        axs[i].plot(\n",
    "            times,\n",
    "            df.loc[row_label],\n",
    "            # (0)\n",
    "            c='black',\n",
    "            alpha=0.5,\n",
    "            # (1)\n",
    "            # c=[np.random.rand(3, )],\n",
    "            # (2)\n",
    "            # c=[cmap(i)],\n",
    "            # (1)\n",
    "            # linewidth=1,\n",
    "            # marker='o',\n",
    "            # markersize=6,\n",
    "            # (2)\n",
    "            # marker='.',\n",
    "            # label=legend_label\n",
    "        )\n",
    "        sc = axs[i].scatter(\n",
    "            times,\n",
    "            [0] * len(times),\n",
    "            # df.loc[row_label2],\n",
    "            s=df.loc[row_label2] * 100,\n",
    "            c=df.loc[row_label2],\n",
    "            marker='|',\n",
    "            alpha=1.0,\n",
    "            vmin=0,\n",
    "            # vmin=min(df.loc[row_label2]),\n",
    "            vmax=28,\n",
    "            # vmax=max(df.loc[row_label2]),\n",
    "            cmap=cm.Wistia,\n",
    "        )\n",
    "        axs[i].set_ylabel('Node ' + str(a_nodes[i]))\n",
    "    # fig.legend(loc='center left', bbox_to_anchor=(1.03, 0.5))\n",
    "    fig.text(0.5, 1.02, 'Temporal HITS', ha='center')\n",
    "    fig.text(0.5, -0.02, 'Time', ha='center')\n",
    "    fig.text(-0.02, 0.5, 'Authority Score', va='center', rotation='vertical')\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.5, pad=0.01)\n",
    "    fig.savefig(file_image[2], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Hub scores\n",
    "    fig, axs = plt.subplots(\n",
    "        len(h_nodes),\n",
    "        1,\n",
    "        figsize=(16, 12),\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    for i in range(len(h_nodes)):\n",
    "        row_label = str(h_nodes[i]) + '_h'\n",
    "        row_label2 = str(h_nodes[i]) + '_od'  # id,od,iw,ow\n",
    "        axs[i].plot(\n",
    "            times,\n",
    "            df.loc[row_label],\n",
    "            c='black',\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sc = axs[i].scatter(\n",
    "            times,\n",
    "            [0] * len(times),\n",
    "            s=df.loc[row_label2] * 100,\n",
    "            c=df.loc[row_label2],\n",
    "            marker='|',\n",
    "            alpha=1.0,\n",
    "            vmin=0,\n",
    "            vmax=28,\n",
    "            cmap=cm.Wistia,\n",
    "        )\n",
    "        axs[i].set_ylabel('Node ' + str(a_nodes[i]))\n",
    "    fig.text(0.5, 1.02, 'Temporal HITS', ha='center')\n",
    "    fig.text(0.5, -0.02, 'Time', ha='center')\n",
    "    fig.text(-0.02, 0.5, 'Hub Score', va='center', rotation='vertical')\n",
    "    fig.colorbar(sc, ax=ax, shrink=0.5, pad=0.01)\n",
    "    fig.savefig(file_image[3], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # TOP\n",
    "    # ---\n",
    "    # Top nodes of TON graph\n",
    "\n",
    "    if report_num > N * (T + 1) or report_num == -1:\n",
    "        # Analyze all nodes\n",
    "        report_num = N * (T + 1)\n",
    "\n",
    "    # Top rank TON graph nodes based on HITS score\n",
    "    A_top = rank(a, return_rank=True)[:report_num]\n",
    "    H_top = rank(h, return_rank=True)[:report_num]\n",
    "\n",
    "    df_index = []\n",
    "    col_r = []  # Rank\n",
    "    col_c = []  # Category i.e. A or H\n",
    "    col_n = []  # Node id\n",
    "    col_a = []\n",
    "    col_h = []\n",
    "    col_p = []  # Parent\n",
    "    col_t = []  # Time\n",
    "    col_in_d = []\n",
    "    col_out_d = []\n",
    "    col_in_w = []\n",
    "    col_out_w = []\n",
    "\n",
    "    for n, r in A_top.items():\n",
    "        df_index.append('a_' + str(n))\n",
    "        col_n.append(n)\n",
    "        col_r.append(r)\n",
    "        col_c.append('a')\n",
    "        col_a.append(a[n])\n",
    "        col_h.append(h[n])\n",
    "        col_p.append(n % N)\n",
    "        col_t.append(n // N)\n",
    "        col_in_d.append(graph.in_degree(n))\n",
    "        col_out_d.append(graph.out_degree(n))\n",
    "        sum_w = 0\n",
    "        for nbr in graph.predecessors(n):\n",
    "            sum_w += ew.get((nbr, n), 1)\n",
    "        col_in_w.append(sum_w)\n",
    "        sum_w = 0\n",
    "        for nbr in graph.successors(n):\n",
    "            sum_w += ew.get((n, nbr), 1)\n",
    "        col_out_w.append(sum_w)\n",
    "\n",
    "    for n, r in H_top.items():\n",
    "        df_index.append('h_' + str(n))\n",
    "        col_n.append(n)\n",
    "        col_r.append(r)\n",
    "        col_c.append('h')\n",
    "        col_a.append(a[n])\n",
    "        col_h.append(h[n])\n",
    "        col_p.append(n % N)\n",
    "        col_t.append(n // N)\n",
    "        col_in_d.append(graph.in_degree(n))\n",
    "        col_out_d.append(graph.out_degree(n))\n",
    "        sum_w = 0\n",
    "        for nbr in graph.predecessors(n):\n",
    "            sum_w += ew.get((nbr, n), 1)\n",
    "        col_in_w.append(sum_w)\n",
    "        sum_w = 0\n",
    "        for nbr in graph.successors(n):\n",
    "            sum_w += ew.get((n, nbr), 1)\n",
    "        col_out_w.append(sum_w)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'r': col_r,\n",
    "            'n': col_n,\n",
    "            'a': col_a,\n",
    "            'h': col_h,\n",
    "            'p': col_p,\n",
    "            't': col_t,\n",
    "            'id': col_in_d,\n",
    "            'od': col_out_d,\n",
    "            'iw': col_in_w,\n",
    "            'ow': col_out_w,\n",
    "            'c': col_c\n",
    "        },\n",
    "        index=df_index\n",
    "    )\n",
    "\n",
    "    # Save top node analysis\n",
    "    df.to_csv(file_output[1], index=False)\n",
    "\n",
    "    # Correlation between in/out-degree and HITS scores\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 16), constrained_layout=True)\n",
    "    axs[0, 0].scatter(df[df.c == 'a'].id, df[df.c == 'a'].a)\n",
    "    axs[0, 0].set_xlabel('In-degree')\n",
    "    axs[0, 0].set_ylabel('Authority Score')\n",
    "    axs[0, 1].scatter(df[df.c == 'a'].od, df[df.c == 'a'].a)\n",
    "    axs[0, 1].set_xlabel('Out-degree')\n",
    "    axs[0, 1].set_ylabel('Authority Score')\n",
    "    axs[1, 0].scatter(df[df.c == 'a'].iw, df[df.c == 'a'].a)\n",
    "    axs[1, 0].set_xlabel('In-weight')\n",
    "    axs[1, 0].set_ylabel('Authority Score')\n",
    "    axs[1, 1].scatter(df[df.c == 'a'].ow, df[df.c == 'a'].a)\n",
    "    axs[1, 1].set_xlabel('Out-weight')\n",
    "    axs[1, 1].set_ylabel('Authority Score')\n",
    "    fig.savefig(file_image[4], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 16), constrained_layout=True)\n",
    "    axs[0, 0].scatter(df[df.c == 'h'].id, df[df.c == 'h'].h)\n",
    "    axs[0, 0].set_xlabel('In-degree')\n",
    "    axs[0, 0].set_ylabel('Hub Score')\n",
    "    axs[0, 1].scatter(df[df.c == 'h'].od, df[df.c == 'h'].h)\n",
    "    axs[0, 1].set_xlabel('Out-degree')\n",
    "    axs[0, 1].set_ylabel('Hub Score')\n",
    "    axs[1, 0].scatter(df[df.c == 'h'].iw, df[df.c == 'h'].h)\n",
    "    axs[1, 0].set_xlabel('In-weight')\n",
    "    axs[1, 0].set_ylabel('Hub Score')\n",
    "    axs[1, 1].scatter(df[df.c == 'h'].ow, df[df.c == 'h'].h)\n",
    "    axs[1, 1].set_xlabel('Out-weight')\n",
    "    axs[1, 1].set_ylabel('Hub Score')\n",
    "    fig.savefig(file_image[5], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Correlation matrix\n",
    "    corr = df.corr()  # method='pearson',\n",
    "    # corr = df.corr(method ='kendall')\n",
    "    # corr = df.corr(method ='spearman')\n",
    "    fig, ax = plt.subplots(figsize=(9, 8))\n",
    "    sns.heatmap(corr, ax=ax, cmap='YlGnBu', linewidths=0.1)\n",
    "    fig.savefig(file_image[6], dpi=300, bbox_inches='tight', transparent=True)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T18:49:16.364869Z",
     "start_time": "2021-02-23T18:49:16.360132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a, h = hits(\n",
    "#     file_input=[\n",
    "#         'network/bt_tn_full_network.gpickle',\n",
    "#         'network/bt_tn_light_network.gpickle',\n",
    "#         'network/bt_temporal_times.csv',\n",
    "#         'network/bt_temporal_nodes.csv',\n",
    "#         'network/bt_tn_weights.csv',\n",
    "#     ],\n",
    "#     file_output=[\n",
    "#         'hits/a.csv',\n",
    "#         'hits/h.csv',\n",
    "#     ],\n",
    "#     label_input='',\n",
    "#     label_output='',\n",
    "#     graph=None,\n",
    "#     times=None,\n",
    "#     nodes=None,\n",
    "#     ew=None,\n",
    "#     nstart=None,\n",
    "#     full=False,\n",
    "#     version=3,  # 0\n",
    "#     sigma=0.85,\n",
    "#     max_iter=100,\n",
    "#     tol=1.0e-8,\n",
    "#     norm_max=True,\n",
    "#     norm_final_l1=True,\n",
    "#     norm_final_l2=False,\n",
    "#     norm_iter=False,\n",
    "#     norm_degree=False,\n",
    "#     norm_damping=False,\n",
    "#     output=True,\n",
    "#     plot=True,\n",
    "#     save=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a, h = hits_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-23T21:34:58.470291Z",
     "start_time": "2021-02-23T21:34:58.466160Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hits_conditional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T00:39:22.601036Z",
     "start_time": "2021-02-24T00:39:13.630232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hits_analyze(\n",
    "#     file_input=[\n",
    "#         'network/bt_temporal_nodes.csv',\n",
    "#         'network/bt_temporal_times.csv',\n",
    "#     ],\n",
    "#     file_net=['network/bt_tn_light_network.gpickle'],\n",
    "#     file_ew=['network/bt_tn_weights.csv'],\n",
    "#     file_hits=[\n",
    "#         'hits/a.csv',\n",
    "#         'hits/h.csv',\n",
    "#         'hits/a_array.csv',\n",
    "#         'hits/h_array.csv',\n",
    "#         'hits/a_avg_node.csv',\n",
    "#         'hits/a_avg_time.csv',\n",
    "#         'hits/h_avg_node.csv',\n",
    "#         'hits/h_avg_time.csv',\n",
    "#         'hits/a_norm_node.csv',\n",
    "#         'hits/a_norm_time.csv',\n",
    "#         'hits/h_norm_node.csv',\n",
    "#         'hits/h_norm_time.csv',\n",
    "#     ],\n",
    "#     file_output=['hits/report.csv', 'hits/top.csv'],\n",
    "#     file_image=[\n",
    "#         'hits/fig_a.pdf',\n",
    "#         'hits/fig_h.pdf',\n",
    "#         'hits/fig_a_report.pdf',\n",
    "#         'hits/fig_h_report.pdf',\n",
    "#         'hits/fig_a_corr.pdf',\n",
    "#         'hits/fig_h_corr.pdf',\n",
    "#         'hits/fig_mat.pdf',\n",
    "#     ],\n",
    "#     label_input='',\n",
    "#     label_hits='',\n",
    "#     label_output='',\n",
    "#     label_image='',\n",
    "#     graph=None,\n",
    "#     times=None,\n",
    "#     nodes=None,\n",
    "#     ew=None,\n",
    "#     a=None,\n",
    "#     h=None,\n",
    "#     top=2,\n",
    "#     section=4,\n",
    "#     report_num=100\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Group Test\n",
    "Calculate a series of different HITS versions (1 ... 9), then compute conditional scores and analyze the top rank nodes and finally plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "l = 3\n",
    "ll = 'v' + str(l)\n",
    "# A, H = hits(graph=None,times=None,nodes=None,edge_weight=None,nstart=None,label_input='',label_output=ll,_full=False,_max_iter=300,_tol=1.0e-8,_version=l,_sigma=0.85,_norm_max=True,_norm_final_l1=True,_norm_final_l2=False,_norm_iter=False,_norm_degree=False,_norm_damping=False,_norm_round=False,_round_num=5,save=True)\n",
    "hits_top(label_input='',label_output=ll,label_hits=ll,label_net='',label_ew='',top=10,section=1,report_num=100,round_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for l in range(1,10):\n",
    "#     ll = 'v' + str(l)\n",
    "#     A, H = hits(graph=None,times=None,nodes=None,edge_weight=None,nstart=None,label_input='',label_output=ll,_full=False,_max_iter=300,_tol=1.0e-8,_version=l,_sigma=0.85,_norm_max=True,_norm_final_l1=True,_norm_final_l2=False,_norm_iter=False,_norm_degree=False,_norm_damping=False,_norm_round=False,_round_num=5,save=True)\n",
    "#     hits_top(label_input='',label_output=ll,label_hits=ll,label_net='',label_ew='',top=10,section=1,report_num=100,round_num=5)\n",
    "#     print('\\n===\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T00:58:50.013350Z",
     "start_time": "2021-02-24T00:58:49.994074Z"
    }
   },
   "outputs": [],
   "source": [
    "def tn_bt_to_temporal(\n",
    "    file_input=[\n",
    "        'remove/bt_tn_network.gpickle', 'network/bt_temporal_nodes.csv',\n",
    "        'network/bt_temporal_times.csv'\n",
    "    ],\n",
    "    file_output=[\n",
    "        'remove/bt_temporal_network.gpickle', 'remove/bt_temporal_times.csv',\n",
    "        'remove/bt_temporal_nodes.csv'\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_output='',\n",
    "    label_graph_name='',\n",
    "    temporal=None,\n",
    "    trans_remove=True,\n",
    "    output_network=True,\n",
    "    save_times=True,\n",
    "    save_nodes=True,\n",
    "    save_network_file=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a (directed) time-ordered network (TON) to a (multi-edge) temporal (T) network\n",
    "    \"\"\"\n",
    "    # Amend filenames\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Create empty TEMPORAL network\n",
    "    graph = nx.MultiDiGraph()\n",
    "    graph.name = 'Temporal Network (' + label_graph_name + ')'\n",
    "\n",
    "    # Read temporal networks from file\n",
    "    if temporal is None:\n",
    "        temporal = nx.read_gpickle(file_input[0])\n",
    "\n",
    "    # Nodes\n",
    "    N = line_count(file_input[1])\n",
    "\n",
    "    # Timestamp\n",
    "    times = temporal_bt_times_read()  # TODO: we may not need it\n",
    "    T = line_count(file_input[2])\n",
    "\n",
    "    times_set = set()\n",
    "    for u, v, data in temporal.edges(data=True):\n",
    "        parent_u = u % N\n",
    "        parent_v = v % N\n",
    "        time_uv = u // N  # OR v // N - 1\n",
    "        time_delta = abs(v - u) // N\n",
    "        # Crossed edge\n",
    "        if parent_u != parent_v:  # and time_delta == 1:\n",
    "            # print(f'{parent_u} -> {parent_v} with {data} at time {times.loc[time_uv]}')\n",
    "            if trans_remove and data.get('trans', False):\n",
    "                # If the the edge is transitive and we want to ignore trans -> skip\n",
    "                continue\n",
    "            graph.add_edge(\n",
    "                parent_u, parent_v, t=times.loc[time_uv], w=data['w']\n",
    "            )\n",
    "            # Save timestamp to the new time set\n",
    "            times_set.add(times.loc[time_uv])\n",
    "\n",
    "    # Convert times set to series and save\n",
    "    times_new = pd.Series(sorted(list(times_set)))\n",
    "    nodes_new = pd.Series(sorted(list(graph.nodes)))\n",
    "\n",
    "    # Save graph\n",
    "    if save_network_file: nx.write_gpickle(graph, file_output[0])\n",
    "\n",
    "    # Save times\n",
    "    if save_times:\n",
    "        np.savetxt(file_output[1], times_new, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Save nodes\n",
    "    if save_nodes:\n",
    "        np.savetxt(file_output[2], nodes_new, delimiter=',', fmt='%s')\n",
    "\n",
    "    # Print network statistics\n",
    "    if output_network:\n",
    "        print(nx.info(graph))\n",
    "        print(f'Number of times: {len(times_new)}')\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T01:00:26.001829Z",
     "start_time": "2021-02-24T01:00:25.945894Z"
    }
   },
   "outputs": [],
   "source": [
    "def hits_remove(\n",
    "    a=None,\n",
    "    h=None,\n",
    "    graph=None,\n",
    "    times=None,\n",
    "    nodes=None,\n",
    "    ew=None,\n",
    "    file_input=[\n",
    "        'network/bt_temporal_times.csv',\n",
    "        'network/bt_temporal_nodes.csv',\n",
    "    ],\n",
    "    file_hits=[\n",
    "        'hits/a.csv',\n",
    "        'hits/h.csv',\n",
    "    ],\n",
    "    file_output=[\n",
    "        'remove/bt_tn_network.gpickle',\n",
    "        'remove/bt_temporal_network.gpickle',\n",
    "        'remove/bt_temporal_times.csv',\n",
    "        'remove/bt_temporal_nodes.csv',\n",
    "        'remove/bt_tn_weights.csv',\n",
    "        'remove/bt_tn_probs.csv',\n",
    "        'remove/a.csv',\n",
    "        'remove/h.csv',\n",
    "        'remove/a_array.csv',\n",
    "        'remove/h_array.csv',\n",
    "        'remove/a_avg_node.csv',\n",
    "        'remove/a_avg_time.csv',\n",
    "        'remove/h_avg_node.csv',\n",
    "        'remove/h_avg_time.csv',\n",
    "        'remove/a_norm_node.csv',\n",
    "        'remove/a_norm_time.csv',\n",
    "        'remove/h_norm_node.csv',\n",
    "        'remove/h_norm_time.csv',\n",
    "        'remove/report.csv',\n",
    "        'remove/top.csv',\n",
    "        'remove/fig_a.pdf',\n",
    "        'remove/fig_h.pdf',\n",
    "        'remove/fig_a_report.pdf',\n",
    "        'remove/fig_h_report.pdf',\n",
    "        'remove/fig_a_corr.pdf',\n",
    "        'remove/fig_h_corr.pdf',\n",
    "        'remove/fig_mat.pdf',\n",
    "        'remove/remove.csv',\n",
    "    ],\n",
    "    label_input='',\n",
    "    label_hits='',\n",
    "    label_net='',\n",
    "    label_ew='',\n",
    "    label_output='',\n",
    "    label_output_folder='',\n",
    "    epoch=10,  # How many times repead the node removal action\n",
    "    remove=0.5,  # Stop if X ratio of nodes were removed (even if not reached X epoch)\n",
    "    step=10,  # E.g. Remove 10 % or 10 nodes at each epoch    \n",
    "    strategy_a='a',  # 'a' or 'h' = authority or hub    \n",
    "    strategy_b='t',  # 't' or 's' = temporal or static\n",
    "    strategy_c='r',  # 'r' or 'n' = ratio or number\n",
    "    strategy_d=1,  # Score-based or random approach\n",
    "    time_window=(5, 12),  # Remove high rank nodes between 5 pm - 12 am\n",
    "    rd=8,  # Decimal point\n",
    "    actions=[0, 2],\n",
    "    output=True,\n",
    "    plot_times=False,\n",
    "    save_networks=True,\n",
    "    return_graphs=True,\n",
    "    return_scores=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Strategy\n",
    "        A: Score\n",
    "            a = authority\n",
    "            h = hub\n",
    "        B: Network\n",
    "            s = static\n",
    "            t = temporal\n",
    "        C: Size\n",
    "            n = number number of nodes to be removed (default = 1)\n",
    "            r = ratio of nodes to be removed (default = 1 %)\n",
    "        D: method\n",
    "            0) randomly\n",
    "            1) centrality-based (i.e. temporal HITS)\n",
    "            2) high rank nodes at specific time window\n",
    "            3) degree-based (TODO in future)\n",
    "    Actions\n",
    "        0) Convert TON to TEMPORAL model and save it\n",
    "        1) Convert TEMPORAL model to STATIC model\n",
    "        2) Calculate temporal HITS on modified network\n",
    "            A) Calculate new Edge-Weghts and save\n",
    "            B) Calculate HITS\n",
    "            C) Analyze HITS\n",
    "        ---\n",
    "        TODO\n",
    "        2) intersection similarity\n",
    "        3) centrality robustness\n",
    "        4) influence maximization\n",
    "        5) network diameter (90 threshhold)\n",
    "        6) Number and size of gient connected components (CC)\n",
    "            - Is it still one big CC (time = 1 -> T) or it is broken into pices\n",
    "            - How many CC's or time windows (e.g. start-t2, t2-t10, t10-end)\n",
    "        7) Average pair-wise (tempora and topological=hop) distance\n",
    "        8) Network reachability\n",
    "        9) Epidemic treshhold (spread of information or disease)\n",
    "        10) Shanon diversity\n",
    "    \"\"\"\n",
    "\n",
    "    # Edit files\n",
    "    file_input = label_amend(file_input, label_input)\n",
    "    file_hits = label_amend(file_hits, label_hits)\n",
    "    file_output = label_amend(file_output, label_output)\n",
    "\n",
    "    # Read times\n",
    "    if times is None:\n",
    "        times = list(temporal_bt_times_read([file_input[0]]))\n",
    "        T = line_count(file_input[0])\n",
    "    else:\n",
    "        T = len(times)\n",
    "\n",
    "    # Read nodes\n",
    "    if nodes is None:\n",
    "        nodes = list(temporal_bt_nodes_read([file_input[1]]))\n",
    "        N = line_count(file_input[1])\n",
    "    else:\n",
    "        N = len(nodes)\n",
    "        # N = graph.number_of_nodes() // T\n",
    "\n",
    "    # Read network\n",
    "    if graph == None:\n",
    "        graph = tn_bt_light_read(label_input=label_net)\n",
    "\n",
    "    # Number of nodes and edges of TON graph\n",
    "    N_new = graph.number_of_nodes()\n",
    "    M_new = graph.number_of_edges()\n",
    "\n",
    "    # Size of nodes and timestamp of TON after node removal\n",
    "    # Will be updated later (better to have it as global variable)\n",
    "    M = 0\n",
    "    temporal_N = 0\n",
    "    temporal_T = 0\n",
    "\n",
    "    # Read edge weights\n",
    "    if ew is None: ew = ew_read(label_input=label_ew)  # TODO: can be removed\n",
    "\n",
    "    # Print network info before any node removal\n",
    "    if output: print(nx.info(graph), '\\n')\n",
    "\n",
    "    # Read HITS scores\n",
    "    if a is None or h is None:\n",
    "        a, h = hits_read(\n",
    "            file_input=file_hits,\n",
    "            label_input='',\n",
    "            output=False,\n",
    "            plot=False,\n",
    "        )\n",
    "\n",
    "\n",
    "#     # Set rounding number (decimal point) for HITS scores\n",
    "#     fmt_str = '%1.' + str(rd) + 'f'\n",
    "\n",
    "# Create list of scores by sorting the key of dictionary\n",
    "    a_values, h_values = [], []\n",
    "    for k in sorted(a.keys(), reverse=False):  # Ascending\n",
    "        # Score of node 1 to the last node id number\n",
    "        a_values.append(a[k])\n",
    "        h_values.append(h[k])\n",
    "\n",
    "    # Create A and H matrices by iterating through sorted list of node\n",
    "    A = np.reshape(a_values, (T + 1, N)).T\n",
    "    H = np.reshape(h_values, (T + 1, N)).T\n",
    "\n",
    "    # Create average score of nodes and timestamps\n",
    "    A_avg_node = A.sum(axis=1) / (T + 1)\n",
    "    A_avg_time = A.sum(axis=0) / N\n",
    "    H_avg_node = H.sum(axis=1) / (T + 1)\n",
    "    H_avg_time = H.sum(axis=0) / N\n",
    "\n",
    "    # Time Analysis\n",
    "    t_first = times[0].strftime('%d %B\\n%I %p')\n",
    "    t_last = times[-1].strftime('%d %B\\n%I %p')\n",
    "    t_0 = times[0] - pd.Timedelta(1, unit='h')\n",
    "\n",
    "    # Distribution of timestamps over days and weeks of the month\n",
    "    # Also finding time index of timestamp that hour change to 7am, 12p, 5pm, and 12am\n",
    "    t_day_idx = []\n",
    "    t_day_week = []\n",
    "    t_day_date = []  # 7 am (or earliest time of the day)\n",
    "    t_hour_12 = []\n",
    "    t_hour_17 = []\n",
    "    # Save index of each timestamp belong to what time windows of the day\n",
    "    # [ [7 am - noon], [12 pm - 5 pm], [5 pm - 12 am] ]\n",
    "    for i, time in enumerate(times):\n",
    "        if t_0.weekday() != time.weekday():  # When day of the week changes\n",
    "            t_day_idx.append(\n",
    "                i\n",
    "            )  # Index of the first hour of each day (expected to be 7 am)\n",
    "            t_day_week.append(time.strftime('%a'))  # Monday ...\n",
    "            t_day_date.append(time.strftime('%d'))  # 1st, 2nd, ..., 31\n",
    "            t_0 = time\n",
    "        if time.strftime('%H') == '12':\n",
    "            t_hour_12.append(i)\n",
    "        if time.strftime('%H') == '17':\n",
    "            t_hour_17.append(i)\n",
    "\n",
    "    # Adding first timestamp same as timestamp 1\n",
    "    # Just because of temporal network model requires this method\n",
    "    times.insert(0, times[0])  # Make times list to size T + 1\n",
    "\n",
    "    # Best hour and day in terms of highest authority average score\n",
    "    days_a = defaultdict(list)\n",
    "    hours_a = defaultdict(list)\n",
    "    for i, score in enumerate(A_avg_time):\n",
    "        days_a[times[i].weekday()].append(score)\n",
    "        hours_a[times[i].hour].append(score)\n",
    "    days_a_sum = {k: sum(v) / len(v) for k, v in days_a.items()}\n",
    "    hours_a_sum = {k: sum(v) / len(v) for k, v in hours_a.items()}\n",
    "\n",
    "    # Day-Hour matrix score\n",
    "    # 7 days and 18 hours [7 am ... 11 pm] , missing 6 hours of [12 am ... 6 am]\n",
    "    day_hour_a = np.zeros((7, 18))\n",
    "    day_hour_a_count = np.zeros((7, 18))\n",
    "    for i, t in enumerate(times):\n",
    "        day_hour_a[t.dayofweek][t.hour - 6] += A_avg_time[i]\n",
    "        day_hour_a_count[t.dayofweek][t.hour - 6] += 1\n",
    "    # To avoid ZeroDivisionError, replace all 0 with 1\n",
    "    # Still average produce 0 because observe/count (0/1=0)\n",
    "    day_hour_a_count_new = day_hour_a_count.copy()\n",
    "    day_hour_a_count_new[day_hour_a_count == 0] = 1\n",
    "    day_hour_a_avg = np.divide(day_hour_a, day_hour_a_count_new)[:, 1:]\n",
    "\n",
    "    # Best hour and day in terms of highest hub average score\n",
    "    days_h = defaultdict(list)\n",
    "    hours_h = defaultdict(list)\n",
    "    for i, score in enumerate(H_avg_time):\n",
    "        days_h[times[i].weekday()].append(score)\n",
    "        hours_h[times[i].hour].append(score)\n",
    "    days_h_sum = {k: sum(v) / len(v) for k, v in days_h.items()}\n",
    "    hours_h_sum = {k: sum(v) / len(v) for k, v in hours_h.items()}\n",
    "\n",
    "    # Day-Hour matrix score\n",
    "    day_hour_h = np.zeros((7, 18))\n",
    "    day_hour_h_count = np.zeros((7, 18))\n",
    "    for i, t in enumerate(times):\n",
    "        day_hour_h[t.dayofweek][t.hour - 6] += H_avg_time[i]\n",
    "        day_hour_h_count[t.dayofweek][t.hour - 6] += 1\n",
    "\n",
    "    day_hour_h_count_new = day_hour_h_count.copy()\n",
    "    day_hour_h_count_new[day_hour_h_count == 0] = 1\n",
    "    day_hour_h_avg = np.divide(day_hour_h, day_hour_h_count_new)[:, 1:]\n",
    "\n",
    "    if plot_times:\n",
    "        ax = plt.axes()\n",
    "        sns.heatmap(\n",
    "            day_hour_a_count[:, 1:],\n",
    "            linewidth=0.5,\n",
    "            cmap='YlGnBu',\n",
    "            xticklabels=list(range(7, 24)),\n",
    "            yticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'],\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title('Day and hour frequency of timestamps')\n",
    "        plt.show()\n",
    "        # ---\n",
    "        ax = plt.axes()\n",
    "        sns.heatmap(\n",
    "            day_hour_a_avg,\n",
    "            linewidth=0.5,\n",
    "            cmap='YlGnBu',\n",
    "            xticklabels=list(range(7, 24)),\n",
    "            yticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        )\n",
    "        ax.set_title('Average authority score over days and hours')\n",
    "        plt.show()\n",
    "        # ---\n",
    "        ax = plt.axes()\n",
    "        sns.heatmap(\n",
    "            day_hour_h_avg,\n",
    "            linewidth=0.5,\n",
    "            cmap='YlGnBu',\n",
    "            xticklabels=list(range(7, 24)),\n",
    "            yticklabels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        )\n",
    "        ax.set_title('Average hub score over days and hours')\n",
    "        plt.show()\n",
    "\n",
    "    epoch_0 = 0  # How many times the algorimth is looped\n",
    "    remove_0 = 0  # ratio of removed nodes\n",
    "\n",
    "    # Number of nodes or ratio\n",
    "    if strategy_c == 'n':\n",
    "        # Step should be an integer number\n",
    "        # print(f'Removing {step} nodes or equevelant of {step/N_new*100:.2f} % of all the nodes at every epoch\\n')\n",
    "        pass  # Do nothing, if not printing\n",
    "    elif strategy_c == 'r':\n",
    "        # Step should be a float ratio [0.0  ... 1.0]\n",
    "        # print(f'Removing of {step*100:.2f} % of all the nodes or equevelant of {int(N_new*step)} nodes at every epoch\\n')\n",
    "        # Edit number of removing nodes based on input ratio\n",
    "        step = int(N_new * step)\n",
    "\n",
    "    # Iterative over chunk of X nodes with high HITS scores\n",
    "    bd = []\n",
    "    tops = []\n",
    "    if strategy_a == 'a':\n",
    "        bd = breakdown(a_values, step)\n",
    "        tops = iter(bd)\n",
    "    elif strategy_a == 'h':\n",
    "        bd = breakdown(h_values, step)\n",
    "        tops = iter(bd)\n",
    "\n",
    "    # If wants to return the calculated graphs at the end\n",
    "    graphs = {}  # Dict of temporal graphs\n",
    "    tons = {}  # Dict of TON model graphs\n",
    "    auts = {}  # Dict of authority scores\n",
    "    hubs = {}  # Dict of hub scores\n",
    "\n",
    "    # Add initial graphs and scores here\n",
    "    temporal = temporal_bt_read()\n",
    "    M = temporal.number_of_edges()\n",
    "\n",
    "    if return_graphs:\n",
    "        tons[epoch_0] = graph\n",
    "        graphs[epoch_0] = temporal\n",
    "    if return_scores:\n",
    "        auts[epoch_0] = a\n",
    "        hubs[epoch_0] = h\n",
    "\n",
    "    epoch_0 += 1  # 1\n",
    "    selected_nodes = []\n",
    "    removed_total = []\n",
    "    # Keep removing nodes until one of the conditions happen\n",
    "    # while epoch_0 <= epoch and remove_0 <= remove:\n",
    "    for _ in range(1, len(bd)):\n",
    "        if epoch_0 > epoch or remove_0 >= remove:\n",
    "            break\n",
    "        else:\n",
    "            if output: print(f'EPOCH {epoch_0}:\\n---------')\n",
    "\n",
    "        # Create saving folders and file names\n",
    "        file_output_new = []\n",
    "        if not len(label_output_folder) > 0:  # Empty\n",
    "            label_output_folder_temp = strategy_a + '-' + strategy_c + '-' + str(\n",
    "                strategy_d\n",
    "            ) + '-' + str(epoch_0)\n",
    "            file_output_new = label_amend(\n",
    "                file_output, label_output_folder_temp, end=False\n",
    "            )\n",
    "        else:\n",
    "            file_output_new = file_output\n",
    "\n",
    "        # Empty list of node to be removed ...\n",
    "        selected_nodes.clear()\n",
    "\n",
    "        # Method 0\n",
    "        if strategy_d == 0:\n",
    "            if output:\n",
    "                print(\n",
    "                    'Removing', step, 'nodes out of', graph.number_of_nodes(),\n",
    "                    'selected randomly ...\\n'\n",
    "                )\n",
    "\n",
    "            # Pick a set of nodes uniformy random\n",
    "            # np.random.seed(0)\n",
    "            selected_nodes = list(\n",
    "                np.random.choice(graph.nodes, size=step, replace=False)\n",
    "            )\n",
    "            removed_total.extend(selected_nodes)\n",
    "            np.savetxt(\n",
    "                file_output_new[27],\n",
    "                selected_nodes,\n",
    "                delimiter=',',\n",
    "                fmt='%s',\n",
    "            )\n",
    "            # if output: print(selected_nodes)\n",
    "\n",
    "        # Method 1\n",
    "        elif strategy_d == 1:\n",
    "            if output:\n",
    "                print(\n",
    "                    'Removing', step, 'nodes out of', graph.number_of_nodes(),\n",
    "                    'selected based on temporal HITS scores ...\\n'\n",
    "                )\n",
    "\n",
    "            # Select top ranked HITS score nodes\n",
    "            selected_nodes = next(tops)\n",
    "            removed_total.extend(selected_nodes)\n",
    "            np.savetxt(\n",
    "                file_output_new[27],\n",
    "                selected_nodes,\n",
    "                delimiter=',',\n",
    "                fmt='%s',\n",
    "            )\n",
    "            # if output: print(selected_nodes)\n",
    "\n",
    "        # Method 3 TODO\n",
    "        elif strategy_d == 2:\n",
    "            if output:\n",
    "                print(\n",
    "                    'Removing', step, 'nodes out of', graph.number_of_nodes(),\n",
    "                    'selected based on average HITS scores and in time window of ',\n",
    "                    time_window, ' ...\\n'\n",
    "                )\n",
    "\n",
    "            # Sort best node with highest average score\n",
    "            ids_a = A_avg_node.argsort()[::-1]\n",
    "            ids_h = H_avg_node.argsort()[::-1]\n",
    "            # Select based on desired ratio\n",
    "            selected_a = ids_a[epoch * step:(epoch + 1) * step]\n",
    "            selected_h = ids_h[epoch * step:(epoch + 1) * step]\n",
    "            # Filter for desired time window\n",
    "            selected_noodes = []\n",
    "            selected_times = list(range(T + 1))\n",
    "            for n in selected_a:\n",
    "                selected_noodes.extend([N * t + n for t in selected_times])\n",
    "            removed_total.extend(selected_nodes)\n",
    "\n",
    "        # Remove selected nodes\n",
    "        graph.remove_nodes_from(selected_nodes)\n",
    "        graph.name = 'Time-ordered Network (' + str(epoch_0) + ')'\n",
    "\n",
    "        # If returning\n",
    "        if return_graphs: tons[epoch_0] = graph\n",
    "\n",
    "        # Save\n",
    "        if save_networks: nx.write_gpickle(graph, file_output_new[0])\n",
    "\n",
    "        # Print network statistics after node removal\n",
    "        if output:\n",
    "            print(nx.info(graph))\n",
    "            print(\n",
    "                f'Removed {N_new - graph.number_of_nodes()} nodes out of {N_new}'\n",
    "                f' or {(N_new - graph.number_of_nodes()) / N_new * 100:.2f} % and {M_new - graph.number_of_edges()}'\n",
    "                f' edges out of {M_new} or {(M_new - graph.number_of_edges()) / M_new * 100:.2f} %'\n",
    "            )\n",
    "            print()\n",
    "\n",
    "        # ACTIONS\n",
    "        # -------\n",
    "\n",
    "        # Convert TON to TEMPORAL then save\n",
    "        if 0 in actions:\n",
    "            temporal = tn_bt_to_temporal(\n",
    "                temporal=graph,\n",
    "                file_output=file_output_new[1:4],  # 1...3\n",
    "                label_graph_name=str(epoch_0)\n",
    "            )\n",
    "            # update number of nodes and timestamp in temporal graph (not TON)\n",
    "            temporal_T = line_count(file_output_new[2])\n",
    "            temporal_N = line_count(file_output_new[3])\n",
    "            if output:\n",
    "                # print(nx.info(temporal))\n",
    "                print(\n",
    "                    f'Removed {N - temporal_N} nodes out of {N}'\n",
    "                    f' or {(N - temporal_N) / N*100:.2f} % and {M - temporal.number_of_edges()}'\n",
    "                    f' edges out of {M} or {(M - temporal.number_of_edges()) / M * 100:.2f} %'\n",
    "                )\n",
    "                print()\n",
    "            # If returning\n",
    "            if return_graphs: graphs[epoch_0] = temporal\n",
    "\n",
    "        # Convert TEMPORAL to STATIC then save\n",
    "        if 1 in actions:\n",
    "            pass\n",
    "\n",
    "        # Calculate HITS on new TON graph\n",
    "        if 2 in actions:\n",
    "            # Edge-Weight\n",
    "            ew = ew_create(\n",
    "                file_input=[\n",
    "                    file_output_new[0],  # graph\n",
    "                    'network/bt_temporal_nodes.csv',  # original nodes\n",
    "                    'network/bt_temporal_times.csv',  # original times\n",
    "                ],\n",
    "                file_output=[file_output_new[4]],  # edge-weights\n",
    "                # graph=graph, # after removal\n",
    "                # number_of_nodes=N,  # original\n",
    "                # number_of_times=T,  # original\n",
    "                version=3,  # 0\n",
    "                omega=1,\n",
    "                gamma=0.0001,\n",
    "                epsilon=1,\n",
    "                distance=0.1,  # 1\n",
    "                alpha=0.5,\n",
    "                save_weights=True,\n",
    "                output_weights=False,\n",
    "                plot_weights=False\n",
    "            )\n",
    "            # HITS\n",
    "            a_n, h_n = hits(\n",
    "                file_input=[\n",
    "                    'network/bt_tn_full_network.gpickle',  # not needed\n",
    "                    file_output_new[0],  # graph after removal\n",
    "                    'network/bt_temporal_times.csv',  # original nodes\n",
    "                    'network/bt_temporal_nodes.csv',  # original times\n",
    "                    file_output_new[4],  # edge weights after removal\n",
    "                ],\n",
    "                file_output=file_output_new[6:8],  # 6...7\n",
    "                label_output='',\n",
    "                # graph=graph,  # after removal\n",
    "                # times=times,  # original times list\n",
    "                # nodes=nodes,  # original nodes list\n",
    "                # ew=ew,  # after removal\n",
    "                version=3,  # 0\n",
    "                sigma=0.85,\n",
    "                max_iter=100,\n",
    "                tol=1.0e-8,\n",
    "                norm_max=True,\n",
    "                norm_final_l1=True,\n",
    "                norm_final_l2=False,\n",
    "                norm_iter=False,\n",
    "                norm_degree=False,\n",
    "                norm_damping=False,\n",
    "                output=False,\n",
    "                plot=False,\n",
    "                save=True\n",
    "            )\n",
    "            auts[epoch_0] = a_n\n",
    "            hubs[epoch_0] = h_n\n",
    "            # Conditional HITS scores\n",
    "            hits_conditional(\n",
    "                file_input=file_output_new[6:8],\n",
    "                file_output=file_output_new[8:18],  # 8..17\n",
    "                label_input='',\n",
    "                label_output='',\n",
    "                # a=a_n,\n",
    "                # h=h_n,\n",
    "                # N=N,\n",
    "                # T=T,\n",
    "                removed=True,\n",
    "                save=True\n",
    "            )\n",
    "            # Analyze HITS\n",
    "            hits_analyze(\n",
    "                file_input=[\n",
    "                    'network/bt_temporal_nodes.csv',\n",
    "                    'network/bt_temporal_times.csv',\n",
    "                ],\n",
    "                file_net=[file_output_new[0]],  # graph after removal\n",
    "                file_ew=[file_output_new[4]],  # edge weigths after removal\n",
    "                file_hits=file_output_new[6:18],  # 6...17\n",
    "                file_output=file_output_new[18:20],  # 18...19\n",
    "                file_image=file_output_new[20:27],  # 20...26\n",
    "                label_input='',\n",
    "                label_hits='',\n",
    "                label_output='',\n",
    "                label_image='',\n",
    "                # graph=graph,  # graph updated\n",
    "                # times=times,  # original times\n",
    "                # nodes=nodes,  # original nodes\n",
    "                # ew=ew,  # edge weights updated\n",
    "                # a=a_n,  # new authority scores\n",
    "                # h=h_n,  # new hub scores\n",
    "                top=2,\n",
    "                section=4,\n",
    "                report_num=100\n",
    "            )\n",
    "\n",
    "        # Update epoch and ...\n",
    "        epoch_0 += 1\n",
    "        remove_0 = (N_new - graph.number_of_nodes()) / N_new\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T01:14:09.816178Z",
     "start_time": "2021-02-24T01:07:48.071947Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Time-ordered Network\n",
      "Type: DiGraph\n",
      "Number of nodes: 8428\n",
      "Number of edges: 21384\n",
      "Average in degree:   2.5373\n",
      "Average out degree:   2.5373 \n",
      "\n",
      "EPOCH 1:\n",
      "---------\n",
      "Removing 100 nodes out of 8428 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (1)\n",
      "Type: DiGraph\n",
      "Number of nodes: 8328\n",
      "Number of edges: 19827\n",
      "Average in degree:   2.3808\n",
      "Average out degree:   2.3808\n",
      "Removed 100 nodes out of 8428 or 1.19 % and 1557 edges out of 21384 or 7.28 %\n",
      "\n",
      "Name: Temporal Network (1)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 9250\n",
      "Average in degree: 330.3571\n",
      "Average out degree: 330.3571\n",
      "Number of times: 297\n",
      "Removed 0 nodes out of 28 or 0.00 % and 1144 edges out of 10394 or 11.01 %\n",
      "\n",
      "EPOCH 2:\n",
      "---------\n",
      "Removing 100 nodes out of 8328 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (2)\n",
      "Type: DiGraph\n",
      "Number of nodes: 8228\n",
      "Number of edges: 19017\n",
      "Average in degree:   2.3113\n",
      "Average out degree:   2.3113\n",
      "Removed 200 nodes out of 8428 or 2.37 % and 2367 edges out of 21384 or 11.07 %\n",
      "\n",
      "Name: Temporal Network (2)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 8758\n",
      "Average in degree: 312.7857\n",
      "Average out degree: 312.7857\n",
      "Number of times: 296\n",
      "Removed 0 nodes out of 28 or 0.00 % and 1636 edges out of 10394 or 15.74 %\n",
      "\n",
      "EPOCH 3:\n",
      "---------\n",
      "Removing 100 nodes out of 8228 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (3)\n",
      "Type: DiGraph\n",
      "Number of nodes: 8128\n",
      "Number of edges: 18084\n",
      "Average in degree:   2.2249\n",
      "Average out degree:   2.2249\n",
      "Removed 300 nodes out of 8428 or 3.56 % and 3300 edges out of 21384 or 15.43 %\n",
      "\n",
      "Name: Temporal Network (3)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 8113\n",
      "Average in degree: 289.7500\n",
      "Average out degree: 289.7500\n",
      "Number of times: 296\n",
      "Removed 0 nodes out of 28 or 0.00 % and 2281 edges out of 10394 or 21.95 %\n",
      "\n",
      "EPOCH 4:\n",
      "---------\n",
      "Removing 100 nodes out of 8128 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (4)\n",
      "Type: DiGraph\n",
      "Number of nodes: 8028\n",
      "Number of edges: 17139\n",
      "Average in degree:   2.1349\n",
      "Average out degree:   2.1349\n",
      "Removed 400 nodes out of 8428 or 4.75 % and 4245 edges out of 21384 or 19.85 %\n",
      "\n",
      "Name: Temporal Network (4)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 7449\n",
      "Average in degree: 266.0357\n",
      "Average out degree: 266.0357\n",
      "Number of times: 295\n",
      "Removed 0 nodes out of 28 or 0.00 % and 2945 edges out of 10394 or 28.33 %\n",
      "\n",
      "EPOCH 5:\n",
      "---------\n",
      "Removing 100 nodes out of 8028 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (5)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7928\n",
      "Number of edges: 16212\n",
      "Average in degree:   2.0449\n",
      "Average out degree:   2.0449\n",
      "Removed 500 nodes out of 8428 or 5.93 % and 5172 edges out of 21384 or 24.19 %\n",
      "\n",
      "Name: Temporal Network (5)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 6803\n",
      "Average in degree: 242.9643\n",
      "Average out degree: 242.9643\n",
      "Number of times: 293\n",
      "Removed 0 nodes out of 28 or 0.00 % and 3591 edges out of 10394 or 34.55 %\n",
      "\n",
      "EPOCH 6:\n",
      "---------\n",
      "Removing 100 nodes out of 7928 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (6)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7828\n",
      "Number of edges: 15416\n",
      "Average in degree:   1.9693\n",
      "Average out degree:   1.9693\n",
      "Removed 600 nodes out of 8428 or 7.12 % and 5968 edges out of 21384 or 27.91 %\n",
      "\n",
      "Name: Temporal Network (6)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 6298\n",
      "Average in degree: 224.9286\n",
      "Average out degree: 224.9286\n",
      "Number of times: 291\n",
      "Removed 0 nodes out of 28 or 0.00 % and 4096 edges out of 10394 or 39.41 %\n",
      "\n",
      "EPOCH 7:\n",
      "---------\n",
      "Removing 100 nodes out of 7828 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (7)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7728\n",
      "Number of edges: 14665\n",
      "Average in degree:   1.8976\n",
      "Average out degree:   1.8976\n",
      "Removed 700 nodes out of 8428 or 8.31 % and 6719 edges out of 21384 or 31.42 %\n",
      "\n",
      "Name: Temporal Network (7)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 5815\n",
      "Average in degree: 207.6786\n",
      "Average out degree: 207.6786\n",
      "Number of times: 286\n",
      "Removed 0 nodes out of 28 or 0.00 % and 4579 edges out of 10394 or 44.05 %\n",
      "\n",
      "EPOCH 8:\n",
      "---------\n",
      "Removing 100 nodes out of 7728 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (8)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7628\n",
      "Number of edges: 13993\n",
      "Average in degree:   1.8344\n",
      "Average out degree:   1.8344\n",
      "Removed 800 nodes out of 8428 or 9.49 % and 7391 edges out of 21384 or 34.56 %\n",
      "\n",
      "Name: Temporal Network (8)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 5396\n",
      "Average in degree: 192.7143\n",
      "Average out degree: 192.7143\n",
      "Number of times: 275\n",
      "Removed 0 nodes out of 28 or 0.00 % and 4998 edges out of 10394 or 48.09 %\n",
      "\n",
      "EPOCH 9:\n",
      "---------\n",
      "Removing 100 nodes out of 7628 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (9)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7528\n",
      "Number of edges: 13280\n",
      "Average in degree:   1.7641\n",
      "Average out degree:   1.7641\n",
      "Removed 900 nodes out of 8428 or 10.68 % and 8104 edges out of 21384 or 37.90 %\n",
      "\n",
      "Name: Temporal Network (9)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 4924\n",
      "Average in degree: 175.8571\n",
      "Average out degree: 175.8571\n",
      "Number of times: 266\n",
      "Removed 0 nodes out of 28 or 0.00 % and 5470 edges out of 10394 or 52.63 %\n",
      "\n",
      "EPOCH 10:\n",
      "---------\n",
      "Removing 100 nodes out of 7528 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (10)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7428\n",
      "Number of edges: 12583\n",
      "Average in degree:   1.6940\n",
      "Average out degree:   1.6940\n",
      "Removed 1000 nodes out of 8428 or 11.87 % and 8801 edges out of 21384 or 41.16 %\n",
      "\n",
      "Name: Temporal Network (10)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 4445\n",
      "Average in degree: 158.7500\n",
      "Average out degree: 158.7500\n",
      "Number of times: 254\n",
      "Removed 0 nodes out of 28 or 0.00 % and 5949 edges out of 10394 or 57.23 %\n",
      "\n",
      "EPOCH 11:\n",
      "---------\n",
      "Removing 100 nodes out of 7428 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (11)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7328\n",
      "Number of edges: 12009\n",
      "Average in degree:   1.6388\n",
      "Average out degree:   1.6388\n",
      "Removed 1100 nodes out of 8428 or 13.05 % and 9375 edges out of 21384 or 43.84 %\n",
      "\n",
      "Name: Temporal Network (11)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 4057\n",
      "Average in degree: 144.8929\n",
      "Average out degree: 144.8929\n",
      "Number of times: 245\n",
      "Removed 0 nodes out of 28 or 0.00 % and 6337 edges out of 10394 or 60.97 %\n",
      "\n",
      "EPOCH 12:\n",
      "---------\n",
      "Removing 100 nodes out of 7328 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (12)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7228\n",
      "Number of edges: 11312\n",
      "Average in degree:   1.5650\n",
      "Average out degree:   1.5650\n",
      "Removed 1200 nodes out of 8428 or 14.24 % and 10072 edges out of 21384 or 47.10 %\n",
      "\n",
      "Name: Temporal Network (12)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 3616\n",
      "Average in degree: 129.1429\n",
      "Average out degree: 129.1429\n",
      "Number of times: 234\n",
      "Removed 0 nodes out of 28 or 0.00 % and 6778 edges out of 10394 or 65.21 %\n",
      "\n",
      "EPOCH 13:\n",
      "---------\n",
      "Removing 100 nodes out of 7228 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (13)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7128\n",
      "Number of edges: 10761\n",
      "Average in degree:   1.5097\n",
      "Average out degree:   1.5097\n",
      "Removed 1300 nodes out of 8428 or 15.42 % and 10623 edges out of 21384 or 49.68 %\n",
      "\n",
      "Name: Temporal Network (13)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 3268\n",
      "Average in degree: 116.7143\n",
      "Average out degree: 116.7143\n",
      "Number of times: 212\n",
      "Removed 0 nodes out of 28 or 0.00 % and 7126 edges out of 10394 or 68.56 %\n",
      "\n",
      "EPOCH 14:\n",
      "---------\n",
      "Removing 100 nodes out of 7128 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (14)\n",
      "Type: DiGraph\n",
      "Number of nodes: 7028\n",
      "Number of edges: 10289\n",
      "Average in degree:   1.4640\n",
      "Average out degree:   1.4640\n",
      "Removed 1400 nodes out of 8428 or 16.61 % and 11095 edges out of 21384 or 51.88 %\n",
      "\n",
      "Name: Temporal Network (14)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 2981\n",
      "Average in degree: 106.4643\n",
      "Average out degree: 106.4643\n",
      "Number of times: 190\n",
      "Removed 0 nodes out of 28 or 0.00 % and 7413 edges out of 10394 or 71.32 %\n",
      "\n",
      "EPOCH 15:\n",
      "---------\n",
      "Removing 100 nodes out of 7028 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (15)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6928\n",
      "Number of edges: 9782\n",
      "Average in degree:   1.4120\n",
      "Average out degree:   1.4120\n",
      "Removed 1500 nodes out of 8428 or 17.80 % and 11602 edges out of 21384 or 54.26 %\n",
      "\n",
      "Name: Temporal Network (15)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 2680\n",
      "Average in degree:  95.7143\n",
      "Average out degree:  95.7143\n",
      "Number of times: 164\n",
      "Removed 0 nodes out of 28 or 0.00 % and 7714 edges out of 10394 or 74.22 %\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16:\n",
      "---------\n",
      "Removing 100 nodes out of 6928 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (16)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6828\n",
      "Number of edges: 9248\n",
      "Average in degree:   1.3544\n",
      "Average out degree:   1.3544\n",
      "Removed 1600 nodes out of 8428 or 18.98 % and 12136 edges out of 21384 or 56.75 %\n",
      "\n",
      "Name: Temporal Network (16)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 2329\n",
      "Average in degree:  83.1786\n",
      "Average out degree:  83.1786\n",
      "Number of times: 138\n",
      "Removed 0 nodes out of 28 or 0.00 % and 8065 edges out of 10394 or 77.59 %\n",
      "\n",
      "EPOCH 17:\n",
      "---------\n",
      "Removing 100 nodes out of 6828 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (17)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6728\n",
      "Number of edges: 8769\n",
      "Average in degree:   1.3034\n",
      "Average out degree:   1.3034\n",
      "Removed 1700 nodes out of 8428 or 20.17 % and 12615 edges out of 21384 or 58.99 %\n",
      "\n",
      "Name: Temporal Network (17)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 2046\n",
      "Average in degree:  73.0714\n",
      "Average out degree:  73.0714\n",
      "Number of times: 104\n",
      "Removed 0 nodes out of 28 or 0.00 % and 8348 edges out of 10394 or 80.32 %\n",
      "\n",
      "EPOCH 18:\n",
      "---------\n",
      "Removing 100 nodes out of 6728 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (18)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6628\n",
      "Number of edges: 8387\n",
      "Average in degree:   1.2654\n",
      "Average out degree:   1.2654\n",
      "Removed 1800 nodes out of 8428 or 21.36 % and 12997 edges out of 21384 or 60.78 %\n",
      "\n",
      "Name: Temporal Network (18)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 1818\n",
      "Average in degree:  64.9286\n",
      "Average out degree:  64.9286\n",
      "Number of times: 72\n",
      "Removed 0 nodes out of 28 or 0.00 % and 8576 edges out of 10394 or 82.51 %\n",
      "\n",
      "EPOCH 19:\n",
      "---------\n",
      "Removing 100 nodes out of 6628 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (19)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6528\n",
      "Number of edges: 7783\n",
      "Average in degree:   1.1922\n",
      "Average out degree:   1.1922\n",
      "Removed 1900 nodes out of 8428 or 22.54 % and 13601 edges out of 21384 or 63.60 %\n",
      "\n",
      "Name: Temporal Network (19)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 1417\n",
      "Average in degree:  50.6071\n",
      "Average out degree:  50.6071\n",
      "Number of times: 46\n",
      "Removed 0 nodes out of 28 or 0.00 % and 8977 edges out of 10394 or 86.37 %\n",
      "\n",
      "EPOCH 20:\n",
      "---------\n",
      "Removing 100 nodes out of 6528 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (20)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6428\n",
      "Number of edges: 7038\n",
      "Average in degree:   1.0949\n",
      "Average out degree:   1.0949\n",
      "Removed 2000 nodes out of 8428 or 23.73 % and 14346 edges out of 21384 or 67.09 %\n",
      "\n",
      "Name: Temporal Network (20)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 894\n",
      "Average in degree:  31.9286\n",
      "Average out degree:  31.9286\n",
      "Number of times: 43\n",
      "Removed 0 nodes out of 28 or 0.00 % and 9500 edges out of 10394 or 91.40 %\n",
      "\n",
      "EPOCH 21:\n",
      "---------\n",
      "Removing 100 nodes out of 6428 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (21)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6328\n",
      "Number of edges: 6427\n",
      "Average in degree:   1.0156\n",
      "Average out degree:   1.0156\n",
      "Removed 2100 nodes out of 8428 or 24.92 % and 14957 edges out of 21384 or 69.94 %\n",
      "\n",
      "Name: Temporal Network (21)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 28\n",
      "Number of edges: 498\n",
      "Average in degree:  17.7857\n",
      "Average out degree:  17.7857\n",
      "Number of times: 40\n",
      "Removed 0 nodes out of 28 or 0.00 % and 9896 edges out of 10394 or 95.21 %\n",
      "\n",
      "EPOCH 22:\n",
      "---------\n",
      "Removing 100 nodes out of 6328 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (22)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6228\n",
      "Number of edges: 5992\n",
      "Average in degree:   0.9621\n",
      "Average out degree:   0.9621\n",
      "Removed 2200 nodes out of 8428 or 26.10 % and 15392 edges out of 21384 or 71.98 %\n",
      "\n",
      "Name: Temporal Network (22)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 25\n",
      "Number of edges: 218\n",
      "Average in degree:   8.7200\n",
      "Average out degree:   8.7200\n",
      "Number of times: 19\n",
      "Removed 3 nodes out of 28 or 10.71 % and 10176 edges out of 10394 or 97.90 %\n",
      "\n",
      "EPOCH 23:\n",
      "---------\n",
      "Removing 100 nodes out of 6228 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (23)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6128\n",
      "Number of edges: 5679\n",
      "Average in degree:   0.9267\n",
      "Average out degree:   0.9267\n",
      "Removed 2300 nodes out of 8428 or 27.29 % and 15705 edges out of 21384 or 73.44 %\n",
      "\n",
      "Name: Temporal Network (23)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 21\n",
      "Number of edges: 58\n",
      "Average in degree:   2.7619\n",
      "Average out degree:   2.7619\n",
      "Number of times: 15\n",
      "Removed 7 nodes out of 28 or 25.00 % and 10336 edges out of 10394 or 99.44 %\n",
      "\n",
      "EPOCH 24:\n",
      "---------\n",
      "Removing 100 nodes out of 6128 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (24)\n",
      "Type: DiGraph\n",
      "Number of nodes: 6028\n",
      "Number of edges: 5502\n",
      "Average in degree:   0.9127\n",
      "Average out degree:   0.9127\n",
      "Removed 2400 nodes out of 8428 or 28.48 % and 15882 edges out of 21384 or 74.27 %\n",
      "\n",
      "Name: Temporal Network (24)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 25:\n",
      "---------\n",
      "Removing 100 nodes out of 6028 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (25)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5928\n",
      "Number of edges: 5413\n",
      "Average in degree:   0.9131\n",
      "Average out degree:   0.9131\n",
      "Removed 2500 nodes out of 8428 or 29.66 % and 15971 edges out of 21384 or 74.69 %\n",
      "\n",
      "Name: Temporal Network (25)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 26:\n",
      "---------\n",
      "Removing 100 nodes out of 5928 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (26)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5828\n",
      "Number of edges: 5326\n",
      "Average in degree:   0.9139\n",
      "Average out degree:   0.9139\n",
      "Removed 2600 nodes out of 8428 or 30.85 % and 16058 edges out of 21384 or 75.09 %\n",
      "\n",
      "Name: Temporal Network (26)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 27:\n",
      "---------\n",
      "Removing 100 nodes out of 5828 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (27)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5728\n",
      "Number of edges: 5240\n",
      "Average in degree:   0.9148\n",
      "Average out degree:   0.9148\n",
      "Removed 2700 nodes out of 8428 or 32.04 % and 16144 edges out of 21384 or 75.50 %\n",
      "\n",
      "Name: Temporal Network (27)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 28:\n",
      "---------\n",
      "Removing 100 nodes out of 5728 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (28)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5628\n",
      "Number of edges: 5146\n",
      "Average in degree:   0.9144\n",
      "Average out degree:   0.9144\n",
      "Removed 2800 nodes out of 8428 or 33.22 % and 16238 edges out of 21384 or 75.94 %\n",
      "\n",
      "Name: Temporal Network (28)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 29:\n",
      "---------\n",
      "Removing 100 nodes out of 5628 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (29)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5528\n",
      "Number of edges: 5063\n",
      "Average in degree:   0.9159\n",
      "Average out degree:   0.9159\n",
      "Removed 2900 nodes out of 8428 or 34.41 % and 16321 edges out of 21384 or 76.32 %\n",
      "\n",
      "Name: Temporal Network (29)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 30:\n",
      "---------\n",
      "Removing 100 nodes out of 5528 selected based on temporal HITS scores ...\n",
      "\n",
      "Name: Time-ordered Network (30)\n",
      "Type: DiGraph\n",
      "Number of nodes: 5428\n",
      "Number of edges: 4977\n",
      "Average in degree:   0.9169\n",
      "Average out degree:   0.9169\n",
      "Removed 3000 nodes out of 8428 or 35.60 % and 16407 edges out of 21384 or 76.73 %\n",
      "\n",
      "Name: Temporal Network (30)\n",
      "Type: MultiDiGraph\n",
      "Number of nodes: 0\n",
      "Number of edges: 0\n",
      "\n",
      "Number of times: 0\n",
      "Removed 28 nodes out of 28 or 100.00 % and 10394 edges out of 10394 or 100.00 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:61: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "/home/alif/venv/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "g = hits_remove(\n",
    "    epoch=30,  # Repeat for 1 epoch\n",
    "    remove=0.5,  # Stop when reach 10 % removed\n",
    "    step=100,  # Remove 100 nodes at every epoch\n",
    "    strategy_a='a',  # Use authority\n",
    "    strategy_b='t',  # Use temporal\n",
    "    strategy_c='n',  # Use ratio\n",
    "    strategy_d=1,  # Score-based\n",
    "    actions=[0, 2],\n",
    "    save_networks=True,\n",
    "    output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T03:42:06.914298Z",
     "start_time": "2021-02-18T03:42:06.907403Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(g)\n",
    "\n",
    "# print(nx.info(g[45]))\n",
    "\n",
    "# list(g[45].edges(data=True))\n",
    "list(g[47].edges(data=True)) # 47 is the network where its corresponding TON graph is broken in a way that there is no crossed edges left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T18:47:35.771531Z",
     "start_time": "2021-02-14T18:47:35.110040Z"
    }
   },
   "outputs": [],
   "source": [
    "# hits_remove(\n",
    "#     label_input='',\n",
    "#     label_hits='',\n",
    "#     label_net='',\n",
    "#     label_ew='',\n",
    "#     label_output='',\n",
    "#     label_folder='',\n",
    "#     # ---\n",
    "#     epoch=1000,  # Repeat for 1000 epoch\n",
    "#     remove=0.1,  # Stop when reach 10 % removed\n",
    "#     step=0.01,  # Remove 1 % every epoch\n",
    "#     strategy_a='a',  # Use authority\n",
    "#     strategy_b='t',  # Use temporal\n",
    "#     strategy_c='r',  # Use ratio\n",
    "#     strategy_d=1,  # Score-based\n",
    "#     time_window=(5, 12),  # Remove high rank nodes between 5 pm - 12 am\n",
    "#     actions=[1],\n",
    "#     # ---\n",
    "#     save_networks=True,\n",
    "#     save_hits=True,\n",
    "#     # ---\n",
    "#     output_changes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Intersection Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$isim_K(L^1,L^2)=\\frac{1}{k}\\sum_{i=1}^{k}\\frac{|L_{i}^{1}\\Delta L_{i}^{2}|}{2i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isim(list1, list2):\n",
    "    '''\n",
    "    Calculate intercention similarity of two input lists\n",
    "    '''\n",
    "    # check if both list have the same size\n",
    "    if len(list1) < len(list2):\n",
    "        list2 = list2[:len(list1)]\n",
    "    else:\n",
    "        list1 = list1[:len(list2)]\n",
    "\n",
    "    isim = []\n",
    "    for i in range(1, len(list1) + 1):\n",
    "        set1 = set(list1[:i])\n",
    "        set2 = set(list2[:i])\n",
    "        set_dif = set1 ^ set2  # symmetric difference\n",
    "        isim.append(len(set_dif) / (2 * i))\n",
    "\n",
    "    isim_norm = []\n",
    "    for i in range(len(isim)):\n",
    "        isim_norm.append(sum(isim[:i + 1]) / (i + 1))\n",
    "\n",
    "    return isim_norm\n",
    "\n",
    "\n",
    "# l1 = np.random.randint(50, size=20)\n",
    "# l2 = np.random.randint(50, size=20)\n",
    "# print(l1)\n",
    "# print(l2)\n",
    "# isim(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def hits_isim(\n",
    "    times=None,\n",
    "    nodes=None,\n",
    "    edge_weight=None,\n",
    "    analyze_a=True,\n",
    "    analyze_h=True,\n",
    "    folder_input=['hits'],\n",
    "    file_input=['networks/temporal_times.csv', 'networks/temporal_nodes.csv'],\n",
    "    file_output=['output.csv'],\n",
    "    input_label='',\n",
    "    output_label='',\n",
    "    visualized=True,\n",
    "    save=True,\n",
    "    top=100):\n",
    "    \"\"\"\n",
    "    Function to calculate intersection similarity between differnt calculated HITS scores\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(HITS_PATH):\n",
    "        if file.endswith('.csv') and file.startswith('A'):\n",
    "            if 'score' in file:\n",
    "                files.append(os.path.join(HITS_PATH, file))\n",
    "    files.sort()\n",
    "\n",
    "    isims = []\n",
    "    combs = []\n",
    "    comb = combinations(range(len(files)), 2)\n",
    "    for i, j in comb:\n",
    "        combs.append((i, j))\n",
    "        # read size-2 combination of score files\n",
    "        score1 = pd.read_csv(files[i]).values\n",
    "        score2 = pd.read_csv(files[j]).values\n",
    "        # calulate average score of nodes\n",
    "        avg_1 = score1.sum(axis=1) / score1.shape[1]\n",
    "        avg_2 = score2.sum(axis=1) / score2.shape[1]\n",
    "        # index-sort of nodes with highest average score to lowest\n",
    "        top1 = avg_1.argsort()[::-1][:top]\n",
    "        top2 = avg_2.argsort()[::-1][:top]\n",
    "        # calcualte  of two lists of top nodes\n",
    "        isims.append(isim(top1, top2))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(isims), ncols=1, figsize=(8, 6))\n",
    "    for i, ax in enumerate(axes):\n",
    "        x = range(1, len(isims[i]) + 1)\n",
    "        y = isims[i]\n",
    "        l = files[combs[i][0]].split('/')[-1].split('.')[0] + ' vs. ' + files[\n",
    "            combs[i][1]].split('/')[-1].split('.')[0]\n",
    "        ax.plot(x, y, label=l)\n",
    "        ax.legend()\n",
    "        ax.set_title('Intersection Similarity')\n",
    "    fig.savefig(IMAGE_PATH + '/isim.pdf',\n",
    "                dpi=300,\n",
    "                bbox_inches='tight',\n",
    "                transparent=True)\n",
    "\n",
    "    print(isims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Centrality Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$I_R=\\sum_{i=1}^{N}|R^{o}_{i}-R^{n}_{i}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "size = 10\n",
    "l1 = np.random.choice(size, size=size, replace=False)\n",
    "rank1 = dict(zip(l1, range(len(l1))))\n",
    "# or\n",
    "# rank1 = {v: k for k, v in enumerate(np.random.choice(10, size=10, replace=False))}\n",
    "l2 = np.random.permutation(l1)\n",
    "rank2 = dict(zip(l2, range(len(l2))))\n",
    "print(rank1)\n",
    "print(rank2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def robustness(rank1, rank2):\n",
    "    '''\n",
    "    Description:\n",
    "        Calculate robustness of centrality algorithm to imposed noise on network\n",
    "    Parameters:\n",
    "        rank1 (dict): {node_if:rank} of original network\n",
    "        rank2 (dict): top ranking dictionary of noisy network\n",
    "    '''\n",
    "    rob = []  # robustness\n",
    "    for node in sorted(rank1):\n",
    "        rob.append(abs(rank1[node] - rank2[node]))\n",
    "    for node, rank_change in enumerate(rob):\n",
    "        print(node, ':', rank_change)\n",
    "    return sum(rob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "robustness(rank1, rank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Supra-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def supra(\n",
    "    graph,\n",
    "    times,\n",
    "    version=0,\n",
    "    parameter_omega=1,  # horizontal link weight\n",
    "    parameter_epsilon=1,  # crossed link weight\n",
    "    parameter_gamma=0.0001,  # horizontal link teleportation\n",
    "    parameter_sigma=0.85,  # crossed link teleportation\n",
    "    parameter_directed=False,\n",
    "    output=False,\n",
    "    output_int=False,\n",
    "    save=False):\n",
    "    \"\"\"\n",
    "    Create supra-matrix from tempral network\n",
    "    v-0 Taylor paper with undirected inter-layer temporal links and default w=1\n",
    "        .0 C = A eigenvector centrality\n",
    "        .1 C = A^TA authorities centrality\n",
    "        .2 C = AA^T hub centrality\n",
    "        .3 C = (alpha * A^T * D^-1) + ((1-alpha) * N^-1 * 11^T) pageRank\n",
    "    v-1: Taylor paper with directed inter-layer temporal links and default w=1 \n",
    "    \"\"\"\n",
    "\n",
    "    # variables\n",
    "    N = graph.number_of_nodes()\n",
    "    L = graph.number_of_edges()\n",
    "    T = len(times)\n",
    "\n",
    "    # we have timestamps map via \"times\" which is a series index -> timestamp\n",
    "    # we also need such a map for nodes i.e. index -> node_id/name\n",
    "    nodes = pd.Series(sorted(list(graph.nodes())))\n",
    "\n",
    "    S = np.zeros((N * T, N * T))  # supra-matrix\n",
    "    A = np.kron(np.zeros((T, T)), np.identity(N))  # temporal coupling\n",
    "\n",
    "    # list of ordered graph at different time\n",
    "    # i.e. G = [graph(t_1),graph(t_2),...,graph(t_T)]\n",
    "    G = [nx.DiGraph() for _ in range(T)]\n",
    "    for u, v, w in graph.edges(data=True):\n",
    "        t_index = times[times == w['t']].index[0]\n",
    "        u_index = nodes[nodes == u].index[0]\n",
    "        v_index = nodes[nodes == v].index[0]\n",
    "        G[t_index].add_edge(u_index, v_index)\n",
    "\n",
    "    # list of ordered centrality matrices at different time\n",
    "    # i.e. C = [C(t_1),C(t_2),...,C(t_T)]\n",
    "    C = []\n",
    "\n",
    "    # add horizontal (i.e. inter-layer) edges\n",
    "    if not parameter_directed:  # undirected temporal\n",
    "        # (non-vectorized)\n",
    "        #         for t in range(T - 1):\n",
    "        #             S[N * t:N * (t + 1), N * (t + 1):N * (t + 2)] = np.identity(\n",
    "        #                 (N)) * parameter_omega\n",
    "        #             S[N * (t + 1):N * (t + 2), N * t:N * (t + 1)] = np.identity(\n",
    "        #                 (N)) * parameter_omega\n",
    "        # (vectorized)\n",
    "        A = np.kron(np.eye(T, k=1) + np.eye(T, k=-1),\n",
    "                    np.eye(N)) * parameter_omega\n",
    "    else:  # directed\n",
    "        # 1 + gamma for t'-t=1 and gamma for rest\n",
    "        A = (np.kron(np.eye(T, k=1), np.eye(N)) +\n",
    "             np.kron(np.ones(\n",
    "                 (T, T)) * parameter_gamma, np.eye(N))) * parameter_omega\n",
    "\n",
    "    # add crossed (i.e. intra-layer) edges\n",
    "    if version == 0:  # eigenvector centrality\n",
    "        # (non-vectorized)\n",
    "        #         for u, v, w in graph.edges(data=True):\n",
    "        #             t_index = times[times == w['t']].index[0]\n",
    "        #             u_index = nodes[nodes == u].index[0]\n",
    "        #             v_index = nodes[nodes == v].index[0]\n",
    "        #             S[(N * t_index) + u_index,\n",
    "        #               (N * t_index) + v_index] = 1 * parameter_epsilon\n",
    "        # (vectorized)\n",
    "        for g in G:\n",
    "            C.append(nx.to_numpy_matrix(g) * parameter_epsilon)\n",
    "        S = la.block_diag(*C)\n",
    "\n",
    "    if version == 1:  # authority centrality\n",
    "        for g in G:\n",
    "            C.append(\n",
    "                np.dot(np.transpose(nx.to_numpy_matrix(g)),\n",
    "                       nx.to_numpy_matrix(g)) * parameter_epsilon)\n",
    "        S = la.block_diag(*C)\n",
    "\n",
    "    if version == 2:  # hub centrality\n",
    "        for g in G:\n",
    "            C.append(\n",
    "                np.dot(nx.to_numpy_matrix(g),\n",
    "                       np.transpose(nx.to_numpy_matrix(g))) *\n",
    "                parameter_epsilon)\n",
    "        S = la.block_diag(*C)\n",
    "\n",
    "    if version == 3:  # pagerank centrality\n",
    "        # D = diag[d_1,...,d_N] i.e. diagonal matrix that encodes the node out-degrees\n",
    "        for g in G:\n",
    "            Adj = nx.to_numpy_matrix(g)\n",
    "            D_inv = np.linalg.inv(\n",
    "                np.diag(np.asarray(np.sum(Adj, axis=1)).reshape(-1)))\n",
    "            C.append((parameter_sigma *\n",
    "                      np.dot(np.transpose(nx.to_numpy_matrix(g)), D_inv)) +\n",
    "                     (((1 - parameter_sigma) / N) * np.ones((N, N))))\n",
    "        S = la.block_diag(*C)\n",
    "\n",
    "    S = S + A  # at this time S = Zeros\n",
    "\n",
    "    if output:\n",
    "        row_column_index = [node for node in nodes] * T\n",
    "        print('   ', end='')\n",
    "        for element in row_column_index:\n",
    "            if not output_int:\n",
    "                print(element, '  ', end='')  # float\n",
    "            else:\n",
    "                print(element, '', end='')  # int\n",
    "        print()\n",
    "        for i, element in enumerate(S):\n",
    "            if not output_int:\n",
    "                print(row_column_index[i], '', *element)  # float\n",
    "            else:\n",
    "                print(row_column_index[i], '', *element.astype(int))  # int\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# S = supra(T, times)\n",
    "# S = supra(T, times, output_int=True, output=True)\n",
    "# S = supra(T, times, version=2, output=True)\n",
    "S = supra(T, times, version=2, parameter_directed=True, output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mat_centrality(matrix,\n",
    "                   times,\n",
    "                   version=0,\n",
    "                   parameter_negative=True,\n",
    "                   parameter_normalized=True,\n",
    "                   parameter_reshape=True,\n",
    "                   output=False,\n",
    "                   save=False):\n",
    "    \"\"\"\n",
    "    Receive supra-matrix and calulate eigenvalues and eigenvectors\n",
    "    version:\n",
    "        0: general right eigenvector using numpy/scipy\n",
    "        1: general left eigenvector\n",
    "        2: symetric right eigenvector\n",
    "    \"\"\"\n",
    "\n",
    "    T = len(times)\n",
    "    N = int(S.shape[0] / T)\n",
    "\n",
    "    eigvals = []\n",
    "    eigvecs = []\n",
    "\n",
    "    if version == 0:\n",
    "        # eigvals, eigvecs = la.eig(S)\n",
    "        eigvals, eigvecs = np.linalg.eig(S)\n",
    "    elif version == 1:\n",
    "        eigvals, eigvecs, eigvecs_right = la.eig(S, left=True)\n",
    "    elif version == 2:\n",
    "        eigvals, eigvecs = la.eigh(S)\n",
    "\n",
    "    eigvals = eigvals.real\n",
    "\n",
    "    # ind = np.argsort(eigvals)\n",
    "    # eigvals = eigvals[ind]\n",
    "    # eigvecs = eigvecs[:, ind]\n",
    "\n",
    "    max_val_ind = np.argmax(eigvals)\n",
    "    max_val = max(eigvals)\n",
    "    max_vec = eigvecs[:, max_val_ind].reshape(-1, 1).copy()\n",
    "\n",
    "    if parameter_negative and all(max_vec < 0):\n",
    "        if output: print('eigenvector *= -1')\n",
    "        max_vec *= -1\n",
    "\n",
    "    if parameter_normalized:\n",
    "        if output: print('normalizing eigenvector ...')\n",
    "        max_vec /= sum(abs(max_vec))\n",
    "\n",
    "    if parameter_reshape:\n",
    "        # max_vec = max_vec.flatten().reshape((T,N)).T\n",
    "        max_vec = max_vec.reshape((T, N)).T\n",
    "\n",
    "    if output:\n",
    "        # print('maximum eigenvalue is {} at index {}'.format(max_val, max_val_ind))\n",
    "        # print(eigvals)\n",
    "        # print(max_vec)\n",
    "        matrix_print(max_vec, out_int=False)\n",
    "\n",
    "    return max_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mat_cen = mat_centrality(S, times, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Some eigenvector test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SVD\n",
    "# u,s,v = np.linalg.svd(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs = la.eig(S)\n",
    "# eigvals, eigvecs = np.linalg.eig(S)\n",
    "# eigvals, eigvecs, eigvecs_right = la.eig(S, left=True)\n",
    "\n",
    "eigvals = eigvals.real\n",
    "print(eigvals)\n",
    "\n",
    "max_index = np.argmax(eigvals)\n",
    "max_value = max(eigvals)\n",
    "print('max = {} at index {}'.format(max_value, max_index))\n",
    "max_vec = eigvecs[:, max_index].reshape(-1, 1).copy()\n",
    "if all(max_vec < 0):\n",
    "    print('all of components are negative')\n",
    "    max_vec *= -1\n",
    "max_vec /= sum(abs(max_vec))\n",
    "print(max_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# max_vec.flatten()\n",
    "# max_vec.flatten().reshape((6,4)).T\n",
    "max_vec.reshape((6, 4)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dataset(output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Temporal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "# T, times = temporal_create(\n",
    "#     input_label='',\n",
    "#     output_label='',\n",
    "#     # year_from=1936,\n",
    "#     year_until=1990,  # 2018\n",
    "#     filter_degree=1,\n",
    "#     output_times=False,\n",
    "#     output_network=True,\n",
    "#     save_times=True,\n",
    "#     save_nodes=True,\n",
    "#     save_network=True,\n",
    "#     save_edgelist=False)\n",
    "\n",
    "# read\n",
    "# T, times = temporal_read(input_label='90_1', output_network=True)\n",
    "# times = times_read(input_label='90_1')\n",
    "# nodes = nodes_read(input_label='90_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Static Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "# G = static_create(T,\n",
    "#                   output_label='90_1',\n",
    "#                   input_undirected=True,\n",
    "#                   output_network=True,\n",
    "#                   save_network=True,\n",
    "#                   save_edgelist=False)\n",
    "\n",
    "# read\n",
    "# G = static_read(input_label='90_1', output_network=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### DT Network -Light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "# D = dt_create(T,\n",
    "#               times,\n",
    "#               output_label='90_1',\n",
    "#               parameter_light=True,\n",
    "#               parameter_teleport=False,\n",
    "#               parameter_undirected=False,\n",
    "#               parameter_loop=False,\n",
    "#               output_network=True,\n",
    "#               save_network=True,\n",
    "#               save_edgelist=False)\n",
    "\n",
    "# read\n",
    "D = dt_read(input_label='90_1', output_network=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### DT Network -Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "# D = dt_create(\n",
    "#     T,\n",
    "#     times,\n",
    "#     output_label='taylor',\n",
    "#     parameter_light=False,\n",
    "#     version=0,\n",
    "#     parameter_omega=1,\n",
    "#     parameter_epsilon=1,\n",
    "#     parameter_alpha=0.5,\n",
    "#     parameter_distance=1,\n",
    "#     parameter_teleport=True,\n",
    "#     parameter_gamma=0.0001,\n",
    "#     parameter_sigma=0.85,\n",
    "#     parameter_undirected=True,\n",
    "#     parameter_loop=True,\n",
    "#     parameter_color=True,\n",
    "#     parameter_delta=False,\n",
    "#     parameter_delta_type='h',\n",
    "#     output_delta=False,\n",
    "#     output_weight=False,\n",
    "#     output_network=True,\n",
    "#     save_delta=False,\n",
    "#     save_network=True,\n",
    "#     save_edgelist=False)\n",
    "\n",
    "# read\n",
    "# D = dt_read(input_label='taylor', output_network_stat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### EW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create\n",
    "# ew = ew_create(\n",
    "#     D,\n",
    "#     input_label='90_1',\n",
    "#     output_label='90_1',\n",
    "#     undirected=False,\n",
    "#     omega=1,  # 1\n",
    "#     epsilon=1,\n",
    "#     gamma=0.0001,\n",
    "#     penalize=True,  # False\n",
    "#     version=3,  # 1\n",
    "#     distance=1,\n",
    "#     alpha=0.5,\n",
    "#     save=True,\n",
    "#     output=False)\n",
    "\n",
    "# read\n",
    "ew = ew_read(input_label='90_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a, h = hits(\n",
    "    D,\n",
    "    times,\n",
    "    nodes=None,\n",
    "    edge_weight=ew,\n",
    "    input_label='90_1',\n",
    "    output_label='90_1_rand',\n",
    "    max_iter=500,  # 100\n",
    "    version=3,  # 1\n",
    "    sigma=0.95,  # 0.85\n",
    "    norm_max=True,\n",
    "    norm_final=True,\n",
    "    norm_iter=False,\n",
    "    norm_degree=False,\n",
    "    norm_damping=False,\n",
    "    round_num=5,\n",
    "    save=True,\n",
    "    analyze=True,\n",
    "    report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T16:21:37.648131Z",
     "start_time": "2020-04-27T16:21:37.640404Z"
    },
    "hidden": true
   },
   "source": [
    "year_until = 90, filter_node_degree = 1, epsilon = 1, omega = 0.8, penalize = False (no dynamic temporal edge weight)    \n",
    "netx -> finished at 196 iterations    \n",
    "book -> finished at 181 iterations    \n",
    "paper -> finished at 72 iterations    \n",
    "method 4 -> finished at 180 iteration and 52m    \n",
    "method 5 -> finished at 4 iteration and 3m    \n",
    "method 6 -> finished at 2 iteration and 2m    \n",
    "***\n",
    "year_until = 90, filter_node_degree = 1, epsilon = 1, omega = 0.95, penalize = False (no dynamic temporal edge weight)    \n",
    "paper -> finished at 172 iterations  \n",
    "***\n",
    "netx -> finished at 198 iterations and 1h38m   \n",
    "book -> finished at 182 iterations and 1h9m    \n",
    "paper -> finished at 273 iterations and 2h40m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### HITS Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = hits_analyze(\n",
    "    a,\n",
    "    h,\n",
    "    D,\n",
    "    edge_weight=ew,\n",
    "    input_label='90_1',\n",
    "    output_label='',\n",
    "    top=1,\n",
    "    section=3,  # 1\n",
    "    report_num=100,  # 1000\n",
    "    round_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = hits_analyze_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# len(df)\n",
    "# df.head(6)\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:240]  # row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ids = [int(idx[i].split('_')[0]) for i in range(0, len(idx), 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(ids)):\n",
    "    print(i, \":\", ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_nodes, h_nodes = np.array_split(\n",
    "    [int(idx[i].split('_')[0]) for i in range(0, len(idx), 6)], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### ISIM and Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hits_isim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Paper Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[('b_g', 3.49), ('b_rt', 4.24), ('b_tt_1', 4.78), ('o_gy1', 5.0), ('y_rt', 5.08)]# only return the largest eigenvalue and eigenvector\n",
    "# eigvals, eigvecs = la.eigh(S, lower=False, eigvals=(23, 23))\n",
    "# print(eigvals[0])\n",
    "# print(eigvecs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vecc = np.array([\n",
    "    0.0305, 0.0198, 0.0249, 0.0238, 0.0461, 0.0368, 0.0491, 0.0465, 0.0493,\n",
    "    0.0480, 0.0592, 0.0660, 0.0460, 0.0501, 0.0520, 0.0744, 0.0360, 0.0471,\n",
    "    0.0402, 0.0552, 0.0195, 0.0308, 0.0212, 0.0275\n",
    "])\n",
    "print(vecc.sum())\n",
    "vecc = vecc.reshape((6, 4)).T\n",
    "print(vecc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "sum of the all of centralities = 1 meaning the largest eigenvector is normalized with L-1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1fb7b506ebc3278d414a9cb99032ea74abe5bd5354639d5975aa94eabd1f0026"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}