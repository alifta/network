{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ---\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_style('ticks')\n",
    "# sns.set_context('notebook')\n",
    "# sns.set(rc={'figure.dpi': 300, 'savefig.dpi': 300})\n",
    "\n",
    "# ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as clr\n",
    "plt.style.use('science')\n",
    "# plt.style.use(['science','notebook'])\n",
    "# plt.style.use('seaborn-ticks')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# ---\n",
    "\n",
    "from graph.data import *\n",
    "from graph.network import *\n",
    "from graph.temporal import *\n",
    "from graph.utils import *\n",
    "\n",
    "# ---\n",
    "\n",
    "# import graph.data\n",
    "# import graph.network\n",
    "# import graph.temporal\n",
    "# import graph.utils\n",
    "\n",
    "# ---\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(graph.data)\n",
    "# importlib.reload(graph.network)\n",
    "# importlib.reload(graph.temporal)\n",
    "# importlib.reload(graph.utils)\n",
    "\n",
    "# ---\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4]\n",
    "l[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.extend((4, 5))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/home/ali/Projects/Network/data/phonelab/data/svm_result.csv'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['cp'], df['precision'])\n",
    "plt.xlabel('Number of utilized components (or features) in SVM')\n",
    "plt.ylabel('SVM Result (e.g. Percision)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-> SSID (Connect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = np.load('/home/ali/Projects/Network/data/phonelab/data/user_ssid_connect.npy')\n",
    "# sM = sparse.csc_matrix(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sM = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/user_ssid_connect_sparse.npz')\n",
    "# M = sM.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User @ Day -> SSID (Connect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = '/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_sparse.npz'\n",
    "fname = '/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_sparse_selected.npz'\n",
    "sM = sparse.load_npz(fname)\n",
    "M = sM.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User -> Location (Connect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = '/home/ali/Projects/Network/data/phonelab/data/user_location_connect.npy'\n",
    "# M = np.load(fname)\n",
    "# sM = sparse.csc_matrix(M)\n",
    "\n",
    "fname = '/home/ali/Projects/Network/data/phonelab/data/user_location_connect_sparse.npz'\n",
    "sM = sparse.load_npz(fname)\n",
    "M = sM.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User @ Day -> Location (Connect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM = sparse.load_npz(\n",
    "    '/home/ali/Projects/Network/data/phonelab/data/user_day_location_connect_sparse.npz'\n",
    ")\n",
    "M = sM.toarray()\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing connected SSID and users matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 1176)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User -> Location (Connect) has 270 users (row) and 1,176 SSID (column)    \n",
    "User-Day -> SSID-Hour (Connect) has 19,144 rows and 28,224 columns (=1176x24)    \n",
    "User-Day -> Selected-SSID-Hour (Connect) has 5,151 rows and 6,072 columns (253=x24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert M to matrix of zeros and ones\n",
    "M[M > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code, turn matrix M to unweighted i.e. element is 1 if user connected to  \n",
    "SSID and 0 if not connected (before the element was the number of connection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "user_dist = np.array(np.sum(M, axis=1, dtype=int).reshape(1, -1))[0]\n",
    "print(np.where(user_dist == 0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to check to see if there is any USER that has not been connected to any    \n",
    "SSID or visited a location, i.e., if the sum of row (representing a user's data)    \n",
    "has no information, which then implies that there has been a bug in code    \n",
    "(in fact there was one in earlier version of code) and then if the list is empty    \n",
    "we can move on to next step of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_dist[:10])\n",
    "print(list(enumerate(user_dist))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of time the firs 10 users {1,...,10} connected to SSIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(\n",
    "    user_dist,\n",
    "    bins=range(min(user_dist),\n",
    "               max(user_dist) + 1, 1),\n",
    "    # cumulative=True,\n",
    "    alpha=0.7,\n",
    "    rwidth=0.5,\n",
    "    align='left'\n",
    ")\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Connected SSID by Users Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25 users connected to 1 SSID only, and at maximum 1 user connected to 38 unique  \n",
    "SSIDs. Using cumulative plotting, we can see that considering users connecting  \n",
    "to 1 to 5 SSID would add up to about 130 users which is half of 270 users.\n",
    "\n",
    "Now the question is, if we consider top 2 or 3 connected SSID of each user then  \n",
    "how many important SSID would we have at the end ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 14 only has connected to one ssid 51\n",
      "node 18 only has connected to one ssid 72\n",
      "node 19 only has connected to one ssid 51\n",
      "node 31 only has connected to one ssid 1\n",
      "node 32 only has connected to one ssid 51\n",
      "node 43 only has connected to one ssid 1\n",
      "node 47 only has connected to one ssid 1\n",
      "node 54 only has connected to one ssid 266\n",
      "node 58 only has connected to one ssid 293\n",
      "node 83 only has connected to one ssid 407\n",
      "node 89 only has connected to one ssid 85\n",
      "node 94 only has connected to one ssid 1\n",
      "node 130 only has connected to one ssid 645\n",
      "node 140 only has connected to one ssid 691\n",
      "node 165 only has connected to one ssid 1\n",
      "node 181 only has connected to one ssid 1\n",
      "node 197 only has connected to one ssid 722\n",
      "node 198 only has connected to one ssid 1\n",
      "node 234 only has connected to one ssid 1\n",
      "node 238 only has connected to one ssid 1\n",
      "node 250 only has connected to one ssid 1\n",
      "node 260 only has connected to one ssid 1\n",
      "node 264 only has connected to one ssid 51\n",
      "node 266 only has connected to one ssid 100\n",
      "node 267 only has connected to one ssid 1\n"
     ]
    }
   ],
   "source": [
    "# Number of important SSID extracted from each user\n",
    "max_ssid = 3\n",
    "imp_ssids = []\n",
    "imp_ssids_users = {}\n",
    "\n",
    "for idx, row in enumerate(M):\n",
    "\n",
    "    row = row.astype(int)\n",
    "    # Get number of non-zero elements i.e. SSIDs that each user connected to\n",
    "    connected_ssids = np.where(row > 0)[0]\n",
    "\n",
    "    if len(connected_ssids) == 1:\n",
    "        imp_ssids.append(connected_ssids[0])\n",
    "        if connected_ssids[0] not in imp_ssids_users:\n",
    "            imp_ssids_users[connected_ssids[0]] = [idx]\n",
    "        else:\n",
    "            imp_ssids_users[connected_ssids[0]].append(idx)\n",
    "        print('node', idx, 'only has connected to one ssid', *connected_ssids)\n",
    "\n",
    "    if len(connected_ssids) > 1:\n",
    "        row_sort_idx = np.argsort(row)\n",
    "        for ssid in row_sort_idx[::-1][:max_ssid]:\n",
    "            imp_ssids.append(ssid)\n",
    "            if ssid not in imp_ssids_users:\n",
    "                imp_ssids_users[ssid] = [idx]\n",
    "            else:\n",
    "                imp_ssids_users[ssid].append(idx)\n",
    "        # print(*row_sort_idx[::-1][:max_ssid], sep='\\t')\n",
    "        # print(*row[row_sort_idx][::-1][:max_ssid], sep='\\t')\n",
    "\n",
    "# Convert important SSID list to set in order to remove duplicates\n",
    "imp_ssids = sorted(list(set(imp_ssids)))\n",
    "\n",
    "# Save selected SSIDs\n",
    "file_path = f'/home/ali/Projects/Network/data/phonelab/data/user_day_location_selected_ssid.csv'\n",
    "np.savetxt(file_path, imp_ssids, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, many of these nodes (exactly 12) connected to SSID = 1 which we  \n",
    "think is the main university Wi-Fi network's SSID and then 4 of users connected  \n",
    "to SSID = 4 which we don't know what it is, but the rest only has been connected  \n",
    "by 1 user only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 13, 16, 20, 23, 35, 37, 42, 43, 44, 46, 47, 48, 51, 52, 57, 62, 70, 72, 75, 76, 77, 78, 79, 84, 85, 87, 100, 101, 102, 103, 106, 107, 108, 111, 112, 113, 114, 115, 117, 118, 119, 120, 123, 124, 126, 128, 129, 130, 131, 133, 134, 137, 139, 141, 145, 146, 147, 148, 149, 152, 154, 155, 156, 157, 158, 160, 162, 166, 167, 168, 171, 179, 181, 182, 183, 192, 193, 194, 202, 203, 205, 206, 209, 211, 212, 213, 216, 219, 220, 224, 228, 230, 232, 234, 235, 236, 241, 242, 243, 244, 245, 246, 248, 249, 250, 257, 258, 262, 266, 267, 269, 271, 277, 278, 284, 293, 296, 297, 299, 305, 310, 313, 316, 322, 323, 324, 326, 328, 329, 330, 331, 332, 336, 337, 340, 348, 353, 354, 355, 357, 358, 359, 363, 365, 368, 369, 378, 380, 381, 382, 383, 384, 385, 387, 388, 389, 390, 391, 394, 395, 396, 399, 402, 403, 404, 407, 419, 422, 423, 424, 429, 430, 431, 432, 436, 437, 438, 447, 465, 468, 469, 470, 474, 475, 484, 485, 486, 488, 491, 492, 498, 499, 503, 505, 506, 507, 508, 519, 520, 521, 523, 527, 530, 547, 548, 549, 550, 552, 553, 558, 566, 569, 570, 576, 579, 582, 585, 586, 589, 590, 591, 594, 596, 597, 600, 602, 607, 608, 618, 624, 626, 627, 628, 629, 630, 631, 639, 640, 645, 646, 647, 667, 668, 669, 679, 680, 688, 691, 692, 696, 698, 704, 708, 712, 714, 719, 721, 722, 726, 737, 741, 743, 745, 747, 748, 754, 778, 784, 785, 795, 802, 804, 806, 808, 809, 814, 815, 816, 817, 818, 819, 822, 826, 830, 832, 838, 841, 850, 851, 852, 854, 859, 860, 864, 869, 888, 889, 893, 898, 900, 901, 902, 903, 904, 905, 906, 909, 910, 918, 929, 932, 933, 964, 966, 968, 975, 976, 983, 989, 991, 992, 1005, 1006, 1014, 1017, 1018, 1021, 1026, 1027, 1028, 1034, 1035, 1036, 1046, 1053, 1055, 1059, 1064, 1066, 1067, 1071, 1077, 1080, 1087, 1091, 1095, 1099, 1104, 1105, 1112, 1136, 1138, 1139, 1162, 1174, 1175]\n"
     ]
    }
   ],
   "source": [
    "print(len(imp_ssids))\n",
    "print(imp_ssids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use 2 top connected SSIDs for each user we end up with 253 SSIDs at the  \n",
    "end, but if we use 3 top SSIDs we end up with 366 unique SSID. The distribution  \n",
    "if how many users connected to each SSID is analyzed be below ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users_per_ssid = []\n",
    "num_users_ssid = {}\n",
    "for ssid in imp_ssids_users:\n",
    "    # print(f'{ssid}:')\n",
    "    # print(f'\\t{len(imp_ssids_users[ssid])} users: {imp_ssids_users[ssid]}')\n",
    "    num_users_per_ssid.append(len(imp_ssids_users[ssid]))\n",
    "    if len(imp_ssids_users[ssid]) > 3:\n",
    "        num_users_ssid[ssid] = len(imp_ssids_users[ssid])\n",
    "counter = Counter(num_users_per_ssid)\n",
    "# print(counter)\n",
    "# print(sorted(counter,key=counter.get, reverse=True))\n",
    "print('Number of connection to SSID: frequency')\n",
    "print(dict(counter.most_common()))\n",
    "print('SSID: number of connections')\n",
    "print(\n",
    "    dict(\n",
    "        sorted(num_users_ssid.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "210 (out of 253) SSID were only connected by 1 user. At the highest we can see  \n",
    "one SSID (id = 1) was connected by 170 users, then second and third most popular  \n",
    "SSIDs are 43, and 51 with 25 and 8 connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssid_dist = np.array(np.sum(M, axis=0, dtype=int).reshape(1, -1))[0]\n",
    "print(len(np.where(ssid_dist == 0)[0]))\n",
    "print(np.where(ssid_dist == 0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-Day -> SSID-Hour has 17,872 zero columns out of 28,224 or 63%, i.e., many    \n",
    "columns are zero and thus PCA could compact information nicely.    \n",
    "\n",
    "User-Day -> (253)Selected-SSID-Hour has 3,669 zero columns out of 6,072 or 60%    \n",
    "which is fairly similar to un-selected SSID case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ssid_dist[:10])\n",
    "print(list(enumerate(ssid_dist))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of time the firs 10 SSIDs {1,...,10} were connected by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.hist(\n",
    "    ssid_dist,\n",
    "    bins=range(min(ssid_dist),\n",
    "               max(ssid_dist) + 1, 1),\n",
    "    # cumulative=True,\n",
    "    alpha=0.7,\n",
    "    rwidth=0.5,\n",
    "    align='left'\n",
    ")\n",
    "ax.set_ylim((0,50))\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Number of Connected Users for each SSID Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sort the SSIDs based on how many time they have been connected to    \n",
    "in general (any user could visited same location more than once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssid_dist_tuple = sorted(\n",
    "    list(enumerate(ssid_dist)), key=lambda e: e[1], reverse=True\n",
    ")\n",
    "print(ssid_dist_tuple[:10])\n",
    "print('SSIDs sorted based on number of users conncting them:')\n",
    "print(list(zip(*ssid_dist_tuple[:len(imp_ssids)]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 108 SSIDs are common between important SSID from top connected    \n",
    "2 SSIDs for each user (with the total size of 253) and top connected SSIDs based    \n",
    "on total number of connections by users. In other words, 42% or near half of    \n",
    "important SSIDs are chosen from high connected SSIDs and the other half is based    \n",
    "on SSIDs and only 1 (or 2) user(s) would usually interact.\n",
    "\n",
    "If we change the top 2 SSIDs to top 3, we increase the number of important SSIDs    \n",
    "to 366 from 254 (112 more SSIDs or 48% increase) but the ratio of intersection    \n",
    "between two set is still about the same number (exactly at 49% with 180 SSIDs).\n",
    "\n",
    "When we move up to top 5 SSIDs, the intersection is 250 out of 470 or 53% which    \n",
    "is higher than before, but our important SSID set is almost half the size of all    \n",
    "SSIDs in dataset, but the ratio is still about the half.\n",
    "\n",
    "One can conclude that if we no mater the size of top connected SSIDs from each    \n",
    "user, usually about half of those would be important to all users as well, and    \n",
    "the other half would be more important to that specific user mostly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_ssid_inter = set(imp_ssids).intersection(\n",
    "    list(zip(*ssid_dist_tuple[:len(imp_ssids)]))[0]\n",
    ")\n",
    "print(len(imp_ssid_inter))\n",
    "print(len(imp_ssids))\n",
    "print(f'{len(imp_ssid_inter)/len(imp_ssids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User @ Day labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\n",
    "    # '/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_label.csv',\n",
    "    '/home/ali/Projects/Network/data/phonelab/data/user_day_location_connect_label.csv',\n",
    "    index_col=False,\n",
    "    header=None,\n",
    "    names=['label']\n",
    ").astype(int).values\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(labels)))\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the matrix (Min-Max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "M_scaled = min_max_scaler.fit_transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_scaler = MaxAbsScaler()\n",
    "# sM_scaled = min_max_scaler.fit_transform(sM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 19,144 rows of data and 1176 SSID or location or 1,176 x 24 = 28,224 (SSID x Hour) spatio-temporal features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_scaler = StandardScaler()\n",
    "# M_scaled= standard_scaler.fit_transform(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute covariance matrix\n",
    "\n",
    "Due to high computation, is executed on CC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_mat = np.cov(M_scaled.T)\n",
    "# eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load eigen vectors and eigen values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sE = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/user_ssid_connect_eigen_vecs_sparse.npz')\n",
    "\n",
    "# sE = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_eigen_vecs_sparse.npz')\n",
    "\n",
    "# sE = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/user_location_connect_eigen_vecs_sparse.npz')\n",
    "\n",
    "# sE = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/user_day_location_connect_eigen_vecs_sparse.npz')\n",
    "\n",
    "# Selected SSID\n",
    "# sE = sparse.load_npz('/home/ali/Projects/Network/data/phonelab/data/selected_2/user_day_ssid_connect_eigen_vecs.npz')\n",
    "\n",
    "# E = sE.toarray()\n",
    "\n",
    "# V = np.load('/home/ali/Projects/Network/data/phonelab/data/user_ssid_connect_eigen_vals.npy')\n",
    "\n",
    "# V = np.load('/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_eigen_vals.npy')\n",
    "\n",
    "# V = np.load('/home/ali/Projects/Network/data/phonelab/data/user_location_connect_eigen_vals.npy')\n",
    "\n",
    "# V = np.load('/home/ali/Projects/Network/data/phonelab/data/user_day_location_connect_eigen_vals.npy')\n",
    "\n",
    "# Selected SSID\n",
    "V = np.load('/home/ali/Projects/Network/data/phonelab/data/selected_2/user_day_ssid_connect_eigen_vals.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accumulative variance ratio of eigen values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(V)\n",
    "var_exp = np.array([(i / tot) for i in sorted(V, reverse=True)])\n",
    "cum_var_exp = np.cumsum(var_exp).real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 pricipal component variance ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp[:10].real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 10 cumilative pricipal component variance ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of needed components to represent 5,10,15, ... 100 % of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_at = cum_var_exp.searchsorted(np.linspace(0, 1, 20, endpoint=False))\n",
    "split_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_var = [\n",
    "        '{:.0f}'.format(x*100)\n",
    "        for x in np.linspace(0, 1, 20, endpoint=False)[20 -\n",
    "                                                       len(set(split_at)):]\n",
    "    ]\n",
    "cps = [x + 1 for x in sorted(list(set(split_at)))]\n",
    "cp_var = list(zip(split_var, cps))\n",
    "cp_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of components -> ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'80 % of data can be captured using {split_at[-4] + 1} number of components or {np.round((split_at[-4] + 1) / len(V) * 100, 2)} % of features.'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude repeated ones because they are give us same number of components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_at[1:]\n",
    "# split_at[2:]\n",
    "# split_at[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the variance ration at the identified component numbers  \n",
    "Note they should be increasing within 5% ratio ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_var_exp[split_at][1:]\n",
    "# cum_var_exp[split_at][2:]\n",
    "# cum_var_exp[split_at][3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize variance ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12), dpi=400)\n",
    "plt.bar(\n",
    "    # range(1, len(V) + 1),\n",
    "    # var_exp.real,\n",
    "    range(1, 251),\n",
    "    var_exp[:250].real,\n",
    "    alpha=0.5,\n",
    "    align='center',\n",
    "    label='Explained Variance'\n",
    ")\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Number of Principal Component')\n",
    "plt.legend(loc='best')\n",
    "PATH = os.path.join(DATA, 'pca_variance_ratio.pdf')\n",
    "# plt.savefig(PATH, dpi=400)\n",
    "plt.ylim((0, 0.055))\n",
    "plt.title('Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=400)\n",
    "plt.step(\n",
    "    range(1,\n",
    "          len(V) + 1),\n",
    "    cum_var_exp.real,\n",
    "    # range(1, 1051),\n",
    "    # cum_var_exp[0:1050].real,\n",
    "    where='mid',\n",
    "    label='Cumulative Explained Variance'\n",
    ")\n",
    "# plt.plot(cum_var_exp)\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Number of Principal Component')\n",
    "plt.legend(loc='best')\n",
    "PATH = os.path.join(DATA, 'pca_variance_ratio.pdf')\n",
    "# plt.savefig(PATH, dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=400)\n",
    "plt.bar(\n",
    "    range(1, 251),\n",
    "    var_exp[:250].real,\n",
    "    alpha=0.5,\n",
    "    align='center',\n",
    "    label='Explained Variance'\n",
    ")\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Number of Principal Component')\n",
    "plt.legend(loc='best')\n",
    "PATH = os.path.join(DATA, 'pca_variance_ratio.pdf')\n",
    "# plt.savefig(PATH, dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3), dpi=400)\n",
    "plt.step(\n",
    "    range(1, 251),\n",
    "    cum_var_exp[0:250].real,\n",
    "    where='mid',\n",
    "    label='Cumulative Explained Variance'\n",
    ")\n",
    "# plt.plot(cum_var_exp)\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Number of Principal Component')\n",
    "plt.legend(loc='best')\n",
    "PATH = os.path.join(DATA, 'pca_variance_ratio.pdf')\n",
    "# plt.savefig(PATH, dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort eigen vectors and eigen values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_pairs = [(np.abs(V[i]), E[:, i]) for i in range(len(V))]\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = eigen_vals.argsort()[::-1]\n",
    "# eigen_vals = eigen_vals[idx]\n",
    "# eigen_vecs = eigen_vecs[:,idx]\n",
    "# eigen_pairs = (eigen_vals, eigen_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_number = 250\n",
    "W = np.concatenate(\n",
    "    [eigen_pairs[i][1][:, np.newaxis] for i in range(component_number)],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_new = np.dot(M_minmax, W)\n",
    "M_new = M_minmax @ W\n",
    "\n",
    "# file_name = f'/home/ali/Projects/Network/data/phonelab/data/user_ssid_connect_transformed_{component_number}.npy'\n",
    "# np.save(file_name, M_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/ali/Projects/Network/data/phonelab/data/user_day_ssid_connect_label.csv'\n",
    "M_labels = pd.read_csv(PATH, index_col=False, header=None,\n",
    "                       names=['label']).iloc[:, 0]\n",
    "# y = pd.read_csv(PATH, index_col=False, header=None, names=['label']).astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the distribution of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_labels.value_counts().plot(kind='bar', figsize=(32, 6), rot=90)\n",
    "# plt.xticks(rotation=90)\n",
    "# Remoce odd X ticks\n",
    "ax = plt.gca()\n",
    "for label in ax.get_xaxis().get_ticklabels()[::2]:\n",
    "    label.set_visible(False)\n",
    "plt.xlabel('Label (i.e. User)', labelpad=10)\n",
    "plt.ylabel('Number of Instance (i.e. Days of Having Data)', labelpad=10)\n",
    "plt.title('Dataset Label Frequency', pad=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = clr.LinearSegmentedColormap.from_list(\n",
    "    'my_cmap', [\n",
    "        (0, 'skyblue'), (0.25, 'aqua'), (0.5, 'royalblue'), (0.75, 'darkblue'),\n",
    "        (1, 'black')\n",
    "    ],\n",
    "    N=len(M_labels.unique())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cmap = sns.color_palette(\n",
    "    'rocket', n_colors=len(M_labels.unique()), as_cmap=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_cmap(N, base_cmap=None):\n",
    "    '''Create an N-bin discrete colormap from the specified input map'''\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(M_labels.unique())\n",
    "plt.scatter(\n",
    "    M_new[:, 0],\n",
    "    M_new[:, 1],\n",
    "    c=M_labels,\n",
    "    edgecolor='none',\n",
    "    alpha=0.5,\n",
    "    s=10,\n",
    "    # cmap=plt.cm.get_cmap('spectral', 10) # Only if we have 10 classes\n",
    "    # cmap=plt.cm.get_cmap('viridis')\n",
    "    cmap=discrete_cmap(N, 'cubehelix')\n",
    "    # cmap = my_cmap\n",
    ")\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "# plt.colorbar()\n",
    "cbar = plt.colorbar(ticks=range(0, N, 10))\n",
    "cbar.ax.tick_params(labelsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    np.concatenate((M_new.real, M_labels[:, np.newaxis]), axis=1),\n",
    "    columns=['cp1', 'cp2', 'label']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on Dataset After PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dataframe from transformed data after PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=M.real, columns=['pc' + str(i) for i in range(M.shape[1])]\n",
    ")\n",
    "df['target'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of classes (i.e. labels) has only 1 instance which could affect the prediction. So we can (optionally) remove these to improve the classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_labels_counts = df['target'].value_counts()\n",
    "print(M_labels_counts[M_labels_counts < 2].index)\n",
    "\n",
    "df.drop(\n",
    "    df[df['target'].isin(M_labels_counts[M_labels_counts < 2].index)].index,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# M_labels_counts = df['target'].value_counts()\n",
    "# print(M_labels_counts[M_labels_counts < 2].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into training and test sets (while considering distribution of classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    df, test_size=0.2, random_state=1, stratify=df['target']\n",
    ")\n",
    "\n",
    "# print(train_set['target'].value_counts() / len(train_set))\n",
    "# print(test_set['target'].value_counts() / len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of dataset split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n",
    "# for train_index, test_index in strat_split.split(df, df['target']):\n",
    "#     train_set = df.iloc[train_index]\n",
    "#     test_set = df.iloc[test_index]\n",
    "\n",
    "# print(train_set['target'].value_counts() / len(train_set))\n",
    "# print(test_set['target'].value_counts() / len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.iloc[:, :-1]\n",
    "y_train = train_set.iloc[:, -1]\n",
    "X_test = test_set.iloc[:, :-1]\n",
    "y_test = test_set.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.iloc[0].values.reshape(1, -1))\n",
    "print(y_test.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_parameters = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [1]\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [1000],\n",
    "        'gamma': [0.1]\n",
    "    }\n",
    "]\n",
    "gs = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid=grid_parameters,\n",
    "    scoring='precision_macro',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(classification_report(y_test, y_pred,\n",
    "                                            output_dict=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)\n",
    "# report.loc['micro avg', 'precision':'f1-score']\n",
    "report.loc['macro avg', ['precision','recall','f1-score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = [cp_var[cp], cp]\n",
    "sc.extend(report.loc['micro avg', 'precision':'f1-score'])\n",
    "sc.append(gs.refit_time_)\n",
    "scores.append(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(gamma='auto', random_state=42)\n",
    "svm_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.predict(x_test.iloc[0].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# classifier = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "classifier = RandomForestClassifier(max_depth=20, random_state=0)\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(M_labels.unique())\n",
    "base = plt.cm.get_cmap('cubehelix')\n",
    "color_list = base(np.linspace(0, 1, N))\n",
    "# print(color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='randomized')\n",
    "\n",
    "# M_new = pca.fit_transform(M)\n",
    "M_new = pca.fit_transform(M_minmax)\n",
    "\n",
    "print(pca.components_.shape)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.arange(0, M_new.shape[0])\n",
    "plt.scatter(\n",
    "    M_new[:, 0],\n",
    "    M_new[:, 1],\n",
    "    c=target,\n",
    "    edgecolor='none',\n",
    "    alpha=0.5,\n",
    "    # cmap=plt.cm.get_cmap('spectral', 10) # Only if we have 10 classes\n",
    "    cmap=plt.cm.get_cmap('viridis')\n",
    ")\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data=M_new, columns=['pc1', 'pc2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "# ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "# ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA')\n",
    "# ax.set_title('2 component PCA', fontsize = 20)\n",
    "ax.scatter(principalDf['pc1'], principalDf['pc2'], s=1)\n",
    "# ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(M_minmax.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_com = []\n",
    "pca_var = []\n",
    "for dim in np.arange(1, M_minmax.shape[0]):\n",
    "    if dim <= M_minmax.shape[0]:\n",
    "        svd_solver = 'full'\n",
    "    else:\n",
    "        svd_solver = 'randomized'\n",
    "    pca = PCA(n_components=dim, svd_solver=svd_solver).fit(M_minmax)\n",
    "    pca_com.append(dim)\n",
    "    pca_var.append(sum(pca.explained_variance_ratio_))\n",
    "plt.plot(pca_com, pca_var)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(M_minmax)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "170d76527af6bb928a789dc7d6821516e71a979b7f8112ecc205a562846d3426"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
